{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e990a9c5",
   "metadata": {},
   "source": [
    "# Installing WebLLM in Pyodide with Micropip\n",
    "\n",
    "This notebook shows how to install and use WebLLM packages in Pyodide using micropip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4332807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install WebLLM-related packages via micropip\n",
    "import micropip\n",
    "\n",
    "# Install JavaScript interop helpers\n",
    "await micropip.install(['js', 'pyodide-js'])\n",
    "\n",
    "print(\"Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WebLLM from CDN\n",
    "from pyodide import js\n",
    "import asyncio\n",
    "\n",
    "# Dynamically load WebLLM\n",
    "js.eval(\"\"\"\n",
    "// Create a promise to load WebLLM\n",
    "window.loadWebLLM = new Promise((resolve, reject) => {\n",
    "    const script = document.createElement('script');\n",
    "    script.src = 'https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.46/lib/index.min.js';\n",
    "    script.onload = () => {\n",
    "        console.log('WebLLM loaded');\n",
    "        window.WebLLM = window.tvmjs.webllm;\n",
    "        resolve(window.WebLLM);\n",
    "    };\n",
    "    script.onerror = reject;\n",
    "    document.head.appendChild(script);\n",
    "});\n",
    "\"\"\")\n",
    "\n",
    "# Wait for WebLLM to load\n",
    "print(\"Loading WebLLM...\")\n",
    "await js.eval(\"window.loadWebLLM\")\n",
    "print(\"WebLLM loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python wrapper for WebLLM\n",
    "class PyodideWebLLM:\n",
    "    def __init__(self):\n",
    "        self.engine = None\n",
    "        self.ready = False\n",
    "    \n",
    "    async def load_model(self, model_id=\"Llama-3.2-1B-Instruct-q4f16_1-MLC\"):\n",
    "        \"\"\"Load a WebLLM model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading model: {model_id}\")\n",
    "            self.engine = await js.WebLLM.CreateMLCEngine(model_id)\n",
    "            self.ready = True\n",
    "            print(\"Model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    async def chat(self, message, max_tokens=512, temperature=0.7):\n",
    "        \"\"\"Chat with the model\"\"\"\n",
    "        if not self.ready:\n",
    "            return \"Model not loaded. Call load_model() first.\"\n",
    "        \n",
    "        try:\n",
    "            # Prepare chat messages\n",
    "            messages = [{\"role\": \"user\", \"content\": message}]\n",
    "            \n",
    "            # Generate response\n",
    "            response = await self.engine.chat.completions.create(\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    async def complete(self, prompt, max_tokens=512, temperature=0.7):\n",
    "        \"\"\"Complete text\"\"\"\n",
    "        if not self.ready:\n",
    "            return \"Model not loaded. Call load_model() first.\"\n",
    "        \n",
    "        try:\n",
    "            response = await self.engine.completions.create(\n",
    "                prompt=prompt,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            return response.choices[0].text\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "# Create WebLLM instance\n",
    "llm = PyodideWebLLM()\n",
    "print(\"WebLLM wrapper created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3614fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model (this may take a few minutes for first download)\n",
    "await llm.load_model()\n",
    "print(\"Ready to chat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7536812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat example\n",
    "response = await llm.chat(\"Explain what Pyodide is in simple terms\")\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text completion example\n",
    "response = await llm.complete(\"The advantages of running Python in the browser are:\")\n",
    "print(\"Completion:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat function\n",
    "async def interactive_chat():\n",
    "    \"\"\"Simple interactive chat with WebLLM\"\"\"\n",
    "    print(\" WebLLM Chat - Type 'quit' to exit\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input (in Pyodide, we'll simulate this)\n",
    "            user_input = input(\"You: \")\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit']:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if user_input.strip():\n",
    "                response = await llm.chat(user_input)\n",
    "                print(f\": {response}\")\n",
    "                print()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Note: input() doesn't work in JupyterLite, so we'll demo with predefined messages\n",
    "test_messages = [\n",
    "    \"Hello! What can you do?\",\n",
    "    \"Tell me about machine learning\",\n",
    "    \"How does WebLLM work?\"\n",
    "]\n",
    "\n",
    "print(\" Demo Chat Session\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for msg in test_messages:\n",
    "    print(f\"You: {msg}\")\n",
    "    response = await llm.chat(msg)\n",
    "    print(f\": {response}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
