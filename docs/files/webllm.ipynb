{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61d0521-be1d-4f22-bfd0-b8a6f2dee6ad",
   "metadata": {},
   "source": [
    "This [WebLLM](https://webllm.mlc.ai/) in a [JupyterLite](https://github.com/jupyterlite) demo notebook is from [Holoviz Panel App Gallery WebLLM](https://panel.holoviz.org/gallery/webllm.html).\n",
    "\n",
    "Recommended quick start is to choose \"Run: Run All Cells\".  That will take a few moments to load the wheels in the next cell and get the JupyterLite engine running.  Scroll to the end and when it is run you'll see a message that says \"Load a model...\".  Scroll up a bit until you see the \"Load\" button and the model selector.  The default is the smallest, Smol2-360M (130MB), so just click \"Load\" and you should see the progress bar run after a bit.  When it is done you'll see a message in the chat at the bottom saying the model is loaded.  Go ahead and type in the chat.  Smol 130M gives some pretty entertaining responses.  Everything is running in your browser.\n",
    "\n",
    "TODO: What is the behavior when WebGPU isn't available?\n",
    "\n",
    "This version at [https://wiki3.ai/notebooks/?path=webllm.ipynb](https://wiki3.ai/notebooks/?path=webllm.ipynb) is by [Jim White](https://github.com/jimwhite) and the project repo is at [https://github.com/fovi-llc/wiki3ai](https://github.com/fovi-llc/wiki3ai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c533087c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'micropip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmicropip\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Get the latest wheel URLs from the HoloViz CDN (adjust version as needed)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# You might need to check the HoloViz documentation for the absolute latest versions\u001b[39;00m\n\u001b[32m      5\u001b[39m bk_whl = \u001b[33m\"\u001b[39m\u001b[33mhttps://cdn.holoviz.org/panel/1.8.2/dist/wheels/bokeh-3.8.0-py3-none-any.whl\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'micropip'"
     ]
    }
   ],
   "source": [
    "import micropip\n",
    "\n",
    "# Get the latest wheel URLs from the HoloViz CDN (adjust version as needed)\n",
    "# You might need to check the HoloViz documentation for the absolute latest versions\n",
    "bk_whl = \"https://cdn.holoviz.org/panel/1.8.2/dist/wheels/bokeh-3.8.0-py3-none-any.whl\"\n",
    "pn_whl = \"https://cdn.holoviz.org/panel/1.8.2/dist/wheels/panel-1.8.2-py3-none-any.whl\"\n",
    "\n",
    "await micropip.install([bk_whl, pn_whl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c6078a-0398-425c-82f7-d516b01b713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import panel as pn\n",
    "import param\n",
    "\n",
    "from panel.custom import JSComponent, ESMEvent\n",
    "\n",
    "pn.extension('mathjax', template='material')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea88fd-3ae9-453f-a66e-b3bdc44c22e3",
   "metadata": {},
   "source": [
    "This example demonstrates how to wrap an external library (specifically [WebLLM](https://github.com/mlc-ai/web-llm)) as a `JSComponent` and interface it with the `ChatInterface`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c9fab-3fa7-48d7-83d0-5532bde6e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    'SmolLM2 (130MB)': 'SmolLM2-360M-Instruct-q4f16_1-MLC',\n",
    "    'TinyLlama-1.1B-Chat (675 MB)': 'TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC-1k',\n",
    "    'Gemma-2b (2GB)': 'gemma-2-2b-it-q4f16_1-MLC',\n",
    "    'Llama-3.2-3B-Instruct (2.2GB)': 'Llama-3.2-3B-Instruct-q4f16_1-MLC',\n",
    "    'Mistral-7b-Instruct (5GB)': 'Mistral-7B-Instruct-v0.3-q4f16_1-MLC',\n",
    "}\n",
    "\n",
    "class WebLLM(JSComponent):\n",
    "\n",
    "    loaded = param.Boolean(default=False, doc=\"\"\"\n",
    "        Whether the model is loaded.\"\"\")\n",
    "\n",
    "    history = param.Integer(default=3)\n",
    "\n",
    "    status = param.Dict(default={'text': '', 'progress': 0})\n",
    "\n",
    "    load_model = param.Event()\n",
    "\n",
    "    model = param.Selector(default='SmolLM2-360M-Instruct-q4f16_1-MLC', objects=MODELS)\n",
    "\n",
    "    running = param.Boolean(default=False, doc=\"\"\"\n",
    "        Whether the LLM is currently running.\"\"\")\n",
    "    \n",
    "    temperature = param.Number(default=1, bounds=(0, 2), doc=\"\"\"\n",
    "        Temperature of the model completions.\"\"\")\n",
    "\n",
    "    _esm = \"\"\"\n",
    "    import * as webllm from \"https://esm.run/@mlc-ai/web-llm\";\n",
    "\n",
    "    const engines = new Map()\n",
    "\n",
    "    export async function render({ model }) {\n",
    "      model.on(\"msg:custom\", async (event) => {\n",
    "        if (event.type === 'load') {\n",
    "          if (!engines.has(model.model)) {\n",
    "            const initProgressCallback = (status) => {\n",
    "              model.status = status\n",
    "            }\n",
    "            const mlc = await webllm.CreateMLCEngine(\n",
    "               model.model,\n",
    "               {initProgressCallback}\n",
    "            )\n",
    "            engines.set(model.model, mlc)\n",
    "          }\n",
    "          model.loaded = true\n",
    "        } else if (event.type === 'completion') {\n",
    "          const engine = engines.get(model.model)\n",
    "          if (engine == null) {\n",
    "            model.send_msg({'finish_reason': 'error'})\n",
    "          }\n",
    "          const chunks = await engine.chat.completions.create({\n",
    "            messages: event.messages,\n",
    "            temperature: model.temperature ,\n",
    "            stream: true,\n",
    "          })\n",
    "          model.running = true\n",
    "          for await (const chunk of chunks) {\n",
    "            if (!model.running) {\n",
    "              break\n",
    "            }\n",
    "            model.send_msg(chunk.choices[0])\n",
    "          }\n",
    "        }\n",
    "      })\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **params):\n",
    "        super().__init__(**params)\n",
    "        if pn.state.location:\n",
    "            pn.state.location.sync(self, {'model': 'model'})\n",
    "        self._buffer = []\n",
    "\n",
    "    @param.depends('load_model', watch=True)\n",
    "    def _load_model(self):\n",
    "        self.loading = True\n",
    "        self._send_msg({'type': 'load'})\n",
    "\n",
    "    @param.depends('loaded', watch=True)\n",
    "    def _loaded_model(self):\n",
    "        self.loading = False\n",
    "\n",
    "    @param.depends('model', watch=True)\n",
    "    def _update_load_model(self):\n",
    "        self.loaded = False\n",
    "\n",
    "    def _handle_msg(self, msg):\n",
    "        if self.running:\n",
    "            self._buffer.insert(0, msg)\n",
    "\n",
    "    async def create_completion(self, msgs):\n",
    "        self._send_msg({'type': 'completion', 'messages': msgs})\n",
    "        while True:\n",
    "            await asyncio.sleep(0.01)\n",
    "            if not self._buffer:\n",
    "                continue\n",
    "            choice = self._buffer.pop()\n",
    "            yield choice\n",
    "            reason = choice['finish_reason']\n",
    "            if reason == 'error':\n",
    "                raise RuntimeError('Model not loaded')\n",
    "            elif reason:\n",
    "                return\n",
    "\n",
    "    async def callback(self, contents: str, user: str):\n",
    "        if not self.loaded:\n",
    "            if self.loading:\n",
    "                yield pn.pane.Markdown(\n",
    "                    f'## `{self.model}`\\n\\n' + self.param.status.rx()['text']\n",
    "                )\n",
    "            else:\n",
    "                yield 'Load the model'\n",
    "            return\n",
    "        self.running = False\n",
    "        self._buffer.clear()\n",
    "        message = \"\"\n",
    "        async for chunk in self.create_completion([{'role': 'user', 'content': contents}]):\n",
    "            message += chunk['delta'].get('content', '')\n",
    "            yield message\n",
    "\n",
    "    def menu(self):\n",
    "        status = self.param.status.rx()\n",
    "        return pn.Column(\n",
    "            pn.widgets.Select.from_param(self.param.model, sizing_mode='stretch_width'),\n",
    "            pn.widgets.FloatSlider.from_param(self.param.temperature, sizing_mode='stretch_width'),\n",
    "            pn.widgets.Button.from_param(\n",
    "                self.param.load_model, sizing_mode='stretch_width',\n",
    "                disabled=self.param.loaded.rx().rx.or_(self.param.loading)\n",
    "            ),\n",
    "            pn.indicators.Progress(\n",
    "                value=(status['progress']*100).rx.pipe(int), visible=self.param.loading,\n",
    "                sizing_mode='stretch_width'\n",
    "            ),\n",
    "            pn.pane.Markdown(status['text'], visible=self.param.loading)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a663e937-797b-468f-875d-5bb8c2af002b",
   "metadata": {},
   "source": [
    "Having implemented the `WebLLM` component we can render the WebLLM UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58269444-868b-41e4-abe2-c4fcf031dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = WebLLM()\n",
    "\n",
    "intro = pn.pane.Alert(\"\"\"\n",
    "`WebLLM` runs large-language models entirely in your browser.\n",
    "When visiting the application the first time the model has\n",
    "to be downloaded and loaded into memory, which may take \n",
    "some time. Models are ordered by size (and capability),\n",
    "e.g. SmolLLM is very quick to download but produces poor\n",
    "quality output while Mistral-7b will take a while to\n",
    "download but produces much higher quality output.\n",
    "\"\"\".replace('\\n', ' '))\n",
    "\n",
    "pn.Column(\n",
    "    llm.menu(),\n",
    "    intro,\n",
    "    llm\n",
    ").servable(area='sidebar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96229aa4-c5ed-4c4e-944a-789ee65d768f",
   "metadata": {},
   "source": [
    "And connect it to a `ChatInterface`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f899068-8975-4cf4-9e1d-f3fdb5772a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface = pn.chat.ChatInterface(callback=llm.callback)\n",
    "chat_interface.send(\n",
    "    \"Load a model and start chatting.\",\n",
    "    user=\"System\",\n",
    "    respond=False,\n",
    ")\n",
    "\n",
    "llm.param.watch(lambda e: chat_interface.send(f'Loaded `{e.obj.model}`, start chatting!', user='System', respond=False), 'loaded')\n",
    "\n",
    "pn.Row(chat_interface).servable(title='WebLLM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki3ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
