"use strict";(self.webpackChunk_wiki3ai_lite_kernel=self.webpackChunk_wiki3ai_lite_kernel||[]).push([[509],{78:(e,t,n)=>{Object.defineProperty(t,"__esModule",{value:!0}),t.ChatHttpKernel=void 0;const s=n(394),o=n(206);t.ChatHttpKernel=class{constructor(e={}){const t="undefined"!=typeof window?window.webllmModelId:void 0;this.modelName=e.model??t??"Llama-3.2-3B-Instruct-q4f16_1-MLC",this.model=(0,o.webLLM)(this.modelName,{initProgressCallback:e=>{"undefined"!=typeof window&&window.dispatchEvent(new CustomEvent("webllm:model-progress",{detail:e}))}}),console.log("[ChatHttpKernel] Using WebLLM model:",this.modelName)}async send(e){const t=await this.model.availability();if("unavailable"===t)throw new Error("Browser does not support WebLLM / WebGPU.");"downloadable"!==t&&"downloading"!==t||await this.model.createSessionWithProgress(e=>{"undefined"!=typeof window&&window.dispatchEvent(new CustomEvent("webllm:model-progress",{detail:e}))});const n=await(0,s.streamText)({model:this.model,messages:[{role:"user",content:e}]});let o="";for await(const e of n.textStream)o+=e;return o}}},79:(e,t,n)=>{Object.defineProperty(t,"__esModule",{value:!0}),t.DEFAULT_WEBLLM_MODEL=t.WEBLLM_MODELS=void 0,t.isValidWebLLMModel=function(e){return t.WEBLLM_MODELS.includes(e)};const s=n(139);t.WEBLLM_MODELS=s.prebuiltAppConfig.model_list.map(e=>e.model_id),t.DEFAULT_WEBLLM_MODEL=["Llama-3.2-3B-Instruct-q4f16_1-MLC","Llama-3.1-8B-Instruct-q4f32_1-MLC","Mistral-7B-Instruct-v0.3-q4f16_1-MLC"].find(e=>t.WEBLLM_MODELS.includes(e))??t.WEBLLM_MODELS[0]},509:(e,t,n)=>{Object.defineProperty(t,"__esModule",{value:!0});const s=n(596),o=n(716),a=n(79),r="http-chat",l=[{id:"http-chat-kernel:plugin",autoStart:!0,requires:[s.IKernelSpecs],activate:async(e,t)=>{if(console.log("[http-chat-kernel] Activating plugin"),!t||"function"!=typeof t.register)return void console.warn("[http-chat-kernel] kernelspecs.register is not available; kernel will not be registered.",t);try{const t=[],n=e.serviceManager?.ready;n&&t.push(Promise.resolve(n)),e.restored&&t.push(e.restored),t.length&&await Promise.all(t)}catch(e){console.warn("[http-chat-kernel] Failed waiting for kernelspecs readiness",e)}if(t.register({spec:{name:r,display_name:"HTTP Chat (ACP)",language:"python",argv:[],resources:{}},create:async e=>(console.log("[http-chat-kernel] Creating HttpLiteKernel instance",e),Promise.resolve((0,o.createHttpLiteKernel)(e)))}),console.log(`[http-chat-kernel] Kernel spec '${r}' registered`),"undefined"==typeof document)return;const n=document.createElement("div");n.style.position="fixed",n.style.top="8px",n.style.right="8px",n.style.zIndex="9999",n.style.padding="4px 8px",n.style.background="rgba(0,0,0,0.7)",n.style.color="#fff",n.style.fontSize="12px",n.style.borderRadius="4px",n.style.display="flex",n.style.gap="4px",n.style.alignItems="center";const s=document.createElement("span");s.textContent="WebLLM model:",n.appendChild(s);const l=document.createElement("select"),i=window.localStorage.getItem("webllm:modelId")??a.DEFAULT_WEBLLM_MODEL;a.WEBLLM_MODELS.forEach(e=>{const t=document.createElement("option");t.value=e,t.textContent=e,e===i&&(t.selected=!0),l.appendChild(t)}),window.webllmModelId=i,l.onchange=()=>{window.webllmModelId=l.value,window.localStorage.setItem("webllm:modelId",l.value)},n.appendChild(l);const c=document.createElement("progress");c.max=1,c.value=0,c.style.width="120px",c.style.display="none",n.appendChild(c);const d=document.createElement("span");d.textContent="",n.appendChild(d),window.addEventListener("webllm:model-progress",e=>{const{detail:t={}}=e,{progress:n,text:s}=t,o=void 0!==n&&n>0&&n<1;c.style.display=o?"inline-block":"none",c.value=n??0,d.textContent=s??""}),document.body.appendChild(n)}}];t.default=l},716:(e,t,n)=>{Object.defineProperty(t,"__esModule",{value:!0}),t.HttpLiteKernel=void 0,t.createHttpLiteKernel=function(e){return new a(e)};const s=n(596),o=n(78);class a extends s.BaseKernel{constructor(e){super(e);const t=e.model;this.chat=new o.ChatHttpKernel({model:t})}async executeRequest(e){const t=String(e.code??"");try{const e=await this.chat.send(t);return this.publishExecuteResult({data:{"text/plain":e},metadata:{},execution_count:this.executionCount},this.parentHeader),{status:"ok",execution_count:this.executionCount,payload:[],user_expressions:{}}}catch(e){const t=e?.message??String(e);return this.publishExecuteError({ename:"Error",evalue:t,traceback:[]},this.parentHeader),{status:"error",execution_count:this.executionCount,ename:"Error",evalue:t,traceback:[]}}}async kernelInfoRequest(){return{status:"ok",protocol_version:"5.3",implementation:"http-lite-kernel",implementation_version:"0.1.0",language_info:{name:"markdown",version:"0.0.0",mimetype:"text/markdown",file_extension:".md"},banner:"HTTP-backed LLM chat kernel",help_links:[]}}async completeRequest(e){return{status:"ok",matches:[],cursor_start:e.cursor_pos??0,cursor_end:e.cursor_pos??0,metadata:{}}}async inspectRequest(e){return{status:"ok",found:!1,data:{},metadata:{}}}async isCompleteRequest(e){return{status:"complete",indent:""}}async commInfoRequest(e){return{status:"ok",comms:{}}}async historyRequest(e){return{status:"ok",history:[]}}async shutdownRequest(e){return{status:"ok",restart:!1}}async inputReply(e){}async commOpen(e){}async commMsg(e){}async commClose(e){}}t.HttpLiteKernel=a}}]);