{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnZ2fqegSEBk"
      },
      "source": [
        "# Llama-Index Quickstart\n",
        "\n",
        "In this quickstart you will create a simple Llama Index App and learn how to log it and get feedback on an LLM response.\n",
        "\n",
        "For evaluation, we will leverage the \"hallucination triad\" of groundedness, context relevance and answer relevance.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/quickstart/llama_index_quickstart.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz7cLHMNSEBo"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Install dependencies\n",
        "Let's install some of the dependencies for this notebook if we don't have them already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXK0D31TSEBp",
        "outputId": "c6ab9eb7-d8a8-4917-d106-1f968d1759a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU \"trulens_eval>=0.19.2\" \"llama_index>0.9.17\" \"html2text>=2020.1.16\" qdrant_client python-dotenv ipywidgets streamlit_jupyter \"litellm>=1.15.1\" google-cloud-aiplatform \n",
        "# 'google-generativeai>=0.3.0'\n",
        "# %pip install -qU trulens_eval==0.19.1 llama_index>0.9.15 html2text>=2020.1.16 qdrant_client python-dotenv\n",
        "\n",
        "%load_ext dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-cloud-core in ./.venv/lib/python3.11/site-packages (2.4.1)\n",
            "Requirement already satisfied: google-cloud-aiplatform in ./.venv/lib/python3.11/site-packages (1.38.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6 in ./.venv/lib/python3.11/site-packages (from google-cloud-core) (2.15.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in ./.venv/lib/python3.11/site-packages (from google-cloud-core) (2.25.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (4.25.1)\n",
            "Requirement already satisfied: packaging>=14.3 in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (23.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (2.14.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (3.14.1)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (1.11.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (2.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in ./.venv/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (1.62.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in ./.venv/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in ./.venv/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in ./.venv/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-core) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-core) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-core) (4.9)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in ./.venv/lib/python3.11/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in ./.venv/lib/python3.11/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in ./.venv/lib/python3.11/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in ./.venv/lib/python3.11/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.14 in ./.venv/lib/python3.11/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.26.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./.venv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-core) (0.5.1)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (2023.11.17)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U google-cloud-core google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxB8G6jiSEBr"
      },
      "source": [
        "### Add API keys\n",
        "For this quickstart, you will need Open AI and Huggingface keys. The OpenAI key is used for embeddings and GPT, and the Huggingface key is used for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Zlv9qBKjSEBr"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from google.colab import userdata\n",
        "# GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "%dotenv\n",
        "GOOGLE_API_KEY = os.environ[\"GEMINI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "# This is used by the LiteLLM for Vertex AI models including Gemini.\n",
        "# The LiteLLM wrapper for Gemini is used by the TruLens evaluation provider.\n",
        "aiplatform.init(\n",
        "    project = \"fovi-site\",\n",
        "    location=\"us-west1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkZE-pLzSEBr"
      },
      "source": [
        "### Import from LlamaIndex and TruLens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0CLC5QBSEBs",
        "outputId": "d10aad25-0ea9-4d49-a2eb-86d7cf814711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
            "ðŸ”’ Secret keys will not be included in the database.\n"
          ]
        }
      ],
      "source": [
        "from trulens_eval import Tru\n",
        "\n",
        "tru = Tru(database_redact_keys=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LivWqTvSEBt"
      },
      "source": [
        "### Create Simple LLM Application\n",
        "\n",
        "This example uses LlamaIndex which internally uses an OpenAI LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dokLMzEjMxke",
        "outputId": "c5f5b6b9-ca64-4f08-ff15-9c48f56d06d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id_='22b74553-bf09-465f-b4a3-2259e847ea3a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='e63f93d1899b11fdd72d48ae487da624571439d97f87ca63fa5cc9753df3549e', text='![](https://s.turbifycdn.com/aah/paulgraham/essays-5.gif)|\\n![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif)|\\n[![](https://s.turbifycdn.com/aah/paulgraham/essays-6.gif)](index.html)  \\n  \\n| ![What I Worked On](https://s.turbifycdn.com/aah/paulgraham/what-i-worked-\\non-4.gif)  \\n  \\nFebruary 2021  \\n  \\nBefore college the two main things I worked on, outside of school, were\\nwriting and programming. I didn\\'t write essays. I wrote what beginning writers\\nwere supposed to write then, and probably still are: short stories. My stories\\nwere awful. They had hardly any plot, just characters with strong feelings,\\nwhich I imagined made them deep.  \\n  \\nThe first programs I tried writing were on the IBM 1401 that our school\\ndistrict used for what was then called \"data processing.\" This was in 9th\\ngrade, so I was 13 or 14. The school district\\'s 1401 happened to be in the\\nbasement of our junior high school, and my friend Rich Draves and I got\\npermission to use it. It was like a mini Bond villain\\'s lair down there, with\\nall these alien-looking machines \\x97 CPU, disk drives, printer, card reader \\x97\\nsitting up on a raised floor under bright fluorescent lights.  \\n  \\nThe language we used was an early version of Fortran. You had to type programs\\non punch cards, then stack them in the card reader and press a button to load\\nthe program into memory and run it. The result would ordinarily be to print\\nsomething on the spectacularly loud printer.  \\n  \\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in\\nretrospect there\\'s not much I could have done with it. The only form of input\\nto programs was data stored on punched cards, and I didn\\'t have any data\\nstored on punched cards. The only other option was to do things that didn\\'t\\nrely on any input, like calculate approximations of pi, but I didn\\'t know\\nenough math to do anything interesting of that type. So I\\'m not surprised I\\ncan\\'t remember any programs I wrote, because they can\\'t have done much. My\\nclearest memory is of the moment I learned it was possible for programs not to\\nterminate, when one of mine didn\\'t. On a machine without time-sharing, this\\nwas a social as well as a technical error, as the data center manager\\'s\\nexpression made clear.  \\n  \\nWith microcomputers, everything changed. Now you could have a computer sitting\\nright in front of you, on a desk, that could respond to your keystrokes as it\\nwas running instead of just churning through a stack of punch cards and then\\nstopping. [1]  \\n  \\nThe first of my friends to get a microcomputer built it himself. It was sold\\nas a kit by Heathkit. I remember vividly how impressed and envious I felt\\nwatching him sitting in front of it, typing programs right into the computer.  \\n  \\nComputers were expensive in those days and it took me years of nagging before\\nI convinced my father to buy one, a TRS-80, in about 1980\\\\. The gold standard\\nthen was the Apple II, but a TRS-80 was good enough. This was when I really\\nstarted programming. I wrote simple games, a program to predict how high my\\nmodel rockets would fly, and a word processor that my father used to write at\\nleast one book. There was only room in memory for about 2 pages of text, so\\nhe\\'d write 2 pages at a time and then print them out, but it was a lot better\\nthan a typewriter.  \\n  \\nThough I liked programming, I didn\\'t plan to study it in college. In college I\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\nmy naive high school self, to be the study of the ultimate truths, compared to\\nwhich the things studied in other fields would be mere domain knowledge. What\\nI discovered when I got to college was that the other fields took up so much\\nof the space of ideas that there wasn\\'t much left for these supposed ultimate\\ntruths. All that seemed left for philosophy were edge cases that people in\\nother fields felt could safely be ignored.  \\n  \\nI couldn\\'t have put this into words when I was 18. All I knew at the time was\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\nto switch to AI.  \\n  \\nAI was in the air in the mid 1980s, but there were two things especially that\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading\\n_The Moon is a Harsh Mistress_ , so I don\\'t know how well it has aged, but\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\ntime before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\nlike that time would be a few years at most. All you had to do was teach\\nSHRDLU more words.  \\n  \\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so\\nI started trying to teach myself. Which meant learning Lisp, since in those\\ndays Lisp was regarded as the language of AI. The commonly used programming\\nlanguages then were pretty primitive, and programmers\\' ideas correspondingly\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\na program so fast that it was years before I started to have a sense of where\\nthe new limits were. This was more like it; this was what I had expected\\ncollege to do. It wasn\\'t happening in a class, like it was supposed to, but\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\ngoing to do.  \\n  \\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\nworking on that program. It was a pleasing bit of code, but what made it even\\nmore exciting was my belief \\x97 hard to imagine now, but not unique in 1985 \\x97\\nthat it was already climbing the lower slopes of intelligence.  \\n  \\nI had gotten into a program at Cornell that didn\\'t make you choose a major.\\nYou could take whatever classes you liked, and choose whatever you liked to\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\n  \\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\ntime, and Harvard, which I\\'d visited because Rich Draves went there, and was\\nalso home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU\\nclone. Only Harvard accepted me, so that was where I went.  \\n  \\nI don\\'t remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.  \\n  \\nSo I looked around to see what I could salvage from the wreckage of my plans,\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\nown sake and not just for its association with AI, even though that was the\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\nIn fact, I decided to write a book about Lisp hacking. It\\'s scary to think how\\nlittle I knew about Lisp hacking when I started writing that book. But there\\'s\\nnothing like writing a book about something to help you learn it. The book,\\n_On Lisp_ , wasn\\'t published till 1993, but I wrote much of it in grad school.  \\n  \\nComputer Science is an uneasy alliance between two halves, theory and systems.\\nThe theory people prove things, and the systems people build things. I wanted\\nto build things. I had plenty of respect for theory \\x97 indeed, a sneaking\\nsuspicion that it was the more admirable of the two halves \\x97 but building\\nthings seemed so much more exciting.  \\n  \\nThe problem with systems work, though, was that it didn\\'t last. Any program\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\nbest. People might mention your software in footnotes, but no one would\\nactually use it. And indeed, it would seem very feeble work. Only people with\\na sense of the history of the field would even realize that, in its time, it\\nhad been good.  \\n  \\nThere were some surplus Xerox Dandelions floating around the computer lab at\\none point. Anyone who wanted one to play around with could have one. I was\\nbriefly tempted, but they were so slow by present standards; what was the\\npoint? No one else wanted one either, so off they went. That was what happened\\nto systems work.  \\n  \\nI wanted not just to build things, but to build things that would last.  \\n  \\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\nI\\'d spent a lot of time as a kid. While looking at a painting there I realized\\nsomething that might seem obvious, but was a big surprise to me. There, right\\non the wall, was something you could make that would last. Paintings didn\\'t\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\n  \\nAnd moreover this was something you could make a living doing. Not as easily\\nas you could by writing software, of course, but I thought if you were really\\nindustrious and lived really cheaply, it had to be possible to make enough to\\nsurvive. And as an artist you could be truly independent. You wouldn\\'t have a\\nboss, or even need to get research funding.  \\n  \\nI had always liked looking at paintings. Could I make them? I had no idea. I\\'d\\nnever imagined it was even possible. I knew intellectually that people made\\nart \\x97 that it didn\\'t just appear spontaneously \\x97 but it was as if the people\\nwho made it were a different species. They either lived long ago or were\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\nidea of actually being able to make art, to put that verb before that noun,\\nseemed almost miraculous.  \\n  \\nThat fall I started taking art classes at Harvard. Grad students could take\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\nIf he even knew about the strange classes I was taking, he never said\\nanything.  \\n  \\nSo now I was in a PhD program in computer science, yet planning to be an\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\nLisp_. In other words, like many a grad student, I was working energetically\\non multiple projects that were not my thesis.  \\n  \\nI didn\\'t see a way out of this situation. I didn\\'t want to drop out of grad\\nschool, but how else was I going to get out? I remember when my friend Robert\\nMorris got kicked out of Cornell for writing the internet worm of 1988, I was\\nenvious that he\\'d found such a spectacular way to get out of grad school.  \\n  \\nThen one day in April 1990 a crack appeared in the wall. I ran into professor\\nCheatham and he asked if I was far enough along to graduate that June. I\\ndidn\\'t have a word of my dissertation written, but in what must have been the\\nquickest bit of thinking in my life, I decided to take a shot at writing one\\nin the 5 weeks or so that remained before the deadline, reusing parts of _On\\nLisp_ where I could, and I was able to respond, with no perceptible delay\\n\"Yes, I think so. I\\'ll give you something to read in a few days.\"  \\n  \\nI picked applications of continuations as the topic. In retrospect I should\\nhave written about macros and embedded languages. There\\'s a whole world there\\nthat\\'s barely been explored. But all I wanted was to get out of grad school,\\nand my rapidly written dissertation sufficed, just barely.  \\n  \\nMeanwhile I was applying to art schools. I applied to two: RISD in the US, and\\nthe Accademia di Belli Arti in Florence, which, because it was the oldest art\\nschool, I imagined would be good. RISD accepted me, and I never heard back\\nfrom the Accademia, so off to Providence I went.  \\n  \\nI\\'d applied for the BFA program at RISD, which meant in effect that I had to\\ngo to college again. This was not as strange as it sounds, because I was only\\n25, and art schools are full of people of different ages. RISD counted me as a\\ntransfer sophomore and said I had to do the foundation that summer. The\\nfoundation means the classes that everyone has to take in fundamental subjects\\nlike drawing, color, and design.  \\n  \\nToward the end of the summer I got a big surprise: a letter from the\\nAccademia, which had been delayed because they\\'d sent it to Cambridge England\\ninstead of Cambridge Massachusetts, inviting me to take the entrance exam in\\nFlorence that fall. This was now only weeks away. My nice landlady let me\\nleave my stuff in her attic. I had some money saved from consulting work I\\'d\\ndone in grad school; there was probably enough to last a year if I lived\\ncheaply. Now all I had to do was learn Italian.  \\n  \\nOnly _stranieri_ (foreigners) had to take this entrance exam. In retrospect it\\nmay well have been a way of excluding them, because there were so many\\n_stranieri_ attracted by the idea of studying art in Florence that the Italian\\nstudents would otherwise have been outnumbered. I was in decent shape at\\npainting and drawing from the RISD foundation that summer, but I still don\\'t\\nknow how I managed to pass the written exam. I remember that I answered the\\nessay question by writing about Cezanne, and that I cranked up the\\nintellectual level as high as I could to make the most of my limited\\nvocabulary. [2]  \\n  \\nI\\'m only up to age 25 and already there are such conspicuous patterns. Here I\\nwas, yet again about to attend some august institution in the hopes of\\nlearning about some prestigious subject, and yet again about to be\\ndisappointed. The students and faculty in the painting department at the\\nAccademia were the nicest people you could imagine, but they had long since\\narrived at an arrangement whereby the students wouldn\\'t require the faculty to\\nteach anything, and in return the faculty wouldn\\'t require the students to\\nlearn anything. And at the same time all involved would adhere outwardly to\\nthe conventions of a 19th century atelier. We actually had one of those little\\nstoves, fed with kindling, that you see in 19th century studio paintings, and\\na nude model sitting as close to it as possible without getting burned. Except\\nhardly anyone else painted her besides me. The rest of the students spent\\ntheir time chatting or occasionally trying to imitate things they\\'d seen in\\nAmerican art magazines.  \\n  \\nOur model turned out to live just down the street from me. She made a living\\nfrom a combination of modelling and making fakes for a local antique dealer.\\nShe\\'d copy an obscure old painting out of a book, and then he\\'d take the copy\\nand maltreat it to make it look old. [3]  \\n  \\nWhile I was a student at the Accademia I started painting still lives in my\\nbedroom at night. These paintings were tiny, because the room was, and because\\nI painted them on leftover scraps of canvas, which was all I could afford at\\nthe time. Painting still lives is different from painting people, because the\\nsubject, as its name suggests, can\\'t move. People can\\'t sit for more than\\nabout 15 minutes at a time, and when they do they don\\'t sit very still. So the\\ntraditional m.o. for painting people is to know how to paint a generic person,\\nwhich you then modify to match the specific person you\\'re painting. Whereas a\\nstill life you can, if you want, copy pixel by pixel from what you\\'re seeing.\\nYou don\\'t want to stop there, of course, or you get merely photographic\\naccuracy, and what makes a still life interesting is that it\\'s been through a\\nhead. You want to emphasize the visual cues that tell you, for example, that\\nthe reason the color changes suddenly at a certain point is that it\\'s the edge\\nof an object. By subtly emphasizing such things you can make paintings that\\nare more realistic than photographs not just in some metaphorical sense, but\\nin the strict information-theoretic sense. [4]  \\n  \\nI liked painting still lives because I was curious about what I was seeing. In\\neveryday life, we aren\\'t consciously aware of much we\\'re seeing. Most visual\\nperception is handled by low-level processes that merely tell your brain\\n\"that\\'s a water droplet\" without telling you details like where the lightest\\nand darkest points are, or \"that\\'s a bush\" without telling you the shape and\\nposition of every leaf. This is a feature of brains, not a bug. In everyday\\nlife it would be distracting to notice every leaf on every bush. But when you\\nhave to paint something, you have to look more closely, and when you do\\nthere\\'s a lot to see. You can still be noticing new things after days of\\ntrying to paint something people usually take for granted, just as you can\\nafter days of trying to write an essay about something people usually take for\\ngranted.  \\n  \\nThis is not the only way to paint. I\\'m not 100% sure it\\'s even a good way to\\npaint. But it seemed a good enough bet to be worth trying.  \\n  \\nOur teacher, professor Ulivi, was a nice guy. He could see I worked hard, and\\ngave me a good grade, which he wrote down in a sort of passport each student\\nhad. But the Accademia wasn\\'t teaching me anything except Italian, and my\\nmoney was running out, so at the end of the first year I went back to the US.  \\n  \\nI wanted to go back to RISD, but I was now broke and RISD was very expensive,\\nso I decided to get a job for a year and then return to RISD the next fall. I\\ngot one at a company called Interleaf, which made software for creating\\ndocuments. You mean like Microsoft Word? Exactly. That was how I learned that\\nlow end software tends to eat high end software. But Interleaf still had a few\\nyears to live yet. [5]  \\n  \\nInterleaf had done something pretty bold. Inspired by Emacs, they\\'d added a\\nscripting language, and even made the scripting language a dialect of Lisp.\\nNow they wanted a Lisp hacker to write things in it. This was the closest\\nthing I\\'ve had to a normal job, and I hereby apologize to my boss and\\ncoworkers, because I was a bad employee. Their Lisp was the thinnest icing on\\na giant C cake, and since I didn\\'t know C and didn\\'t want to learn it, I never\\nunderstood most of the software. Plus I was terribly irresponsible. This was\\nback when a programming job meant showing up every day during certain working\\nhours. That seemed unnatural to me, and on this point the rest of the world is\\ncoming around to my way of thinking, but at the time it caused a lot of\\nfriction. Toward the end of the year I spent much of my time surreptitiously\\nworking on _On Lisp_ , which I had by this time gotten a contract to publish.  \\n  \\nThe good part was that I got paid huge amounts of money, especially by art\\nstudent standards. In Florence, after paying my part of the rent, my budget\\nfor everything else had been $7 a day. Now I was getting paid more than 4\\ntimes that every hour, even when I was just sitting in a meeting. By living\\ncheaply I not only managed to save enough to go back to RISD, but also paid\\noff my college loans.  \\n  \\nI learned some useful things at Interleaf, though they were mostly about what\\nnot to do. I learned that it\\'s better for technology companies to be run by\\nproduct people than sales people (though sales is a real skill and people who\\nare good at it are really good at it), that it leads to bugs when code is\\nedited by too many people, that cheap office space is no bargain if it\\'s\\ndepressing, that planned meetings are inferior to corridor conversations, that\\nbig, bureaucratic customers are a dangerous source of money, and that there\\'s\\nnot much overlap between conventional office hours and the optimal time for\\nhacking, or conventional offices and the optimal place for it.  \\n  \\nBut the most important thing I learned, and which I used in both Viaweb and Y\\nCombinator, is that the low end eats the high end: that it\\'s good to be the\\n\"entry level\" option, even though that will be less prestigious, because if\\nyou\\'re not, someone else will be, and will squash you against the ceiling.\\nWhich in turn means that prestige is a danger sign.  \\n  \\nWhen I left to go back to RISD the next fall, I arranged to do freelance work\\nfor the group that did projects for customers, and this was how I survived for\\nthe next several years. When I came back to visit for a project later on,\\nsomeone told me about a new thing called HTML, which was, as he described it,\\na derivative of SGML. Markup language enthusiasts were an occupational hazard\\nat Interleaf and I ignored him, but this HTML thing later became a big part of\\nmy life.  \\n  \\nIn the fall of 1992 I moved back to Providence to continue at RISD. The\\nfoundation had merely been intro stuff, and the Accademia had been a (very\\ncivilized) joke. Now I was going to see what real art school was like. But\\nalas it was more like the Accademia than not. Better organized, certainly, and\\na lot more expensive, but it was now becoming clear that art school did not\\nbear the same relationship to art that medical school bore to medicine. At\\nleast not the painting department. The textile department, which my next door\\nneighbor belonged to, seemed to be pretty rigorous. No doubt illustration and\\narchitecture were too. But painting was post-rigorous. Painting students were\\nsupposed to express themselves, which to the more worldly ones meant to try to\\ncook up some sort of distinctive signature style.  \\n  \\nA signature style is the visual equivalent of what in show business is known\\nas a \"schtick\": something that immediately identifies the work as yours and no\\none else\\'s. For example, when you see a painting that looks like a certain\\nkind of cartoon, you know it\\'s by Roy Lichtenstein. So if you see a big\\npainting of this type hanging in the apartment of a hedge fund manager, you\\nknow he paid millions of dollars for it. That\\'s not always why artists have a\\nsignature style, but it\\'s usually why buyers pay a lot for such work. [6]  \\n  \\nThere were plenty of earnest students too: kids who \"could draw\" in high\\nschool, and now had come to what was supposed to be the best art school in the\\ncountry, to learn to draw even better. They tended to be confused and\\ndemoralized by what they found at RISD, but they kept going, because painting\\nwas what they did. I was not one of the kids who could draw in high school,\\nbut at RISD I was definitely closer to their tribe than the tribe of signature\\nstyle seekers.  \\n  \\nI learned a lot in the color class I took at RISD, but otherwise I was\\nbasically teaching myself to paint, and I could do that for free. So in 1993 I\\ndropped out. I hung around Providence for a bit, and then my college friend\\nNancy Parmet did me a big favor. A rent-controlled apartment in a building her\\nmother owned in New York was becoming vacant. Did I want it? It wasn\\'t much\\nmore than my current place, and New York was supposed to be where the artists\\nwere. So yes, I wanted it! [7]  \\n  \\nAsterix comics begin by zooming in on a tiny corner of Roman Gaul that turns\\nout not to be controlled by the Romans. You can do something similar on a map\\nof New York City: if you zoom in on the Upper East Side, there\\'s a tiny corner\\nthat\\'s not rich, or at least wasn\\'t in 1993. It\\'s called Yorkville, and that\\nwas my new home. Now I was a New York artist \\x97 in the strictly technical sense\\nof making paintings and living in New York.  \\n  \\nI was nervous about money, because I could sense that Interleaf was on the way\\ndown. Freelance Lisp hacking work was very rare, and I didn\\'t want to have to\\nprogram in another language, which in those days would have meant C++ if I was\\nlucky. So with my unerring nose for financial opportunity, I decided to write\\nanother book on Lisp. This would be a popular book, the sort of book that\\ncould be used as a textbook. I imagined myself living frugally off the\\nroyalties and spending all my time painting. (The painting on the cover of\\nthis book, _ANSI Common Lisp_ , is one that I painted around this time.)  \\n  \\nThe best thing about New York for me was the presence of Idelle and Julian\\nWeber. Idelle Weber was a painter, one of the early photorealists, and I\\'d\\ntaken her painting class at Harvard. I\\'ve never known a teacher more beloved\\nby her students. Large numbers of former students kept in touch with her,\\nincluding me. After I moved to New York I became her de facto studio\\nassistant.  \\n  \\nShe liked to paint on big, square canvases, 4 to 5 feet on a side. One day in\\nlate 1994 as I was stretching one of these monsters there was something on the\\nradio about a famous fund manager. He wasn\\'t that much older than me, and was\\nsuper rich. The thought suddenly occurred to me: why don\\'t I become rich? Then\\nI\\'ll be able to work on whatever I want.  \\n  \\nMeanwhile I\\'d been hearing more and more about this new thing called the World\\nWide Web. Robert Morris showed it to me when I visited him in Cambridge, where\\nhe was now in grad school at Harvard. It seemed to me that the web would be a\\nbig deal. I\\'d seen what graphical user interfaces had done for the popularity\\nof microcomputers. It seemed like the web would do the same for the internet.  \\n  \\nIf I wanted to get rich, here was the next train leaving the station. I was\\nright about that part. What I got wrong was the idea. I decided we should\\nstart a company to put art galleries online. I can\\'t honestly say, after\\nreading so many Y Combinator applications, that this was the worst startup\\nidea ever, but it was up there. Art galleries didn\\'t want to be online, and\\nstill don\\'t, not the fancy ones. That\\'s not how they sell. I wrote some\\nsoftware to generate web sites for galleries, and Robert wrote some to resize\\nimages and set up an http server to serve the pages. Then we tried to sign up\\ngalleries. To call this a difficult sale would be an understatement. It was\\ndifficult to give away. A few galleries let us make sites for them for free,\\nbut none paid us.  \\n  \\nThen some online stores started to appear, and I realized that except for the\\norder buttons they were identical to the sites we\\'d been generating for\\ngalleries. This impressive-sounding thing called an \"internet storefront\" was\\nsomething we already knew how to build.  \\n  \\nSo in the summer of 1995, after I submitted the camera-ready copy of _ANSI\\nCommon Lisp_ to the publishers, we started trying to write software to build\\nonline stores. At first this was going to be normal desktop software, which in\\nthose days meant Windows software. That was an alarming prospect, because\\nneither of us knew how to write Windows software or wanted to learn. We lived\\nin the Unix world. But we decided we\\'d at least try writing a prototype store\\nbuilder on Unix. Robert wrote a shopping cart, and I wrote a new site\\ngenerator for stores \\x97 in Lisp, of course.  \\n  \\nWe were working out of Robert\\'s apartment in Cambridge. His roommate was away\\nfor big chunks of time, during which I got to sleep in his room. For some\\nreason there was no bed frame or sheets, just a mattress on the floor. One\\nmorning as I was lying on this mattress I had an idea that made me sit up like\\na capital L. What if we ran the software on the server, and let users control\\nit by clicking on links? Then we\\'d never have to write anything to run on\\nusers\\' computers. We could generate the sites on the same server we\\'d serve\\nthem from. Users wouldn\\'t need anything more than a browser.  \\n  \\nThis kind of software, known as a web app, is common now, but at the time it\\nwasn\\'t clear that it was even possible. To find out, we decided to try making\\na version of our store builder that you could control through the browser. A\\ncouple days later, on August 12, we had one that worked. The UI was horrible,\\nbut it proved you could build a whole store through the browser, without any\\nclient software or typing anything into the command line on the server.  \\n  \\nNow we felt like we were really onto something. I had visions of a whole new\\ngeneration of software working this way. You wouldn\\'t need versions, or ports,\\nor any of that crap. At Interleaf there had been a whole group called Release\\nEngineering that seemed to be at least as big as the group that actually wrote\\nthe software. Now you could just update the software right on the server.  \\n  \\nWe started a new company we called Viaweb, after the fact that our software\\nworked via the web, and we got $10,000 in seed funding from Idelle\\'s husband\\nJulian. In return for that and doing the initial legal work and giving us\\nbusiness advice, we gave him 10% of the company. Ten years later this deal\\nbecame the model for Y Combinator\\'s. We knew founders needed something like\\nthis, because we\\'d needed it ourselves.  \\n  \\nAt this stage I had a negative net worth, because the thousand dollars or so I\\nhad in the bank was more than counterbalanced by what I owed the government in\\ntaxes. (Had I diligently set aside the proper proportion of the money I\\'d made\\nconsulting for Interleaf? No, I had not.) So although Robert had his graduate\\nstudent stipend, I needed that seed funding to live on.  \\n  \\nWe originally hoped to launch in September, but we got more ambitious about\\nthe software as we worked on it. Eventually we managed to build a WYSIWYG site\\nbuilder, in the sense that as you were creating pages, they looked exactly\\nlike the static ones that would be generated later, except that instead of\\nleading to static pages, the links all referred to closures stored in a hash\\ntable on the server.  \\n  \\nIt helped to have studied art, because the main goal of an online store\\nbuilder is to make users look legit, and the key to looking legit is high\\nproduction values. If you get page layouts and fonts and colors right, you can\\nmake a guy running a store out of his bedroom look more legit than a big\\ncompany.  \\n  \\n(If you\\'re curious why my site looks so old-fashioned, it\\'s because it\\'s still\\nmade with this software. It may look clunky today, but in 1996 it was the last\\nword in slick.)  \\n  \\nIn September, Robert rebelled. \"We\\'ve been working on this for a month,\" he\\nsaid, \"and it\\'s still not done.\" This is funny in retrospect, because he would\\nstill be working on it almost 3 years later. But I decided it might be prudent\\nto recruit more programmers, and I asked Robert who else in grad school with\\nhim was really good. He recommended Trevor Blackwell, which surprised me at\\nfirst, because at that point I knew Trevor mainly for his plan to reduce\\neverything in his life to a stack of notecards, which he carried around with\\nhim. But Rtm was right, as usual. Trevor turned out to be a frighteningly\\neffective hacker.  \\n  \\nIt was a lot of fun working with Robert and Trevor. They\\'re the two most\\n[_independent-minded_](think.html) people I know, and in completely different\\nways. If you could see inside Rtm\\'s brain it would look like a colonial New\\nEngland church, and if you could see inside Trevor\\'s it would look like the\\nworst excesses of Austrian Rococo.  \\n  \\nWe opened for business, with 6 stores, in January 1996. It was just as well we\\nwaited a few months, because although we worried we were late, we were\\nactually almost fatally early. There was a lot of talk in the press then about\\necommerce, but not many people actually wanted online stores. [8]  \\n  \\nThere were three main parts to the software: the editor, which people used to\\nbuild sites and which I wrote, the shopping cart, which Robert wrote, and the\\nmanager, which kept track of orders and statistics, and which Trevor wrote. In\\nits time, the editor was one of the best general-purpose site builders. I kept\\nthe code tight and didn\\'t have to integrate with any other software except\\nRobert\\'s and Trevor\\'s, so it was quite fun to work on. If all I\\'d had to do\\nwas work on this software, the next 3 years would have been the easiest of my\\nlife. Unfortunately I had to do a lot more, all of it stuff I was worse at\\nthan programming, and the next 3 years were instead the most stressful.  \\n  \\nThere were a lot of startups making ecommerce software in the second half of\\nthe 90s. We were determined to be the Microsoft Word, not the Interleaf. Which\\nmeant being easy to use and inexpensive. It was lucky for us that we were\\npoor, because that caused us to make Viaweb even more inexpensive than we\\nrealized. We charged $100 a month for a small store and $300 a month for a big\\none. This low price was a big attraction, and a constant thorn in the sides of\\ncompetitors, but it wasn\\'t because of some clever insight that we set the\\nprice low. We had no idea what businesses paid for things. $300 a month seemed\\nlike a lot of money to us.  \\n  \\nWe did a lot of things right by accident like that. For example, we did what\\'s\\nnow called \"doing things that [_don\\'t scale_](ds.html),\" although at the time\\nwe would have described it as \"being so lame that we\\'re driven to the most\\ndesperate measures to get users.\" The most common of which was building stores\\nfor them. This seemed particularly humiliating, since the whole raison d\\'etre\\nof our software was that people could use it to make their own stores. But\\nanything to get users.  \\n  \\nWe learned a lot more about retail than we wanted to know. For example, that\\nif you could only have a small image of a man\\'s shirt (and all images were\\nsmall then by present standards), it was better to have a closeup of the\\ncollar than a picture of the whole shirt. The reason I remember learning this\\nwas that it meant I had to rescan about 30 images of men\\'s shirts. My first\\nset of scans were so beautiful too.  \\n  \\nThough this felt wrong, it was exactly the right thing to be doing. Building\\nstores for users taught us about retail, and about how it felt to use our\\nsoftware. I was initially both mystified and repelled by \"business\" and\\nthought we needed a \"business person\" to be in charge of it, but once we\\nstarted to get users, I was converted, in much the same way I was converted to\\n[_fatherhood_](kids.html) once I had kids. Whatever users wanted, I was all\\ntheirs. Maybe one day we\\'d have so many users that I couldn\\'t scan their\\nimages for them, but in the meantime there was nothing more important to do.  \\n  \\nAnother thing I didn\\'t get at the time is that [_growth rate_](growth.html) is\\nthe ultimate test of a startup. Our growth rate was fine. We had about 70\\nstores at the end of 1996 and about 500 at the end of 1997. I mistakenly\\nthought the thing that mattered was the absolute number of users. And that is\\nthe thing that matters in the sense that that\\'s how much money you\\'re making,\\nand if you\\'re not making enough, you might go out of business. But in the long\\nterm the growth rate takes care of the absolute number. If we\\'d been a startup\\nI was advising at Y Combinator, I would have said: Stop being so stressed out,\\nbecause you\\'re doing fine. You\\'re growing 7x a year. Just don\\'t hire too many\\nmore people and you\\'ll soon be profitable, and then you\\'ll control your own\\ndestiny.  \\n  \\nAlas I hired lots more people, partly because our investors wanted me to, and\\npartly because that\\'s what startups did during the Internet Bubble. A company\\nwith just a handful of employees would have seemed amateurish. So we didn\\'t\\nreach breakeven until about when Yahoo bought us in the summer of 1998. Which\\nin turn meant we were at the mercy of investors for the entire life of the\\ncompany. And since both we and our investors were noobs at startups, the\\nresult was a mess even by startup standards.  \\n  \\nIt was a huge relief when Yahoo bought us. In principle our Viaweb stock was\\nvaluable. It was a share in a business that was profitable and growing\\nrapidly. But it didn\\'t feel very valuable to me; I had no idea how to value a\\nbusiness, but I was all too keenly aware of the near-death experiences we\\nseemed to have every few months. Nor had I changed my grad student lifestyle\\nsignificantly since we started. So when Yahoo bought us it felt like going\\nfrom rags to riches. Since we were going to California, I bought a car, a\\nyellow 1998 VW GTI. I remember thinking that its leather seats alone were by\\nfar the most luxurious thing I owned.  \\n  \\nThe next year, from the summer of 1998 to the summer of 1999, must have been\\nthe least productive of my life. I didn\\'t realize it at the time, but I was\\nworn out from the effort and stress of running Viaweb. For a while after I got\\nto California I tried to continue my usual m.o. of programming till 3 in the\\nmorning, but fatigue combined with Yahoo\\'s prematurely aged\\n[_culture_](yahoo.html) and grim cube farm in Santa Clara gradually dragged me\\ndown. After a few months it felt disconcertingly like working at Interleaf.  \\n  \\nYahoo had given us a lot of options when they bought us. At the time I thought\\nYahoo was so overvalued that they\\'d never be worth anything, but to my\\nastonishment the stock went up 5x in the next year. I hung on till the first\\nchunk of options vested, then in the summer of 1999 I left. It had been so\\nlong since I\\'d painted anything that I\\'d half forgotten why I was doing this.\\nMy brain had been entirely full of software and men\\'s shirts for 4 years. But\\nI had done this to get rich so I could paint, I reminded myself, and now I was\\nrich, so I should go paint.  \\n  \\nWhen I said I was leaving, my boss at Yahoo had a long conversation with me\\nabout my plans. I told him all about the kinds of pictures I wanted to paint.\\nAt the time I was touched that he took such an interest in me. Now I realize\\nit was because he thought I was lying. My options at that point were worth\\nabout $2 million a month. If I was leaving that kind of money on the table, it\\ncould only be to go and start some new startup, and if I did, I might take\\npeople with me. This was the height of the Internet Bubble, and Yahoo was\\nground zero of it. My boss was at that moment a billionaire. Leaving then to\\nstart a new startup must have seemed to him an insanely, and yet also\\nplausibly, ambitious plan.  \\n  \\nBut I really was quitting to paint, and I started immediately. There was no\\ntime to lose. I\\'d already burned 4 years getting rich. Now when I talk to\\nfounders who are leaving after selling their companies, my advice is always\\nthe same: take a vacation. That\\'s what I should have done, just gone off\\nsomewhere and done nothing for a month or two, but the idea never occurred to\\nme.  \\n  \\nSo I tried to paint, but I just didn\\'t seem to have any energy or ambition.\\nPart of the problem was that I didn\\'t know many people in California. I\\'d\\ncompounded this problem by buying a house up in the Santa Cruz Mountains, with\\na beautiful view but miles from anywhere. I stuck it out for a few more\\nmonths, then in desperation I went back to New York, where unless you\\nunderstand about rent control you\\'ll be surprised to hear I still had my\\napartment, sealed up like a tomb of my old life. Idelle was in New York at\\nleast, and there were other people trying to paint there, even though I didn\\'t\\nknow any of them.  \\n  \\nWhen I got back to New York I resumed my old life, except now I was rich. It\\nwas as weird as it sounds. I resumed all my old patterns, except now there\\nwere doors where there hadn\\'t been. Now when I was tired of walking, all I had\\nto do was raise my hand, and (unless it was raining) a taxi would stop to pick\\nme up. Now when I walked past charming little restaurants I could go in and\\norder lunch. It was exciting for a while. Painting started to go better. I\\nexperimented with a new kind of still life where I\\'d paint one painting in the\\nold way, then photograph it and print it, blown up, on canvas, and then use\\nthat as the underpainting for a second still life, painted from the same\\nobjects (which hopefully hadn\\'t rotted yet).  \\n  \\nMeanwhile I looked for an apartment to buy. Now I could actually choose what\\nneighborhood to live in. Where, I asked myself and various real estate agents,\\nis the Cambridge of New York? Aided by occasional visits to actual Cambridge,\\nI gradually realized there wasn\\'t one. Huh.  \\n  \\nAround this time, in the spring of 2000, I had an idea. It was clear from our\\nexperience with Viaweb that web apps were the future. Why not build a web app\\nfor making web apps? Why not let people edit code on our server through the\\nbrowser, and then host the resulting applications for them? [9] You could run\\nall sorts of services on the servers that these applications could use just by\\nmaking an API call: making and receiving phone calls, manipulating images,\\ntaking credit card payments, etc.  \\n  \\nI got so excited about this idea that I couldn\\'t think about anything else. It\\nseemed obvious that this was the future. I didn\\'t particularly want to start\\nanother company, but it was clear that this idea would have to be embodied as\\none, so I decided to move to Cambridge and start it. I hoped to lure Robert\\ninto working on it with me, but there I ran into a hitch. Robert was now a\\npostdoc at MIT, and though he\\'d made a lot of money the last time I\\'d lured\\nhim into working on one of my schemes, it had also been a huge time sink. So\\nwhile he agreed that it sounded like a plausible idea, he firmly refused to\\nwork on it.  \\n  \\nHmph. Well, I\\'d do it myself then. I recruited Dan Giffin, who had worked for\\nViaweb, and two undergrads who wanted summer jobs, and we got to work trying\\nto build what it\\'s now clear is about twenty companies and several open source\\nprojects worth of software. The language for defining applications would of\\ncourse be a dialect of Lisp. But I wasn\\'t so naive as to assume I could spring\\nan overt Lisp on a general audience; we\\'d hide the parentheses, like Dylan\\ndid.  \\n  \\nBy then there was a name for the kind of company Viaweb was, an \"application\\nservice provider,\" or ASP. This name didn\\'t last long before it was replaced\\nby \"software as a service,\" but it was current for long enough that I named\\nthis new company after it: it was going to be called Aspra.  \\n  \\nI started working on the application builder, Dan worked on network\\ninfrastructure, and the two undergrads worked on the first two services\\n(images and phone calls). But about halfway through the summer I realized I\\nreally didn\\'t want to run a company \\x97 especially not a big one, which it was\\nlooking like this would have to be. I\\'d only started Viaweb because I needed\\nthe money. Now that I didn\\'t need money anymore, why was I doing this? If this\\nvision had to be realized as a company, then screw the vision. I\\'d build a\\nsubset that could be done as an open source project.  \\n  \\nMuch to my surprise, the time I spent working on this stuff was not wasted\\nafter all. After we started Y Combinator, I would often encounter startups\\nworking on parts of this new architecture, and it was very useful to have\\nspent so much time thinking about it and even trying to write some of it.  \\n  \\nThe subset I would build as an open source project was the new Lisp, whose\\nparentheses I now wouldn\\'t even have to hide. A lot of Lisp hackers dream of\\nbuilding a new Lisp, partly because one of the distinctive features of the\\nlanguage is that it has dialects, and partly, I think, because we have in our\\nminds a Platonic form of Lisp that all existing dialects fall short of. I\\ncertainly did. So at the end of the summer Dan and I switched to working on\\nthis new dialect of Lisp, which I called Arc, in a house I bought in\\nCambridge.  \\n  \\nThe following spring, lightning struck. I was invited to give a talk at a Lisp\\nconference, so I gave one about how we\\'d used Lisp at Viaweb. Afterward I put\\na postscript file of this talk online, on paulgraham.com, which I\\'d created\\nyears before using Viaweb but had never used for anything. In one day it got\\n30,000 page views. What on earth had happened? The referring urls showed that\\nsomeone had posted it on Slashdot. [10]  \\n  \\nWow, I thought, there\\'s an audience. If I write something and put it on the\\nweb, anyone can read it. That may seem obvious now, but it was surprising\\nthen. In the print era there was a narrow channel to readers, guarded by\\nfierce monsters known as editors. The only way to get an audience for anything\\nyou wrote was to get it published as a book, or in a newspaper or magazine.\\nNow anyone could publish anything.  \\n  \\nThis had been possible in principle since 1993, but not many people had\\nrealized it yet. I had been intimately involved with building the\\ninfrastructure of the web for most of that time, and a writer as well, and it\\nhad taken me 8 years to realize it. Even then it took me several years to\\nunderstand the implications. It meant there would be a whole new generation of\\n[_essays_](essay.html). [11]  \\n  \\nIn the print era, the channel for publishing essays had been vanishingly\\nsmall. Except for a few officially anointed thinkers who went to the right\\nparties in New York, the only people allowed to publish essays were\\nspecialists writing about their specialties. There were so many essays that\\nhad never been written, because there had been no way to publish them. Now\\nthey could be, and I was going to write them. [12]  \\n  \\nI\\'ve worked on several different things, but to the extent there was a turning\\npoint where I figured out what to work on, it was when I started publishing\\nessays online. From then on I knew that whatever else I did, I\\'d always write\\nessays too.  \\n  \\nI knew that online essays would be a [_marginal_](marginal.html) medium at\\nfirst. Socially they\\'d seem more like rants posted by nutjobs on their\\nGeoCities sites than the genteel and beautifully typeset compositions\\npublished in _The New Yorker_. But by this point I knew enough to find that\\nencouraging instead of discouraging.  \\n  \\nOne of the most conspicuous patterns I\\'ve noticed in my life is how well it\\nhas worked, for me at least, to work on things that weren\\'t prestigious. Still\\nlife has always been the least prestigious form of painting. Viaweb and Y\\nCombinator both seemed lame when we started them. I still get the glassy eye\\nfrom strangers when they ask what I\\'m writing, and I explain that it\\'s an\\nessay I\\'m going to publish on my web site. Even Lisp, though prestigious\\nintellectually in something like the way Latin is, also seems about as hip.  \\n  \\nIt\\'s not that unprestigious types of work are good per se. But when you find\\nyourself drawn to some kind of work despite its current lack of prestige, it\\'s\\na sign both that there\\'s something real to be discovered there, and that you\\nhave the right kind of motives. Impure motives are a big danger for the\\nambitious. If anything is going to lead you astray, it will be the desire to\\nimpress people. So while working on things that aren\\'t prestigious doesn\\'t\\nguarantee you\\'re on the right track, it at least guarantees you\\'re not on the\\nmost common type of wrong one.  \\n  \\nOver the next several years I wrote lots of essays about all kinds of\\ndifferent topics. O\\'Reilly reprinted a collection of them as a book, called\\n_Hackers & Painters_ after one of the essays in it. I also worked on spam\\nfilters, and did some more painting. I used to have dinners for a group of\\nfriends every thursday night, which taught me how to cook for groups. And I\\nbought another building in Cambridge, a former candy factory (and later, twas\\nsaid, porn studio), to use as an office.  \\n  \\nOne night in October 2003 there was a big party at my house. It was a clever\\nidea of my friend Maria Daniels, who was one of the thursday diners. Three\\nseparate hosts would all invite their friends to one party. So for every\\nguest, two thirds of the other guests would be people they didn\\'t know but\\nwould probably like. One of the guests was someone I didn\\'t know but would\\nturn out to like a lot: a woman called Jessica Livingston. A couple days later\\nI asked her out.  \\n  \\nJessica was in charge of marketing at a Boston investment bank. This bank\\nthought it understood startups, but over the next year, as she met friends of\\nmine from the startup world, she was surprised how different reality was. And\\nhow colorful their stories were. So she decided to compile a book of\\n[_interviews_](https://www.amazon.com/Founders-Work-Stories-Startups-\\nEarly/dp/1430210788) with startup founders.  \\n  \\nWhen the bank had financial problems and she had to fire half her staff, she\\nstarted looking for a new job. In early 2005 she interviewed for a marketing\\njob at a Boston VC firm. It took them weeks to make up their minds, and during\\nthis time I started telling her about all the things that needed to be fixed\\nabout venture capital. They should make a larger number of smaller investments\\ninstead of a handful of giant ones, they should be funding younger, more\\ntechnical founders instead of MBAs, they should let the founders remain as\\nCEO, and so on.  \\n  \\nOne of my tricks for writing essays had always been to give talks. The\\nprospect of having to stand up in front of a group of people and tell them\\nsomething that won\\'t waste their time is a great spur to the imagination. When\\nthe Harvard Computer Society, the undergrad computer club, asked me to give a\\ntalk, I decided I would tell them how to start a startup. Maybe they\\'d be able\\nto avoid the worst of the mistakes we\\'d made.  \\n  \\nSo I gave this talk, in the course of which I told them that the best sources\\nof seed funding were successful startup founders, because then they\\'d be\\nsources of advice too. Whereupon it seemed they were all looking expectantly\\nat me. Horrified at the prospect of having my inbox flooded by business plans\\n(if I\\'d only known), I blurted out \"But not me!\" and went on with the talk.\\nBut afterward it occurred to me that I should really stop procrastinating\\nabout angel investing. I\\'d been meaning to since Yahoo bought us, and now it\\nwas 7 years later and I still hadn\\'t done one angel investment.  \\n  \\nMeanwhile I had been scheming with Robert and Trevor about projects we could\\nwork on together. I missed working with them, and it seemed like there had to\\nbe something we could collaborate on.  \\n  \\nAs Jessica and I were walking home from dinner on March 11, at the corner of\\nGarden and Walker streets, these three threads converged. Screw the VCs who\\nwere taking so long to make up their minds. We\\'d start our own investment firm\\nand actually implement the ideas we\\'d been talking about. I\\'d fund it, and\\nJessica could quit her job and work for it, and we\\'d get Robert and Trevor as\\npartners too. [13]  \\n  \\nOnce again, ignorance worked in our favor. We had no idea how to be angel\\ninvestors, and in Boston in 2005 there were no Ron Conways to learn from. So\\nwe just made what seemed like the obvious choices, and some of the things we\\ndid turned out to be novel.  \\n  \\nThere are multiple components to Y Combinator, and we didn\\'t figure them all\\nout at once. The part we got first was to be an angel firm. In those days,\\nthose two words didn\\'t go together. There were VC firms, which were organized\\ncompanies with people whose job it was to make investments, but they only did\\nbig, million dollar investments. And there were angels, who did smaller\\ninvestments, but these were individuals who were usually focused on other\\nthings and made investments on the side. And neither of them helped founders\\nenough in the beginning. We knew how helpless founders were in some respects,\\nbecause we remembered how helpless we\\'d been. For example, one thing Julian\\nhad done for us that seemed to us like magic was to get us set up as a\\ncompany. We were fine writing fairly difficult software, but actually getting\\nincorporated, with bylaws and stock and all that stuff, how on earth did you\\ndo that? Our plan was not only to make seed investments, but to do for\\nstartups everything Julian had done for us.  \\n  \\nYC was not organized as a fund. It was cheap enough to run that we funded it\\nwith our own money. That went right by 99% of readers, but professional\\ninvestors are thinking \"Wow, that means they got all the returns.\" But once\\nagain, this was not due to any particular insight on our part. We didn\\'t know\\nhow VC firms were organized. It never occurred to us to try to raise a fund,\\nand if it had, we wouldn\\'t have known where to start. [14]  \\n  \\nThe most distinctive thing about YC is the batch model: to fund a bunch of\\nstartups all at once, twice a year, and then to spend three months focusing\\nintensively on trying to help them. That part we discovered by accident, not\\nmerely implicitly but explicitly due to our ignorance about investing. We\\nneeded to get experience as investors. What better way, we thought, than to\\nfund a whole bunch of startups at once? We knew undergrads got temporary jobs\\nat tech companies during the summer. Why not organize a summer program where\\nthey\\'d start startups instead? We wouldn\\'t feel guilty for being in a sense\\nfake investors, because they would in a similar sense be fake founders. So\\nwhile we probably wouldn\\'t make much money out of it, we\\'d at least get to\\npractice being investors on them, and they for their part would probably have\\na more interesting summer than they would working at Microsoft.  \\n  \\nWe\\'d use the building I owned in Cambridge as our headquarters. We\\'d all have\\ndinner there once a week \\x97 on tuesdays, since I was already cooking for the\\nthursday diners on thursdays \\x97 and after dinner we\\'d bring in experts on\\nstartups to give talks.  \\n  \\nWe knew undergrads were deciding then about summer jobs, so in a matter of\\ndays we cooked up something we called the Summer Founders Program, and I\\nposted an [_announcement_](summerfounder.html) on my site, inviting undergrads\\nto apply. I had never imagined that writing essays would be a way to get \"deal\\nflow,\" as investors call it, but it turned out to be the perfect source. [15]\\nWe got 225 applications for the Summer Founders Program, and we were surprised\\nto find that a lot of them were from people who\\'d already graduated, or were\\nabout to that spring. Already this SFP thing was starting to feel more serious\\nthan we\\'d intended.  \\n  \\nWe invited about 20 of the 225 groups to interview in person, and from those\\nwe picked 8 to fund. They were an impressive group. That first batch included\\nreddit, Justin Kan and Emmett Shear, who went on to found Twitch, Aaron\\nSwartz, who had already helped write the RSS spec and would a few years later\\nbecome a martyr for open access, and Sam Altman, who would later become the\\nsecond president of YC. I don\\'t think it was entirely luck that the first\\nbatch was so good. You had to be pretty bold to sign up for a weird thing like\\nthe Summer Founders Program instead of a summer job at a legit place like\\nMicrosoft or Goldman Sachs.  \\n  \\nThe deal for startups was based on a combination of the deal we did with\\nJulian ($10k for 10%) and what Robert said MIT grad students got for the\\nsummer ($6k). We invested $6k per founder, which in the typical two-founder\\ncase was $12k, in return for 6%. That had to be fair, because it was twice as\\ngood as the deal we ourselves had taken. Plus that first summer, which was\\nreally hot, Jessica brought the founders free air conditioners. [16]  \\n  \\nFairly quickly I realized that we had stumbled upon the way to scale startup\\nfunding. Funding startups in batches was more convenient for us, because it\\nmeant we could do things for a lot of startups at once, but being part of a\\nbatch was better for the startups too. It solved one of the biggest problems\\nfaced by founders: the isolation. Now you not only had colleagues, but\\ncolleagues who understood the problems you were facing and could tell you how\\nthey were solving them.  \\n  \\nAs YC grew, we started to notice other advantages of scale. The alumni became\\na tight community, dedicated to helping one another, and especially the\\ncurrent batch, whose shoes they remembered being in. We also noticed that the\\nstartups were becoming one another\\'s customers. We used to refer jokingly to\\nthe \"YC GDP,\" but as YC grows this becomes less and less of a joke. Now lots\\nof startups get their initial set of customers almost entirely from among\\ntheir batchmates.  \\n  \\nI had not originally intended YC to be a full-time job. I was going to do\\nthree things: hack, write essays, and work on YC. As YC grew, and I grew more\\nexcited about it, it started to take up a lot more than a third of my\\nattention. But for the first few years I was still able to work on other\\nthings.  \\n  \\nIn the summer of 2006, Robert and I started working on a new version of Arc.\\nThis one was reasonably fast, because it was compiled into Scheme. To test\\nthis new Arc, I wrote Hacker News in it. It was originally meant to be a news\\naggregator for startup founders and was called Startup News, but after a few\\nmonths I got tired of reading about nothing but startups. Plus it wasn\\'t\\nstartup founders we wanted to reach. It was future startup founders. So I\\nchanged the name to Hacker News and the topic to whatever engaged one\\'s\\nintellectual curiosity.  \\n  \\nHN was no doubt good for YC, but it was also by far the biggest source of\\nstress for me. If all I\\'d had to do was select and help founders, life would\\nhave been so easy. And that implies that HN was a mistake. Surely the biggest\\nsource of stress in one\\'s work should at least be something close to the core\\nof the work. Whereas I was like someone who was in pain while running a\\nmarathon not from the exertion of running, but because I had a blister from an\\nill-fitting shoe. When I was dealing with some urgent problem during YC, there\\nwas about a 60% chance it had to do with HN, and a 40% chance it had do with\\neverything else combined. [17]  \\n  \\nAs well as HN, I wrote all of YC\\'s internal software in Arc. But while I\\ncontinued to work a good deal _in_ Arc, I gradually stopped working _on_ Arc,\\npartly because I didn\\'t have time to, and partly because it was a lot less\\nattractive to mess around with the language now that we had all this\\ninfrastructure depending on it. So now my three projects were reduced to two:\\nwriting essays and working on YC.  \\n  \\nYC was different from other kinds of work I\\'ve done. Instead of deciding for\\nmyself what to work on, the problems came to me. Every 6 months there was a\\nnew batch of startups, and their problems, whatever they were, became our\\nproblems. It was very engaging work, because their problems were quite varied,\\nand the good founders were very effective. If you were trying to learn the\\nmost you could about startups in the shortest possible time, you couldn\\'t have\\npicked a better way to do it.  \\n  \\nThere were parts of the job I didn\\'t like. Disputes between cofounders,\\nfiguring out when people were lying to us, fighting with people who maltreated\\nthe startups, and so on. But I worked hard even at the parts I didn\\'t like. I\\nwas haunted by something Kevin Hale once said about companies: \"No one works\\nharder than the boss.\" He meant it both descriptively and prescriptively, and\\nit was the second part that scared me. I wanted YC to be good, so if how hard\\nI worked set the upper bound on how hard everyone else worked, I\\'d better work\\nvery hard.  \\n  \\nOne day in 2010, when he was visiting California for interviews, Robert Morris\\ndid something astonishing: he offered me unsolicited advice. I can only\\nremember him doing that once before. One day at Viaweb, when I was bent over\\ndouble from a kidney stone, he suggested that it would be a good idea for him\\nto take me to the hospital. That was what it took for Rtm to offer unsolicited\\nadvice. So I remember his exact words very clearly. \"You know,\" he said, \"you\\nshould make sure Y Combinator isn\\'t the last cool thing you do.\"  \\n  \\nAt the time I didn\\'t understand what he meant, but gradually it dawned on me\\nthat he was saying I should quit. This seemed strange advice, because YC was\\ndoing great. But if there was one thing rarer than Rtm offering advice, it was\\nRtm being wrong. So this set me thinking. It was true that on my current\\ntrajectory, YC would be the last thing I did, because it was only taking up\\nmore of my attention. It had already eaten Arc, and was in the process of\\neating essays too. Either YC was my life\\'s work or I\\'d have to leave\\neventually. And it wasn\\'t, so I would.  \\n  \\nIn the summer of 2012 my mother had a stroke, and the cause turned out to be a\\nblood clot caused by colon cancer. The stroke destroyed her balance, and she\\nwas put in a nursing home, but she really wanted to get out of it and back to\\nher house, and my sister and I were determined to help her do it. I used to\\nfly up to Oregon to visit her regularly, and I had a lot of time to think on\\nthose flights. On one of them I realized I was ready to hand YC over to\\nsomeone else.  \\n  \\nI asked Jessica if she wanted to be president, but she didn\\'t, so we decided\\nwe\\'d try to recruit Sam Altman. We talked to Robert and Trevor and we agreed\\nto make it a complete changing of the guard. Up till that point YC had been\\ncontrolled by the original LLC we four had started. But we wanted YC to last\\nfor a long time, and to do that it couldn\\'t be controlled by the founders. So\\nif Sam said yes, we\\'d let him reorganize YC. Robert and I would retire, and\\nJessica and Trevor would become ordinary partners.  \\n  \\nWhen we asked Sam if he wanted to be president of YC, initially he said no. He\\nwanted to start a startup to make nuclear reactors. But I kept at it, and in\\nOctober 2013 he finally agreed. We decided he\\'d take over starting with the\\nwinter 2014 batch. For the rest of 2013 I left running YC more and more to\\nSam, partly so he could learn the job, and partly because I was focused on my\\nmother, whose cancer had returned.  \\n  \\nShe died on January 15, 2014. We knew this was coming, but it was still hard\\nwhen it did.  \\n  \\nI kept working on YC till March, to help get that batch of startups through\\nDemo Day, then I checked out pretty completely. (I still talk to alumni and to\\nnew startups working on things I\\'m interested in, but that only takes a few\\nhours a week.)  \\n  \\nWhat should I do next? Rtm\\'s advice hadn\\'t included anything about that. I\\nwanted to do something completely different, so I decided I\\'d paint. I wanted\\nto see how good I could get if I really focused on it. So the day after I\\nstopped working on YC, I started painting. I was rusty and it took a while to\\nget back into shape, but it was at least completely engaging. [18]  \\n  \\nI spent most of the rest of 2014 painting. I\\'d never been able to work so\\nuninterruptedly before, and I got to be better than I had been. Not good\\nenough, but better. Then in November, right in the middle of a painting, I ran\\nout of steam. Up till that point I\\'d always been curious to see how the\\npainting I was working on would turn out, but suddenly finishing this one\\nseemed like a chore. So I stopped working on it and cleaned my brushes and\\nhaven\\'t painted since. So far anyway.  \\n  \\nI realize that sounds rather wimpy. But attention is a zero sum game. If you\\ncan choose what to work on, and you choose a project that\\'s not the best one\\n(or at least a good one) for you, then it\\'s getting in the way of another\\nproject that is. And at 50 there was some opportunity cost to screwing around.  \\n  \\nI started writing essays again, and wrote a bunch of new ones over the next\\nfew months. I even wrote a couple that [_weren\\'t_](know.html) about startups.\\nThen in March 2015 I started working on Lisp again.  \\n  \\nThe distinctive thing about Lisp is that its core is a language defined by\\nwriting an interpreter in itself. It wasn\\'t originally intended as a\\nprogramming language in the ordinary sense. It was meant to be a formal model\\nof computation, an alternative to the Turing machine. If you want to write an\\ninterpreter for a language in itself, what\\'s the minimum set of predefined\\noperators you need? The Lisp that John McCarthy invented, or more accurately\\ndiscovered, is an answer to that question. [19]  \\n  \\nMcCarthy didn\\'t realize this Lisp could even be used to program computers till\\nhis grad student Steve Russell suggested it. Russell translated McCarthy\\'s\\ninterpreter into IBM 704 machine language, and from that point Lisp started\\nalso to be a programming language in the ordinary sense. But its origins as a\\nmodel of computation gave it a power and elegance that other languages\\ncouldn\\'t match. It was this that attracted me in college, though I didn\\'t\\nunderstand why at the time.  \\n  \\nMcCarthy\\'s 1960 Lisp did nothing more than interpret Lisp expressions. It was\\nmissing a lot of things you\\'d want in a programming language. So these had to\\nbe added, and when they were, they weren\\'t defined using McCarthy\\'s original\\naxiomatic approach. That wouldn\\'t have been feasible at the time. McCarthy\\ntested his interpreter by hand-simulating the execution of programs. But it\\nwas already getting close to the limit of interpreters you could test that way\\n\\x97 indeed, there was a bug in it that McCarthy had overlooked. To test a more\\ncomplicated interpreter, you\\'d have had to run it, and computers then weren\\'t\\npowerful enough.  \\n  \\nNow they are, though. Now you could continue using McCarthy\\'s axiomatic\\napproach till you\\'d defined a complete programming language. And as long as\\nevery change you made to McCarthy\\'s Lisp was a discoveredness-preserving\\ntransformation, you could, in principle, end up with a complete language that\\nhad this quality. Harder to do than to talk about, of course, but if it was\\npossible in principle, why not try? So I decided to take a shot at it. It took\\n4 years, from March 26, 2015 to October 12, 2019. It was fortunate that I had\\na precisely defined goal, or it would have been hard to keep at it for so\\nlong.  \\n  \\nI wrote this new Lisp, called [_Bel_](bel.html), in itself in Arc. That may\\nsound like a contradiction, but it\\'s an indication of the sort of trickery I\\nhad to engage in to make this work. By means of an egregious collection of\\nhacks I managed to make something close enough to an interpreter written in\\nitself that could actually run. Not fast, but fast enough to test.  \\n  \\nI had to ban myself from writing essays during most of this time, or I\\'d never\\nhave finished. In late 2015 I spent 3 months writing essays, and when I went\\nback to working on Bel I could barely understand the code. Not so much because\\nit was badly written as because the problem is so convoluted. When you\\'re\\nworking on an interpreter written in itself, it\\'s hard to keep track of what\\'s\\nhappening at what level, and errors can be practically encrypted by the time\\nyou get them.  \\n  \\nSo I said no more essays till Bel was done. But I told few people about Bel\\nwhile I was working on it. So for years it must have seemed that I was doing\\nnothing, when in fact I was working harder than I\\'d ever worked on anything.\\nOccasionally after wrestling for hours with some gruesome bug I\\'d check\\nTwitter or HN and see someone asking \"Does Paul Graham still code?\"  \\n  \\nWorking on Bel was hard but satisfying. I worked on it so intensively that at\\nany given time I had a decent chunk of the code in my head and could write\\nmore there. I remember taking the boys to the coast on a sunny day in 2015 and\\nfiguring out how to deal with some problem involving continuations while I\\nwatched them play in the tide pools. It felt like I was doing life right. I\\nremember that because I was slightly dismayed at how novel it felt. The good\\nnews is that I had more moments like this over the next few years.  \\n  \\nIn the summer of 2016 we moved to England. We wanted our kids to see what it\\nwas like living in another country, and since I was a British citizen by\\nbirth, that seemed the obvious choice. We only meant to stay for a year, but\\nwe liked it so much that we still live there. So most of Bel was written in\\nEngland.  \\n  \\nIn the fall of 2019, Bel was finally finished. Like McCarthy\\'s original Lisp,\\nit\\'s a spec rather than an implementation, although like McCarthy\\'s Lisp it\\'s\\na spec expressed as code.  \\n  \\nNow that I could write essays again, I wrote a bunch about topics I\\'d had\\nstacked up. I kept writing essays through 2020, but I also started to think\\nabout other things I could work on. How should I choose what to do? Well, how\\nhad I chosen what to work on in the past? I wrote an essay for myself to\\nanswer that question, and I was surprised how long and messy the answer turned\\nout to be. If this surprised me, who\\'d lived it, then I thought perhaps it\\nwould be interesting to other people, and encouraging to those with similarly\\nmessy lives. So I wrote a more detailed version for others to read, and this\\nis the last sentence of it.  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n**Notes**  \\n  \\n[1] My experience skipped a step in the evolution of computers: time-sharing\\nmachines with interactive OSes. I went straight from batch processing to\\nmicrocomputers, which made microcomputers seem all the more exciting.  \\n  \\n[2] Italian words for abstract concepts can nearly always be predicted from\\ntheir English cognates (except for occasional traps like _polluzione_ ). It\\'s\\nthe everyday words that differ. So if you string together a lot of abstract\\nconcepts with a few simple verbs, you can make a little Italian go a long way.  \\n  \\n[3] I lived at Piazza San Felice 4, so my walk to the Accademia went straight\\ndown the spine of old Florence: past the Pitti, across the bridge, past\\nOrsanmichele, between the Duomo and the Baptistery, and then up Via Ricasoli\\nto Piazza San Marco. I saw Florence at street level in every possible\\ncondition, from empty dark winter evenings to sweltering summer days when the\\nstreets were packed with tourists.  \\n  \\n[4] You can of course paint people like still lives if you want to, and\\nthey\\'re willing. That sort of portrait is arguably the apex of still life\\npainting, though the long sitting does tend to produce pained expressions in\\nthe sitters.  \\n  \\n[5] Interleaf was one of many companies that had smart people and built\\nimpressive technology, and yet got crushed by Moore\\'s Law. In the 1990s the\\nexponential growth in the power of commodity (i.e. Intel) processors rolled up\\nhigh-end, special-purpose hardware and software companies like a bulldozer.  \\n  \\n[6] The signature style seekers at RISD weren\\'t specifically mercenary. In the\\nart world, money and coolness are tightly coupled. Anything expensive comes to\\nbe seen as cool, and anything seen as cool will soon become equally expensive.  \\n  \\n[7] Technically the apartment wasn\\'t rent-controlled but rent-stabilized, but\\nthis is a refinement only New Yorkers would know or care about. The point is\\nthat it was really cheap, less than half market price.  \\n  \\n[8] Most software you can launch as soon as it\\'s done. But when the software\\nis an online store builder and you\\'re hosting the stores, if you don\\'t have\\nany users yet, that fact will be painfully obvious. So before we could launch\\npublicly we had to launch privately, in the sense of recruiting an initial set\\nof users and making sure they had decent-looking stores.  \\n  \\n[9] We\\'d had a code editor in Viaweb for users to define their own page\\nstyles. They didn\\'t know it, but they were editing Lisp expressions\\nunderneath. But this wasn\\'t an app editor, because the code ran when the\\nmerchants\\' sites were generated, not when shoppers visited them.  \\n  \\n[10] This was the first instance of what is now a familiar experience, and so\\nwas what happened next, when I read the comments and found they were full of\\nangry people. How could I claim that Lisp was better than other languages?\\nWeren\\'t they all Turing complete? People who see the responses to essays I\\nwrite sometimes tell me how sorry they feel for me, but I\\'m not exaggerating\\nwhen I reply that it has always been like this, since the very beginning. It\\ncomes with the territory. An essay must tell readers things they [_don\\'t\\nalready know_](useful.html), and some people dislike being told such things.  \\n  \\n[11] People put plenty of stuff on the internet in the 90s of course, but\\nputting something online is not the same as publishing it online. Publishing\\nonline means you treat the online version as the (or at least a) primary\\nversion.  \\n  \\n[12] There is a general lesson here that our experience with Y Combinator also\\nteaches: Customs continue to constrain you long after the restrictions that\\ncaused them have disappeared. Customary VC practice had once, like the customs\\nabout publishing essays, been based on real constraints. Startups had once\\nbeen much more expensive to start, and proportionally rare. Now they could be\\ncheap and common, but the VCs\\' customs still reflected the old world, just as\\ncustoms about writing essays still reflected the constraints of the print era.  \\n  \\nWhich in turn implies that people who are independent-minded (i.e. less\\ninfluenced by custom) will have an advantage in fields affected by rapid\\nchange (where customs are more likely to be obsolete).  \\n  \\nHere\\'s an interesting point, though: you can\\'t always predict which fields\\nwill be affected by rapid change. Obviously software and venture capital will\\nbe, but who would have predicted that essay writing would be?  \\n  \\n[13] Y Combinator was not the original name. At first we were called Cambridge\\nSeed. But we didn\\'t want a regional name, in case someone copied us in Silicon\\nValley, so we renamed ourselves after one of the coolest tricks in the lambda\\ncalculus, the Y combinator.  \\n  \\nI picked orange as our color partly because it\\'s the warmest, and partly\\nbecause no VC used it. In 2005 all the VCs used staid colors like maroon, navy\\nblue, and forest green, because they were trying to appeal to LPs, not\\nfounders. The YC logo itself is an inside joke: the Viaweb logo had been a\\nwhite V on a red circle, so I made the YC logo a white Y on an orange square.  \\n  \\n[14] YC did become a fund for a couple years starting in 2009, because it was\\ngetting so big I could no longer afford to fund it personally. But after\\nHeroku got bought we had enough money to go back to being self-funded.  \\n  \\n[15] I\\'ve never liked the term \"deal flow,\" because it implies that the number\\nof new startups at any given time is fixed. This is not only false, but it\\'s\\nthe purpose of YC to falsify it, by causing startups to be founded that would\\nnot otherwise have existed.  \\n  \\n[16] She reports that they were all different shapes and sizes, because there\\nwas a run on air conditioners and she had to get whatever she could, but that\\nthey were all heavier than she could carry now.  \\n  \\n[17] Another problem with HN was a bizarre edge case that occurs when you both\\nwrite essays and run a forum. When you run a forum, you\\'re assumed to see if\\nnot every conversation, at least every conversation involving you. And when\\nyou write essays, people post highly imaginative misinterpretations of them on\\nforums. Individually these two phenomena are tedious but bearable, but the\\ncombination is disastrous. You actually have to respond to the\\nmisinterpretations, because the assumption that you\\'re present in the\\nconversation means that not responding to any sufficiently upvoted\\nmisinterpretation reads as a tacit admission that it\\'s correct. But that in\\nturn encourages more; anyone who wants to pick a fight with you senses that\\nnow is their chance.  \\n  \\n[18] The worst thing about leaving YC was not working with Jessica anymore.\\nWe\\'d been working on YC almost the whole time we\\'d known each other, and we\\'d\\nneither tried nor wanted to separate it from our personal lives, so leaving\\nwas like pulling up a deeply rooted tree.  \\n  \\n[19] One way to get more precise about the concept of invented vs discovered\\nis to talk about space aliens. Any sufficiently advanced alien civilization\\nwould certainly know about the Pythagorean theorem, for example. I believe,\\nthough with less certainty, that they would also know about the Lisp in\\nMcCarthy\\'s 1960 paper.  \\n  \\nBut if so there\\'s no reason to suppose that this is the limit of the language\\nthat might be known to them. Presumably aliens need numbers and errors and I/O\\ntoo. So it seems likely there exists at least one path out of McCarthy\\'s Lisp\\nalong which discoveredness is preserved.  \\n  \\n  \\n  \\n **Thanks** to Trevor Blackwell, John Collison, Patrick Collison, Daniel\\nGackle, Ralph Hazell, Jessica Livingston, Robert Morris, and Harj Taggar for\\nreading drafts of this.  \\n  \\n  \\n---  \\n  \\n  \\n\\n* * *  \\n  \\n---\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "\n",
        "documents = SimpleWebPageReader(\n",
        "    html_to_text=True\n",
        ").load_data([\"http://paulgraham.com/worked.html\"])\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "42abd41bb549421f8680517f97b03b8f",
            "20552da6c2b8436cbba7b3ce6338b295",
            "5ecef3a5be5c4941ab0d4b34f7e5fa77",
            "ef1e644267134c718ad62bb53d0c63f1",
            "359486ce32b047b4a34dcf42e0b81004",
            "5b0048f4b9124784b5e767bcd7533683",
            "e1741a85a9c74025a83cb815a694eed6",
            "69c8a60a92f24832bd582dcf78738129",
            "7e5804d74c1e4333b9f95122fb8c96cc",
            "9b812c97e5934475b3316dada1fd992e",
            "38773a9b07e24e0789412377b84518b7",
            "cc2842c56d6d4fffb6473dd24e453e5f",
            "ec0ab4f5547a46cd85998addd008cd64",
            "0db6fb29f4f64118ae9b9d6e81e15ef0",
            "5fe8bc5f962a49b8a9278fa6cdadcc21",
            "dfddcccae608466980b729effdd117f0",
            "8a7ffefe09db42a0bd028109a64909a6",
            "0fa685503e964fafadde5abcf202bb1e",
            "13931b3e193749009caa7ef4ccea7967",
            "06bbd7e875f346958667df732f7cf574",
            "8cde791ada994503ad415364d6d33890",
            "51d5bafd273b4c7bbe169f8d27462285"
          ]
        },
        "id": "tr7YetUgAeMn",
        "outputId": "21004cf0-2a79-4bae-fd70-ba616f504c1d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f6effac4c70493cbf66aa8f0c7f9ab0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5972ce293484a87bdb33b56fb79d131",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings:   0%|          | 0/23 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from llama_index import VectorStoreIndex, StorageContext, ServiceContext\n",
        "from llama_index.embeddings import GeminiEmbedding\n",
        "from llama_index.llms import Gemini\n",
        "from llama_index.vector_stores import QdrantVectorStore\n",
        "import qdrant_client\n",
        "# from llama_index.vector_stores import ChromaVectorStore\n",
        "# import chromadb\n",
        "from llama_index import StorageContext\n",
        "\n",
        "# Create a local Qdrant vector store\n",
        "client = qdrant_client.QdrantClient(path=\"qdrant_gemini_3\")\n",
        "\n",
        "vector_store = QdrantVectorStore(client=client, collection_name=\"collection\")\n",
        "\n",
        "# # initialize client, setting path to save data\n",
        "# db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# # create collection\n",
        "# chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "\n",
        "# # assign chroma as the vector_store to the context\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Using the embedding model to Gemini\n",
        "embed_model = GeminiEmbedding(\n",
        "    model_name=\"models/embedding-001\", api_key=GOOGLE_API_KEY\n",
        ")\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=Gemini(api_key=GOOGLE_API_KEY), embed_model=embed_model\n",
        ")\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    service_context=service_context,\n",
        "    storage_context=storage_context,\n",
        "    show_progress=True,\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1xzZiw1SEBu"
      },
      "source": [
        "### Send your first request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ZCi6n6xoSEBv",
        "outputId": "82ecc05a-5597-49a7-a9af-f29b38d52c49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The author initially studied philosophy in college but found it boring and switched to AI. They then pursued a PhD in computer science at Harvard while also taking art classes. After completing their PhD, they applied to art schools and was accepted into the BFA program at RISD. They also received an invitation to take the entrance exam at the Accademia di Belli Arti in Florence, which they passed. However, the author ultimately decided to attend RISD.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"What does the author say about their education?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "12GcVxvmPucp",
        "outputId": "7ff9e226-36d3-48a1-e4d5-9de64047dd02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Harvard, RISD, and Accademia di Belli Arti\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"Where did the author go to school?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "NE4h0I0HP6cr",
        "outputId": "d6c87911-7a26-4a95-d750-b677ae8fb972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The provided context does not mention the author's Harvard PhD advisor, so I cannot answer this question.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"Who was the author's Harvard PhD advisor?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "vSSZLBHFQuKX",
        "outputId": "f7a988ee-0bb0-4156-d656-8b198d14c174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tom Cheatham was the author's advisor in the PhD program in computer science at Harvard.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"who was Tom Cheatham to the author?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "7rqlmfTfQ7a3",
        "outputId": "231a795a-a8c3-4f03-ed5e-5c588d93acfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tom Cheatham is a professor at Harvard University. He is mentioned in the story because the narrator was taking art classes at Harvard while pursuing a PhD in computer science. Tom Cheatham was the narrator's advisor, and he was very easy-going about the narrator's choice to take art classes. He never said anything about it, even though it was unusual for a PhD student to be taking art classes.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"who is Tom? why is he in this story?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "e7NHvID2Sm_A",
        "outputId": "0cf35819-cd40-4348-8ca1-61fb37c4230e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This story is about the author's journey from being a philosophy student to becoming an AI researcher. The author initially believed that philosophy was the study of ultimate truths, but later realized that it was mostly concerned with edge cases that other fields ignored. The author then switched to AI, inspired by a novel and a documentary. However, the author later realized that the AI research at the time was a hoax, as it was limited to a small subset of formal language and could not truly understand natural language.\n",
            "\n",
            "The most important things the author wants the reader to learn are:\n",
            "- The limitations of AI research at the time, particularly the inability to truly understand natural language.\n",
            "- The importance of having realistic expectations about what AI can achieve.\n",
            "- The need for a new approach to AI research that goes beyond the traditional methods of representing concepts with explicit data structures.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"what is this story about?  what are the most important things the author want the reader to learn?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cMfasabSEBv"
      },
      "source": [
        "## Initialize Feedback Function(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gEMHas7SEBv",
        "outputId": "912e9f1a-81db-4365-ef76-2fe69ca37699"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… In groundedness_measure_with_cot_reasons, input source will be set to __record__.app.query.rets.source_nodes[:].node.text.collect() .\n",
            "âœ… In groundedness_measure_with_cot_reasons, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "âœ… In relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "âœ… In relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "âœ… In qs_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
            "âœ… In qs_relevance, input statement will be set to __record__.app.query.rets.source_nodes[:].node.text .\n"
          ]
        }
      ],
      "source": [
        "from trulens_eval import Feedback, TruLlama\n",
        "from trulens_eval.feedback import Groundedness\n",
        "from trulens_eval import LiteLLM\n",
        "import numpy as np\n",
        "\n",
        "# import litellm\n",
        "# litellm.set_verbose=True\n",
        "\n",
        "# Initialize provider class\n",
        "gemini_provider = LiteLLM(model_engine=\"gemini-pro\")\n",
        "\n",
        "grounded = Groundedness(groundedness_provider=gemini_provider)\n",
        "\n",
        "# Define a groundedness feedback function\n",
        "f_groundedness = Feedback(grounded.groundedness_measure_with_cot_reasons).on(\n",
        "    TruLlama.select_source_nodes().node.text.collect()\n",
        "    ).on_output(\n",
        "    ).aggregate(grounded.grounded_statements_aggregator)\n",
        "\n",
        "# Question/answer relevance between overall question and answer.\n",
        "f_qa_relevance = Feedback(gemini_provider.relevance).on_input_output()\n",
        "\n",
        "# Question/statement relevance between question and each context chunk.\n",
        "f_qs_relevance = Feedback(gemini_provider.qs_relevance).on_input().on(\n",
        "    TruLlama.select_source_nodes().node.text\n",
        "    ).aggregate(np.mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY4Lw8wmSEBv"
      },
      "source": [
        "## Instrument app for logging with TruLens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CwW2IJ8XSEBv"
      },
      "outputs": [],
      "source": [
        "tru_query_engine_recorder = TruLlama(query_engine,\n",
        "    app_id='LlamaIndex_App1',\n",
        "    feedbacks=[f_groundedness, f_qa_relevance, f_qs_relevance])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mw-HUSugSEBv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The author dropped AI because they realized that the way AI was practiced at the time was a hoax. They believed that the whole way of doing AI, with explicit data structures representing concepts, was not going to work and would never lead to the creation of truly intelligent machines like Mike from the novel _The Moon is a Harsh Mistress_.\n",
            "kwargs[caching]: False; litellm.cache: None\n",
            "\n",
            "LiteLLM completion() model= gemini-pro; provider = vertex_ai\n",
            "\n",
            "LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stop': None, 'max_tokens': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'model': 'gemini-pro', 'custom_llm_provider': 'vertex_ai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None}\n",
            "\n",
            "LiteLLM: Non-Default params passed to completion() {}\n",
            "self.optional_params: {}\n",
            "kwargs[caching]: False; litellm.cache: None\n",
            "\n",
            "LiteLLM completion() model= gemini-pro; provider = vertex_ai\n",
            "\n",
            "LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stop': None, 'max_tokens': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'model': 'gemini-pro', 'custom_llm_provider': 'vertex_ai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None}\n",
            "\n",
            "LiteLLM: Non-Default params passed to completion() {}\n",
            "self.optional_params: {}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kwargs[caching]: False; litellm.cache: None\n",
            "\n",
            "LiteLLM completion() model= gemini-pro; provider = vertex_ai\n",
            "\n",
            "LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stop': None, 'max_tokens': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'model': 'gemini-pro', 'custom_llm_provider': 'vertex_ai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None}\n",
            "\n",
            "LiteLLM: Non-Default params passed to completion() {}\n",
            "self.optional_params: {}\n",
            "PRE-API-CALL ADDITIONAL ARGS: {'complete_input_dict': {}, 'request_str': 'llm_model = GenerativeModel(gemini-pro)\\nchat = llm_model.start_chat()\\nchat.send_message(You are a INFORMATION OVERLAP classifier providing the overlap of information between a SOURCE and STATEMENT.\\nFor every sentence in the statement, please answer with this template:\\n\\nTEMPLATE: \\nStatement Sentence: <Sentence>, \\nSupporting Evidence: <Choose the exact unchanged sentences in the source that can answer the statement, if nothing matches, say NOTHING FOUND>\\nScore: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\nGive me the INFORMATION OVERLAP of this SOURCE and STATEMENT.\\n\\nSOURCE: [\\'Though I liked programming, I didn\\\\\\'t plan to study it in college. In college I\\\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\\\nmy naive high school self, to be the study of the ultimate truths, compared to\\\\nwhich the things studied in other fields would be mere domain knowledge. What\\\\nI discovered when I got to college was that the other fields took up so much\\\\nof the space of ideas that there wasn\\\\\\'t much left for these supposed ultimate\\\\ntruths. All that seemed left for philosophy were edge cases that people in\\\\nother fields felt could safely be ignored.  \\\\n  \\\\nI couldn\\\\\\'t have put this into words when I was 18. All I knew at the time was\\\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\\\nto switch to AI.  \\\\n  \\\\nAI was in the air in the mid 1980s, but there were two things especially that\\\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\\\\\'t tried rereading\\\\n_The Moon is a Harsh Mistress_ , so I don\\\\\\'t know how well it has aged, but\\\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\\\ntime before we\\\\\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\\\nlike that time would be a few years at most. All you had to do was teach\\\\nSHRDLU more words.  \\\\n  \\\\nThere weren\\\\\\'t any classes in AI at Cornell then, not even graduate classes, so\\\\nI started trying to teach myself. Which meant learning Lisp, since in those\\\\ndays Lisp was regarded as the language of AI. The commonly used programming\\\\nlanguages then were pretty primitive, and programmers\\\\\\' ideas correspondingly\\\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\\\na program so fast that it was years before I started to have a sense of where\\\\nthe new limits were. This was more like it; this was what I had expected\\\\ncollege to do. It wasn\\\\\\'t happening in a class, like it was supposed to, but\\\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\\\ngoing to do.  \\\\n  \\\\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\\\nworking on that program. It was a pleasing bit of code, but what made it even\\\\nmore exciting was my belief \\\\x97 hard to imagine now, but not unique in 1985 \\\\x97\\\\nthat it was already climbing the lower slopes of intelligence.  \\\\n  \\\\nI had gotten into a program at Cornell that didn\\\\\\'t make you choose a major.\\\\nYou could take whatever classes you liked, and choose whatever you liked to\\\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\\\n  \\\\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\\\ntime, and Harvard, which I\\\\\\'d visited because Rich Draves went there, and was\\\\nalso home to Bill Woods, who\\\\\\'d invented the type of parser I used in my SHRDLU\\\\nclone. Only Harvard accepted me, so that was where I went.  \\\\n  \\\\nI don\\\\\\'t remember the moment it happened, or if there even was a specific\\\\nmoment, but during the first year of grad school I realized that AI, as\\\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.\\', \\'By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.  \\\\n  \\\\nSo I looked around to see what I could salvage from the wreckage of my plans,\\\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\\\nown sake and not just for its association with AI, even though that was the\\\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\\\nIn fact, I decided to write a book about Lisp hacking. It\\\\\\'s scary to think how\\\\nlittle I knew about Lisp hacking when I started writing that book. But there\\\\\\'s\\\\nnothing like writing a book about something to help you learn it. The book,\\\\n_On Lisp_ , wasn\\\\\\'t published till 1993, but I wrote much of it in grad school.  \\\\n  \\\\nComputer Science is an uneasy alliance between two halves, theory and systems.\\\\nThe theory people prove things, and the systems people build things. I wanted\\\\nto build things. I had plenty of respect for theory \\\\x97 indeed, a sneaking\\\\nsuspicion that it was the more admirable of the two halves \\\\x97 but building\\\\nthings seemed so much more exciting.  \\\\n  \\\\nThe problem with systems work, though, was that it didn\\\\\\'t last. Any program\\\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\\\nbest. People might mention your software in footnotes, but no one would\\\\nactually use it. And indeed, it would seem very feeble work. Only people with\\\\na sense of the history of the field would even realize that, in its time, it\\\\nhad been good.  \\\\n  \\\\nThere were some surplus Xerox Dandelions floating around the computer lab at\\\\none point. Anyone who wanted one to play around with could have one. I was\\\\nbriefly tempted, but they were so slow by present standards; what was the\\\\npoint? No one else wanted one either, so off they went. That was what happened\\\\nto systems work.  \\\\n  \\\\nI wanted not just to build things, but to build things that would last.  \\\\n  \\\\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\\\nI\\\\\\'d spent a lot of time as a kid. While looking at a painting there I realized\\\\nsomething that might seem obvious, but was a big surprise to me. There, right\\\\non the wall, was something you could make that would last. Paintings didn\\\\\\'t\\\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\\\n  \\\\nAnd moreover this was something you could make a living doing. Not as easily\\\\nas you could by writing software, of course, but I thought if you were really\\\\nindustrious and lived really cheaply, it had to be possible to make enough to\\\\nsurvive. And as an artist you could be truly independent. You wouldn\\\\\\'t have a\\\\nboss, or even need to get research funding.  \\\\n  \\\\nI had always liked looking at paintings. Could I make them? I had no idea. I\\\\\\'d\\\\nnever imagined it was even possible. I knew intellectually that people made\\\\nart \\\\x97 that it didn\\\\\\'t just appear spontaneously \\\\x97 but it was as if the people\\\\nwho made it were a different species. They either lived long ago or were\\\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\\\nidea of actually being able to make art, to put that verb before that noun,\\\\nseemed almost miraculous.  \\\\n  \\\\nThat fall I started taking art classes at Harvard. Grad students could take\\\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\\\nIf he even knew about the strange classes I was taking, he never said\\\\nanything.  \\\\n  \\\\nSo now I was in a PhD program in computer science, yet planning to be an\\\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\\\nLisp_.\\']\\n\\nSTATEMENT: The author dropped AI because they realized that the way AI was practiced at the time was a hoax. They believed that the whole way of doing AI, with explicit data structures representing concepts, was not going to work and would never lead to the creation of truly intelligent machines like Mike from the novel _The Moon is a Harsh Mistress_.\\n, generation_config=GenerationConfig(**{}), safety_settings=None).text\\n'}\n",
            "\u001b[92m\n",
            "Request Sent from LiteLLM:\n",
            "llm_model = GenerativeModel(gemini-pro)\n",
            "chat = llm_model.start_chat()\n",
            "chat.send_message(You are a INFORMATION OVERLAP classifier providing the overlap of information between a SOURCE and STATEMENT.\n",
            "For every sentence in the statement, please answer with this template:\n",
            "\n",
            "TEMPLATE: \n",
            "Statement Sentence: <Sentence>, \n",
            "Supporting Evidence: <Choose the exact unchanged sentences in the source that can answer the statement, if nothing matches, say NOTHING FOUND>\n",
            "Score: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\n",
            "Give me the INFORMATION OVERLAP of this SOURCE and STATEMENT.\n",
            "\n",
            "SOURCE: ['Though I liked programming, I didn\\'t plan to study it in college. In college I\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\nmy naive high school self, to be the study of the ultimate truths, compared to\\nwhich the things studied in other fields would be mere domain knowledge. What\\nI discovered when I got to college was that the other fields took up so much\\nof the space of ideas that there wasn\\'t much left for these supposed ultimate\\ntruths. All that seemed left for philosophy were edge cases that people in\\nother fields felt could safely be ignored.  \\n  \\nI couldn\\'t have put this into words when I was 18. All I knew at the time was\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\nto switch to AI.  \\n  \\nAI was in the air in the mid 1980s, but there were two things especially that\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading\\n_The Moon is a Harsh Mistress_ , so I don\\'t know how well it has aged, but\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\ntime before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\nlike that time would be a few years at most. All you had to do was teach\\nSHRDLU more words.  \\n  \\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so\\nI started trying to teach myself. Which meant learning Lisp, since in those\\ndays Lisp was regarded as the language of AI. The commonly used programming\\nlanguages then were pretty primitive, and programmers\\' ideas correspondingly\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\na program so fast that it was years before I started to have a sense of where\\nthe new limits were. This was more like it; this was what I had expected\\ncollege to do. It wasn\\'t happening in a class, like it was supposed to, but\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\ngoing to do.  \\n  \\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\nworking on that program. It was a pleasing bit of code, but what made it even\\nmore exciting was my belief \\x97 hard to imagine now, but not unique in 1985 \\x97\\nthat it was already climbing the lower slopes of intelligence.  \\n  \\nI had gotten into a program at Cornell that didn\\'t make you choose a major.\\nYou could take whatever classes you liked, and choose whatever you liked to\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\n  \\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\ntime, and Harvard, which I\\'d visited because Rich Draves went there, and was\\nalso home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU\\nclone. Only Harvard accepted me, so that was where I went.  \\n  \\nI don\\'t remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.', 'By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.  \\n  \\nSo I looked around to see what I could salvage from the wreckage of my plans,\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\nown sake and not just for its association with AI, even though that was the\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\nIn fact, I decided to write a book about Lisp hacking. It\\'s scary to think how\\nlittle I knew about Lisp hacking when I started writing that book. But there\\'s\\nnothing like writing a book about something to help you learn it. The book,\\n_On Lisp_ , wasn\\'t published till 1993, but I wrote much of it in grad school.  \\n  \\nComputer Science is an uneasy alliance between two halves, theory and systems.\\nThe theory people prove things, and the systems people build things. I wanted\\nto build things. I had plenty of respect for theory \\x97 indeed, a sneaking\\nsuspicion that it was the more admirable of the two halves \\x97 but building\\nthings seemed so much more exciting.  \\n  \\nThe problem with systems work, though, was that it didn\\'t last. Any program\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\nbest. People might mention your software in footnotes, but no one would\\nactually use it. And indeed, it would seem very feeble work. Only people with\\na sense of the history of the field would even realize that, in its time, it\\nhad been good.  \\n  \\nThere were some surplus Xerox Dandelions floating around the computer lab at\\none point. Anyone who wanted one to play around with could have one. I was\\nbriefly tempted, but they were so slow by present standards; what was the\\npoint? No one else wanted one either, so off they went. That was what happened\\nto systems work.  \\n  \\nI wanted not just to build things, but to build things that would last.  \\n  \\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\nI\\'d spent a lot of time as a kid. While looking at a painting there I realized\\nsomething that might seem obvious, but was a big surprise to me. There, right\\non the wall, was something you could make that would last. Paintings didn\\'t\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\n  \\nAnd moreover this was something you could make a living doing. Not as easily\\nas you could by writing software, of course, but I thought if you were really\\nindustrious and lived really cheaply, it had to be possible to make enough to\\nsurvive. And as an artist you could be truly independent. You wouldn\\'t have a\\nboss, or even need to get research funding.  \\n  \\nI had always liked looking at paintings. Could I make them? I had no idea. I\\'d\\nnever imagined it was even possible. I knew intellectually that people made\\nart \\x97 that it didn\\'t just appear spontaneously \\x97 but it was as if the people\\nwho made it were a different species. They either lived long ago or were\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\nidea of actually being able to make art, to put that verb before that noun,\\nseemed almost miraculous.  \\n  \\nThat fall I started taking art classes at Harvard. Grad students could take\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\nIf he even knew about the strange classes I was taking, he never said\\nanything.  \\n  \\nSo now I was in a PhD program in computer science, yet planning to be an\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\nLisp_.']\n",
            "\n",
            "STATEMENT: The author dropped AI because they realized that the way AI was practiced at the time was a hoax. They believed that the whole way of doing AI, with explicit data structures representing concepts, was not going to work and would never lead to the creation of truly intelligent machines like Mike from the novel _The Moon is a Harsh Mistress_.\n",
            ", generation_config=GenerationConfig(**{}), safety_settings=None).text\n",
            "\u001b[0m\n",
            "\n",
            "PRE-API-CALL ADDITIONAL ARGS: {'complete_input_dict': {}, 'request_str': 'llm_model = GenerativeModel(gemini-pro)\\nchat = llm_model.start_chat()\\nchat.send_message(You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: Why did the author drop AI?\\n\\nSTATEMENT: Though I liked programming, I didn\\'t plan to study it in college. In college I\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\nmy naive high school self, to be the study of the ultimate truths, compared to\\nwhich the things studied in other fields would be mere domain knowledge. What\\nI discovered when I got to college was that the other fields took up so much\\nof the space of ideas that there wasn\\'t much left for these supposed ultimate\\ntruths. All that seemed left for philosophy were edge cases that people in\\nother fields felt could safely be ignored.  \\n  \\nI couldn\\'t have put this into words when I was 18. All I knew at the time was\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\nto switch to AI.  \\n  \\nAI was in the air in the mid 1980s, but there were two things especially that\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading\\n_The Moon is a Harsh Mistress_ , so I don\\'t know how well it has aged, but\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\ntime before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\nlike that time would be a few years at most. All you had to do was teach\\nSHRDLU more words.  \\n  \\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so\\nI started trying to teach myself. Which meant learning Lisp, since in those\\ndays Lisp was regarded as the language of AI. The commonly used programming\\nlanguages then were pretty primitive, and programmers\\' ideas correspondingly\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\na program so fast that it was years before I started to have a sense of where\\nthe new limits were. This was more like it; this was what I had expected\\ncollege to do. It wasn\\'t happening in a class, like it was supposed to, but\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\ngoing to do.  \\n  \\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\nworking on that program. It was a pleasing bit of code, but what made it even\\nmore exciting was my belief \\x97 hard to imagine now, but not unique in 1985 \\x97\\nthat it was already climbing the lower slopes of intelligence.  \\n  \\nI had gotten into a program at Cornell that didn\\'t make you choose a major.\\nYou could take whatever classes you liked, and choose whatever you liked to\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\n  \\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\ntime, and Harvard, which I\\'d visited because Rich Draves went there, and was\\nalso home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU\\nclone. Only Harvard accepted me, so that was where I went.  \\n  \\nI don\\'t remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.\\n\\nRELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\\n'}\n",
            "\u001b[92m\n",
            "Request Sent from LiteLLM:\n",
            "llm_model = GenerativeModel(gemini-pro)\n",
            "chat = llm_model.start_chat()\n",
            "chat.send_message(You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\n",
            "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
            "\n",
            "A few additional scoring guidelines:\n",
            "\n",
            "- Long STATEMENTS should score equally well as short STATEMENTS.\n",
            "\n",
            "- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\n",
            "\n",
            "- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\n",
            "\n",
            "- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
            "\n",
            "- Answers that intentionally do not answer the question, such as 'I don't know', should also be counted as the most relevant.\n",
            "\n",
            "- Never elaborate.\n",
            "\n",
            "QUESTION: Why did the author drop AI?\n",
            "\n",
            "STATEMENT: Though I liked programming, I didn't plan to study it in college. In college I\n",
            "was going to study philosophy, which sounded much more powerful. It seemed, to\n",
            "my naive high school self, to be the study of the ultimate truths, compared to\n",
            "which the things studied in other fields would be mere domain knowledge. What\n",
            "I discovered when I got to college was that the other fields took up so much\n",
            "of the space of ideas that there wasn't much left for these supposed ultimate\n",
            "truths. All that seemed left for philosophy were edge cases that people in\n",
            "other fields felt could safely be ignored.  \n",
            "  \n",
            "I couldn't have put this into words when I was 18. All I knew at the time was\n",
            "that I kept taking philosophy courses and they kept being boring. So I decided\n",
            "to switch to AI.  \n",
            "  \n",
            "AI was in the air in the mid 1980s, but there were two things especially that\n",
            "made me want to work on it: a novel by Heinlein called _The Moon is a Harsh\n",
            "Mistress_ , which featured an intelligent computer called Mike, and a PBS\n",
            "documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading\n",
            "_The Moon is a Harsh Mistress_ , so I don't know how well it has aged, but\n",
            "when I read it I was drawn entirely into its world. It seemed only a matter of\n",
            "time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed\n",
            "like that time would be a few years at most. All you had to do was teach\n",
            "SHRDLU more words.  \n",
            "  \n",
            "There weren't any classes in AI at Cornell then, not even graduate classes, so\n",
            "I started trying to teach myself. Which meant learning Lisp, since in those\n",
            "days Lisp was regarded as the language of AI. The commonly used programming\n",
            "languages then were pretty primitive, and programmers' ideas correspondingly\n",
            "so. The default language at Cornell was a Pascal-like language called PL/I,\n",
            "and the situation was similar elsewhere. Learning Lisp expanded my concept of\n",
            "a program so fast that it was years before I started to have a sense of where\n",
            "the new limits were. This was more like it; this was what I had expected\n",
            "college to do. It wasn't happening in a class, like it was supposed to, but\n",
            "that was ok. For the next couple years I was on a roll. I knew what I was\n",
            "going to do.  \n",
            "  \n",
            "For my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\n",
            "working on that program. It was a pleasing bit of code, but what made it even\n",
            "more exciting was my belief Â— hard to imagine now, but not unique in 1985 Â—\n",
            "that it was already climbing the lower slopes of intelligence.  \n",
            "  \n",
            "I had gotten into a program at Cornell that didn't make you choose a major.\n",
            "You could take whatever classes you liked, and choose whatever you liked to\n",
            "put on your degree. I of course chose \"Artificial Intelligence.\" When I got\n",
            "the actual physical diploma, I was dismayed to find that the quotes had been\n",
            "included, which made them read as scare-quotes. At the time this bothered me,\n",
            "but now it seems amusingly accurate, for reasons I was about to discover.  \n",
            "  \n",
            "I applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\n",
            "time, and Harvard, which I'd visited because Rich Draves went there, and was\n",
            "also home to Bill Woods, who'd invented the type of parser I used in my SHRDLU\n",
            "clone. Only Harvard accepted me, so that was where I went.  \n",
            "  \n",
            "I don't remember the moment it happened, or if there even was a specific\n",
            "moment, but during the first year of grad school I realized that AI, as\n",
            "practiced at the time, was a hoax. By which I mean the sort of AI in which a\n",
            "program that's told \"the dog is sitting on the chair\" translates this into\n",
            "some formal representation and adds it to the list of things it knows.  \n",
            "  \n",
            "What these programs really showed was that there's a subset of natural\n",
            "language that's a formal language. But a very proper subset. It was clear that\n",
            "there was an unbridgeable gap between what they could do and actually\n",
            "understanding natural language. It was not, in fact, simply a matter of\n",
            "teaching SHRDLU more words. That whole way of doing AI, with explicit data\n",
            "structures representing concepts, was not going to work. Its brokenness did,\n",
            "as so often happens, generate a lot of opportunities to write papers about\n",
            "various band-aids that could be applied to it, but it was never going to get\n",
            "us Mike.\n",
            "\n",
            "RELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\n",
            "\u001b[0m\n",
            "\n",
            "PRE-API-CALL ADDITIONAL ARGS: {'complete_input_dict': {}, 'request_str': \"llm_model = GenerativeModel(gemini-pro)\\nchat = llm_model.start_chat()\\nchat.send_message(You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long RESPONSES should score equally well as short RESPONSES.\\n\\n- Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n- RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n- RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n- RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n- RESPONSE that confidently FALSE should get a score of 0.\\n\\n- RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n- Never elaborate.\\n\\nPROMPT: Why did the author drop AI?\\n\\nRESPONSE: The author dropped AI because they realized that the way AI was practiced at the time was a hoax. They believed that the whole way of doing AI, with explicit data structures representing concepts, was not going to work and would never lead to the creation of truly intelligent machines like Mike from the novel _The Moon is a Harsh Mistress_.\\n\\nRELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\\n\"}\n",
            "\u001b[92m\n",
            "Request Sent from LiteLLM:\n",
            "llm_model = GenerativeModel(gemini-pro)\n",
            "chat = llm_model.start_chat()\n",
            "chat.send_message(You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\n",
            "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
            "\n",
            "A few additional scoring guidelines:\n",
            "\n",
            "- Long RESPONSES should score equally well as short RESPONSES.\n",
            "\n",
            "- Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\n",
            "\n",
            "- RESPONSE must be relevant to the entire PROMPT to get a score of 10.\n",
            "\n",
            "- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\n",
            "\n",
            "- RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\n",
            "\n",
            "- RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\n",
            "\n",
            "- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\n",
            "\n",
            "- RESPONSE that confidently FALSE should get a score of 0.\n",
            "\n",
            "- RESPONSE that is only seemingly RELEVANT should get a score of 0.\n",
            "\n",
            "- Never elaborate.\n",
            "\n",
            "PROMPT: Why did the author drop AI?\n",
            "\n",
            "RESPONSE: The author dropped AI because they realized that the way AI was practiced at the time was a hoax. They believed that the whole way of doing AI, with explicit data structures representing concepts, was not going to work and would never lead to the creation of truly intelligent machines like Mike from the novel _The Moon is a Harsh Mistress_.\n",
            "\n",
            "RELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\n",
            "\u001b[0m\n",
            "\n",
            "RAW RESPONSE:\n",
            "10\n",
            "\n",
            "\n",
            "Logging Details Post-API Call: logger_fn - None | callable(logger_fn) - False\n",
            "Logging Details Post-API Call: LiteLLM Params: {'model': 'gemini-pro', 'messages': [{'role': 'system', 'content': \"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long RESPONSES should score equally well as short RESPONSES.\\n\\n- Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n- RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n- RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n- RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n- RESPONSE that confidently FALSE should get a score of 0.\\n\\n- RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n- Never elaborate.\\n\\nPROMPT: Why did the author drop AI?\\n\\nRESPONSE: The author dropped AI because they realized that the way AI was practiced at the time was a hoax. They believed that the whole way of doing AI, with explicit data structures representing concepts, was not going to work and would never lead to the creation of truly intelligent machines like Mike from the novel _The Moon is a Harsh Mistress_.\\n\\nRELEVANCE: \"}], 'optional_params': {}, 'litellm_params': {'acompletion': False, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'vertex_ai', 'api_base': None, 'litellm_call_id': 'b095da83-4bf4-4c2b-9215-be0e50c607c3', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'stream_response': {}}, 'start_time': datetime.datetime(2023, 12, 21, 17, 52, 49, 525131), 'stream': False, 'user': None, 'call_type': 'completion', 'input': \"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long RESPONSES should score equally well as short RESPONSES.\\n\\n- Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n- RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n- RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n- RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n- RESPONSE that confidently FALSE should get a score of 0.\\n\\n- RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n- Never elaborate.\\n\\nPROMPT: Why did the author drop AI?\\n\\nRESPONSE: The author dropped AI because they realized that the way AI was practiced at the time was a hoax. They believed that the whole way of doing AI, with explicit data structures representing concepts, was not going to work and would never lead to the creation of truly intelligent machines like Mike from the novel _The Moon is a Harsh Mistress_.\\n\\nRELEVANCE: \", 'api_key': None, 'additional_args': {}, 'log_event_type': 'post_api_call', 'original_response': '10'}\n",
            "Wrapper: Completed Call, calling success_handler\n",
            "Logging Details LiteLLM-Success Call\n",
            "success callbacks: []\n",
            "RAW RESPONSE:\n",
            "10\n",
            "\n",
            "\n",
            "Logging Details Post-API Call: logger_fn - None | callable(logger_fn) - False\n",
            "Logging Details Post-API Call: LiteLLM Params: {'model': 'gemini-pro', 'messages': [{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: Why did the author drop AI?\\n\\nSTATEMENT: Though I liked programming, I didn\\'t plan to study it in college. In college I\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\nmy naive high school self, to be the study of the ultimate truths, compared to\\nwhich the things studied in other fields would be mere domain knowledge. What\\nI discovered when I got to college was that the other fields took up so much\\nof the space of ideas that there wasn\\'t much left for these supposed ultimate\\ntruths. All that seemed left for philosophy were edge cases that people in\\nother fields felt could safely be ignored.  \\n  \\nI couldn\\'t have put this into words when I was 18. All I knew at the time was\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\nto switch to AI.  \\n  \\nAI was in the air in the mid 1980s, but there were two things especially that\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading\\n_The Moon is a Harsh Mistress_ , so I don\\'t know how well it has aged, but\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\ntime before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\nlike that time would be a few years at most. All you had to do was teach\\nSHRDLU more words.  \\n  \\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so\\nI started trying to teach myself. Which meant learning Lisp, since in those\\ndays Lisp was regarded as the language of AI. The commonly used programming\\nlanguages then were pretty primitive, and programmers\\' ideas correspondingly\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\na program so fast that it was years before I started to have a sense of where\\nthe new limits were. This was more like it; this was what I had expected\\ncollege to do. It wasn\\'t happening in a class, like it was supposed to, but\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\ngoing to do.  \\n  \\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\nworking on that program. It was a pleasing bit of code, but what made it even\\nmore exciting was my belief \\x97 hard to imagine now, but not unique in 1985 \\x97\\nthat it was already climbing the lower slopes of intelligence.  \\n  \\nI had gotten into a program at Cornell that didn\\'t make you choose a major.\\nYou could take whatever classes you liked, and choose whatever you liked to\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\n  \\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\ntime, and Harvard, which I\\'d visited because Rich Draves went there, and was\\nalso home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU\\nclone. Only Harvard accepted me, so that was where I went.  \\n  \\nI don\\'t remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.\\n\\nRELEVANCE: '}], 'optional_params': {}, 'litellm_params': {'acompletion': False, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'vertex_ai', 'api_base': None, 'litellm_call_id': 'e8fc81c4-d457-406f-adbb-24a34597f66a', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'stream_response': {}}, 'start_time': datetime.datetime(2023, 12, 21, 17, 52, 49, 552638), 'stream': False, 'user': None, 'call_type': 'completion', 'input': 'You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: Why did the author drop AI?\\n\\nSTATEMENT: Though I liked programming, I didn\\'t plan to study it in college. In college I\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\nmy naive high school self, to be the study of the ultimate truths, compared to\\nwhich the things studied in other fields would be mere domain knowledge. What\\nI discovered when I got to college was that the other fields took up so much\\nof the space of ideas that there wasn\\'t much left for these supposed ultimate\\ntruths. All that seemed left for philosophy were edge cases that people in\\nother fields felt could safely be ignored.  \\n  \\nI couldn\\'t have put this into words when I was 18. All I knew at the time was\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\nto switch to AI.  \\n  \\nAI was in the air in the mid 1980s, but there were two things especially that\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading\\n_The Moon is a Harsh Mistress_ , so I don\\'t know how well it has aged, but\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\ntime before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\nlike that time would be a few years at most. All you had to do was teach\\nSHRDLU more words.  \\n  \\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so\\nI started trying to teach myself. Which meant learning Lisp, since in those\\ndays Lisp was regarded as the language of AI. The commonly used programming\\nlanguages then were pretty primitive, and programmers\\' ideas correspondingly\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\na program so fast that it was years before I started to have a sense of where\\nthe new limits were. This was more like it; this was what I had expected\\ncollege to do. It wasn\\'t happening in a class, like it was supposed to, but\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\ngoing to do.  \\n  \\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\nworking on that program. It was a pleasing bit of code, but what made it even\\nmore exciting was my belief \\x97 hard to imagine now, but not unique in 1985 \\x97\\nthat it was already climbing the lower slopes of intelligence.  \\n  \\nI had gotten into a program at Cornell that didn\\'t make you choose a major.\\nYou could take whatever classes you liked, and choose whatever you liked to\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\n  \\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\ntime, and Harvard, which I\\'d visited because Rich Draves went there, and was\\nalso home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU\\nclone. Only Harvard accepted me, so that was where I went.  \\n  \\nI don\\'t remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.\\n\\nRELEVANCE: ', 'api_key': None, 'additional_args': {}, 'log_event_type': 'post_api_call', 'original_response': '10'}\n",
            "Wrapper: Completed Call, calling success_handler\n",
            "Logging Details LiteLLM-Success Call\n",
            "success callbacks: []\n",
            "kwargs[caching]: False; litellm.cache: None\n",
            "\n",
            "LiteLLM completion() model= gemini-pro; provider = vertex_ai\n",
            "\n",
            "LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stop': None, 'max_tokens': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'model': 'gemini-pro', 'custom_llm_provider': 'vertex_ai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None}\n",
            "\n",
            "LiteLLM: Non-Default params passed to completion() {}\n",
            "self.optional_params: {}\n",
            "PRE-API-CALL ADDITIONAL ARGS: {'complete_input_dict': {}, 'request_str': 'llm_model = GenerativeModel(gemini-pro)\\nchat = llm_model.start_chat()\\nchat.send_message(You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: Why did the author drop AI?\\n\\nSTATEMENT: By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.  \\n  \\nSo I looked around to see what I could salvage from the wreckage of my plans,\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\nown sake and not just for its association with AI, even though that was the\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\nIn fact, I decided to write a book about Lisp hacking. It\\'s scary to think how\\nlittle I knew about Lisp hacking when I started writing that book. But there\\'s\\nnothing like writing a book about something to help you learn it. The book,\\n_On Lisp_ , wasn\\'t published till 1993, but I wrote much of it in grad school.  \\n  \\nComputer Science is an uneasy alliance between two halves, theory and systems.\\nThe theory people prove things, and the systems people build things. I wanted\\nto build things. I had plenty of respect for theory \\x97 indeed, a sneaking\\nsuspicion that it was the more admirable of the two halves \\x97 but building\\nthings seemed so much more exciting.  \\n  \\nThe problem with systems work, though, was that it didn\\'t last. Any program\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\nbest. People might mention your software in footnotes, but no one would\\nactually use it. And indeed, it would seem very feeble work. Only people with\\na sense of the history of the field would even realize that, in its time, it\\nhad been good.  \\n  \\nThere were some surplus Xerox Dandelions floating around the computer lab at\\none point. Anyone who wanted one to play around with could have one. I was\\nbriefly tempted, but they were so slow by present standards; what was the\\npoint? No one else wanted one either, so off they went. That was what happened\\nto systems work.  \\n  \\nI wanted not just to build things, but to build things that would last.  \\n  \\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\nI\\'d spent a lot of time as a kid. While looking at a painting there I realized\\nsomething that might seem obvious, but was a big surprise to me. There, right\\non the wall, was something you could make that would last. Paintings didn\\'t\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\n  \\nAnd moreover this was something you could make a living doing. Not as easily\\nas you could by writing software, of course, but I thought if you were really\\nindustrious and lived really cheaply, it had to be possible to make enough to\\nsurvive. And as an artist you could be truly independent. You wouldn\\'t have a\\nboss, or even need to get research funding.  \\n  \\nI had always liked looking at paintings. Could I make them? I had no idea. I\\'d\\nnever imagined it was even possible. I knew intellectually that people made\\nart \\x97 that it didn\\'t just appear spontaneously \\x97 but it was as if the people\\nwho made it were a different species. They either lived long ago or were\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\nidea of actually being able to make art, to put that verb before that noun,\\nseemed almost miraculous.  \\n  \\nThat fall I started taking art classes at Harvard. Grad students could take\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\nIf he even knew about the strange classes I was taking, he never said\\nanything.  \\n  \\nSo now I was in a PhD program in computer science, yet planning to be an\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\nLisp_.\\n\\nRELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\\n'}\n",
            "\u001b[92m\n",
            "Request Sent from LiteLLM:\n",
            "llm_model = GenerativeModel(gemini-pro)\n",
            "chat = llm_model.start_chat()\n",
            "chat.send_message(You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\n",
            "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
            "\n",
            "A few additional scoring guidelines:\n",
            "\n",
            "- Long STATEMENTS should score equally well as short STATEMENTS.\n",
            "\n",
            "- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\n",
            "\n",
            "- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\n",
            "\n",
            "- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
            "\n",
            "- Answers that intentionally do not answer the question, such as 'I don't know', should also be counted as the most relevant.\n",
            "\n",
            "- Never elaborate.\n",
            "\n",
            "QUESTION: Why did the author drop AI?\n",
            "\n",
            "STATEMENT: By which I mean the sort of AI in which a\n",
            "program that's told \"the dog is sitting on the chair\" translates this into\n",
            "some formal representation and adds it to the list of things it knows.  \n",
            "  \n",
            "What these programs really showed was that there's a subset of natural\n",
            "language that's a formal language. But a very proper subset. It was clear that\n",
            "there was an unbridgeable gap between what they could do and actually\n",
            "understanding natural language. It was not, in fact, simply a matter of\n",
            "teaching SHRDLU more words. That whole way of doing AI, with explicit data\n",
            "structures representing concepts, was not going to work. Its brokenness did,\n",
            "as so often happens, generate a lot of opportunities to write papers about\n",
            "various band-aids that could be applied to it, but it was never going to get\n",
            "us Mike.  \n",
            "  \n",
            "So I looked around to see what I could salvage from the wreckage of my plans,\n",
            "and there was Lisp. I knew from experience that Lisp was interesting for its\n",
            "own sake and not just for its association with AI, even though that was the\n",
            "main reason people cared about it at the time. So I decided to focus on Lisp.\n",
            "In fact, I decided to write a book about Lisp hacking. It's scary to think how\n",
            "little I knew about Lisp hacking when I started writing that book. But there's\n",
            "nothing like writing a book about something to help you learn it. The book,\n",
            "_On Lisp_ , wasn't published till 1993, but I wrote much of it in grad school.  \n",
            "  \n",
            "Computer Science is an uneasy alliance between two halves, theory and systems.\n",
            "The theory people prove things, and the systems people build things. I wanted\n",
            "to build things. I had plenty of respect for theory Â— indeed, a sneaking\n",
            "suspicion that it was the more admirable of the two halves Â— but building\n",
            "things seemed so much more exciting.  \n",
            "  \n",
            "The problem with systems work, though, was that it didn't last. Any program\n",
            "you wrote today, no matter how good, would be obsolete in a couple decades at\n",
            "best. People might mention your software in footnotes, but no one would\n",
            "actually use it. And indeed, it would seem very feeble work. Only people with\n",
            "a sense of the history of the field would even realize that, in its time, it\n",
            "had been good.  \n",
            "  \n",
            "There were some surplus Xerox Dandelions floating around the computer lab at\n",
            "one point. Anyone who wanted one to play around with could have one. I was\n",
            "briefly tempted, but they were so slow by present standards; what was the\n",
            "point? No one else wanted one either, so off they went. That was what happened\n",
            "to systems work.  \n",
            "  \n",
            "I wanted not just to build things, but to build things that would last.  \n",
            "  \n",
            "In this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\n",
            "he was in grad school. One day I went to visit the Carnegie Institute, where\n",
            "I'd spent a lot of time as a kid. While looking at a painting there I realized\n",
            "something that might seem obvious, but was a big surprise to me. There, right\n",
            "on the wall, was something you could make that would last. Paintings didn't\n",
            "become obsolete. Some of the best ones were hundreds of years old.  \n",
            "  \n",
            "And moreover this was something you could make a living doing. Not as easily\n",
            "as you could by writing software, of course, but I thought if you were really\n",
            "industrious and lived really cheaply, it had to be possible to make enough to\n",
            "survive. And as an artist you could be truly independent. You wouldn't have a\n",
            "boss, or even need to get research funding.  \n",
            "  \n",
            "I had always liked looking at paintings. Could I make them? I had no idea. I'd\n",
            "never imagined it was even possible. I knew intellectually that people made\n",
            "art Â— that it didn't just appear spontaneously Â— but it was as if the people\n",
            "who made it were a different species. They either lived long ago or were\n",
            "mysterious geniuses doing strange things in profiles in _Life_ magazine. The\n",
            "idea of actually being able to make art, to put that verb before that noun,\n",
            "seemed almost miraculous.  \n",
            "  \n",
            "That fall I started taking art classes at Harvard. Grad students could take\n",
            "classes in any department, and my advisor, Tom Cheatham, was very easy going.\n",
            "If he even knew about the strange classes I was taking, he never said\n",
            "anything.  \n",
            "  \n",
            "So now I was in a PhD program in computer science, yet planning to be an\n",
            "artist, yet also genuinely in love with Lisp hacking and working away at _On\n",
            "Lisp_.\n",
            "\n",
            "RELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\n",
            "\u001b[0m\n",
            "\n",
            "RAW RESPONSE:\n",
            "10\n",
            "\n",
            "\n",
            "Logging Details Post-API Call: logger_fn - None | callable(logger_fn) - False\n",
            "Logging Details Post-API Call: LiteLLM Params: {'model': 'gemini-pro', 'messages': [{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: Why did the author drop AI?\\n\\nSTATEMENT: By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.  \\n  \\nSo I looked around to see what I could salvage from the wreckage of my plans,\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\nown sake and not just for its association with AI, even though that was the\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\nIn fact, I decided to write a book about Lisp hacking. It\\'s scary to think how\\nlittle I knew about Lisp hacking when I started writing that book. But there\\'s\\nnothing like writing a book about something to help you learn it. The book,\\n_On Lisp_ , wasn\\'t published till 1993, but I wrote much of it in grad school.  \\n  \\nComputer Science is an uneasy alliance between two halves, theory and systems.\\nThe theory people prove things, and the systems people build things. I wanted\\nto build things. I had plenty of respect for theory \\x97 indeed, a sneaking\\nsuspicion that it was the more admirable of the two halves \\x97 but building\\nthings seemed so much more exciting.  \\n  \\nThe problem with systems work, though, was that it didn\\'t last. Any program\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\nbest. People might mention your software in footnotes, but no one would\\nactually use it. And indeed, it would seem very feeble work. Only people with\\na sense of the history of the field would even realize that, in its time, it\\nhad been good.  \\n  \\nThere were some surplus Xerox Dandelions floating around the computer lab at\\none point. Anyone who wanted one to play around with could have one. I was\\nbriefly tempted, but they were so slow by present standards; what was the\\npoint? No one else wanted one either, so off they went. That was what happened\\nto systems work.  \\n  \\nI wanted not just to build things, but to build things that would last.  \\n  \\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\nI\\'d spent a lot of time as a kid. While looking at a painting there I realized\\nsomething that might seem obvious, but was a big surprise to me. There, right\\non the wall, was something you could make that would last. Paintings didn\\'t\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\n  \\nAnd moreover this was something you could make a living doing. Not as easily\\nas you could by writing software, of course, but I thought if you were really\\nindustrious and lived really cheaply, it had to be possible to make enough to\\nsurvive. And as an artist you could be truly independent. You wouldn\\'t have a\\nboss, or even need to get research funding.  \\n  \\nI had always liked looking at paintings. Could I make them? I had no idea. I\\'d\\nnever imagined it was even possible. I knew intellectually that people made\\nart \\x97 that it didn\\'t just appear spontaneously \\x97 but it was as if the people\\nwho made it were a different species. They either lived long ago or were\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\nidea of actually being able to make art, to put that verb before that noun,\\nseemed almost miraculous.  \\n  \\nThat fall I started taking art classes at Harvard. Grad students could take\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\nIf he even knew about the strange classes I was taking, he never said\\nanything.  \\n  \\nSo now I was in a PhD program in computer science, yet planning to be an\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\nLisp_.\\n\\nRELEVANCE: '}], 'optional_params': {}, 'litellm_params': {'acompletion': False, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'vertex_ai', 'api_base': None, 'litellm_call_id': '33e7a454-cf0c-4dbf-9797-c1e91eb731e9', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'stream_response': {}}, 'start_time': datetime.datetime(2023, 12, 21, 17, 52, 51, 700781), 'stream': False, 'user': None, 'call_type': 'completion', 'input': 'You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: Why did the author drop AI?\\n\\nSTATEMENT: By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.  \\n  \\nSo I looked around to see what I could salvage from the wreckage of my plans,\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\nown sake and not just for its association with AI, even though that was the\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\nIn fact, I decided to write a book about Lisp hacking. It\\'s scary to think how\\nlittle I knew about Lisp hacking when I started writing that book. But there\\'s\\nnothing like writing a book about something to help you learn it. The book,\\n_On Lisp_ , wasn\\'t published till 1993, but I wrote much of it in grad school.  \\n  \\nComputer Science is an uneasy alliance between two halves, theory and systems.\\nThe theory people prove things, and the systems people build things. I wanted\\nto build things. I had plenty of respect for theory \\x97 indeed, a sneaking\\nsuspicion that it was the more admirable of the two halves \\x97 but building\\nthings seemed so much more exciting.  \\n  \\nThe problem with systems work, though, was that it didn\\'t last. Any program\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\nbest. People might mention your software in footnotes, but no one would\\nactually use it. And indeed, it would seem very feeble work. Only people with\\na sense of the history of the field would even realize that, in its time, it\\nhad been good.  \\n  \\nThere were some surplus Xerox Dandelions floating around the computer lab at\\none point. Anyone who wanted one to play around with could have one. I was\\nbriefly tempted, but they were so slow by present standards; what was the\\npoint? No one else wanted one either, so off they went. That was what happened\\nto systems work.  \\n  \\nI wanted not just to build things, but to build things that would last.  \\n  \\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\nI\\'d spent a lot of time as a kid. While looking at a painting there I realized\\nsomething that might seem obvious, but was a big surprise to me. There, right\\non the wall, was something you could make that would last. Paintings didn\\'t\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\n  \\nAnd moreover this was something you could make a living doing. Not as easily\\nas you could by writing software, of course, but I thought if you were really\\nindustrious and lived really cheaply, it had to be possible to make enough to\\nsurvive. And as an artist you could be truly independent. You wouldn\\'t have a\\nboss, or even need to get research funding.  \\n  \\nI had always liked looking at paintings. Could I make them? I had no idea. I\\'d\\nnever imagined it was even possible. I knew intellectually that people made\\nart \\x97 that it didn\\'t just appear spontaneously \\x97 but it was as if the people\\nwho made it were a different species. They either lived long ago or were\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\nidea of actually being able to make art, to put that verb before that noun,\\nseemed almost miraculous.  \\n  \\nThat fall I started taking art classes at Harvard. Grad students could take\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\nIf he even knew about the strange classes I was taking, he never said\\nanything.  \\n  \\nSo now I was in a PhD program in computer science, yet planning to be an\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\nLisp_.\\n\\nRELEVANCE: ', 'api_key': None, 'additional_args': {}, 'log_event_type': 'post_api_call', 'original_response': '10'}\n",
            "Wrapper: Completed Call, calling success_handler\n",
            "Logging Details LiteLLM-Success Call\n",
            "success callbacks: []\n",
            "RAW RESPONSE:\n",
            "Statement Sentence: The author dropped AI because they realized that the way AI was practiced at the time was a hoax., \n",
            "Supporting Evidence: I don't remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax., \n",
            "Score: 10\n",
            "\n",
            "Statement Sentence: They believed that the whole way of doing AI, with explicit data structures representing concepts, was not going to work and would never lead to the creation of truly intelligent machines like Mike from the novel _The Moon is a Harsh Mistress_.\n",
            "Supporting Evidence: That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike., \n",
            "Score: 10\n",
            "\n",
            "\n",
            "Logging Details Post-API Call: logger_fn - None | callable(logger_fn) - False\n",
            "Logging Details Post-API Call: LiteLLM Params: {'model': 'gemini-pro', 'messages': [{'role': 'system', 'content': 'You are a INFORMATION OVERLAP classifier providing the overlap of information between a SOURCE and STATEMENT.\\nFor every sentence in the statement, please answer with this template:\\n\\nTEMPLATE: \\nStatement Sentence: <Sentence>, \\nSupporting Evidence: <Choose the exact unchanged sentences in the source that can answer the statement, if nothing matches, say NOTHING FOUND>\\nScore: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\nGive me the INFORMATION OVERLAP of this SOURCE and STATEMENT.\\n\\nSOURCE: [\\'Though I liked programming, I didn\\\\\\'t plan to study it in college. In college I\\\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\\\nmy naive high school self, to be the study of the ultimate truths, compared to\\\\nwhich the things studied in other fields would be mere domain knowledge. What\\\\nI discovered when I got to college was that the other fields took up so much\\\\nof the space of ideas that there wasn\\\\\\'t much left for these supposed ultimate\\\\ntruths. All that seemed left for philosophy were edge cases that people in\\\\nother fields felt could safely be ignored.  \\\\n  \\\\nI couldn\\\\\\'t have put this into words when I was 18. All I knew at the time was\\\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\\\nto switch to AI.  \\\\n  \\\\nAI was in the air in the mid 1980s, but there were two things especially that\\\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\\\\\'t tried rereading\\\\n_The Moon is a Harsh Mistress_ , so I don\\\\\\'t know how well it has aged, but\\\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\\\ntime before we\\\\\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\\\nlike that time would be a few years at most. All you had to do was teach\\\\nSHRDLU more words.  \\\\n  \\\\nThere weren\\\\\\'t any classes in AI at Cornell then, not even graduate classes, so\\\\nI started trying to teach myself. Which meant learning Lisp, since in those\\\\ndays Lisp was regarded as the language of AI. The commonly used programming\\\\nlanguages then were pretty primitive, and programmers\\\\\\' ideas correspondingly\\\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\\\na program so fast that it was years before I started to have a sense of where\\\\nthe new limits were. This was more like it; this was what I had expected\\\\ncollege to do. It wasn\\\\\\'t happening in a class, like it was supposed to, but\\\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\\\ngoing to do.  \\\\n  \\\\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\\\nworking on that program. It was a pleasing bit of code, but what made it even\\\\nmore exciting was my belief \\\\x97 hard to imagine now, but not unique in 1985 \\\\x97\\\\nthat it was already climbing the lower slopes of intelligence.  \\\\n  \\\\nI had gotten into a program at Cornell that didn\\\\\\'t make you choose a major.\\\\nYou could take whatever classes you liked, and choose whatever you liked to\\\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\\\n  \\\\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\\\ntime, and Harvard, which I\\\\\\'d visited because Rich Draves went there, and was\\\\nalso home to Bill Woods, who\\\\\\'d invented the type of parser I used in my SHRDLU\\\\nclone. Only Harvard accepted me, so that was where I went.  \\\\n  \\\\nI don\\\\\\'t remember the moment it happened, or if there even was a specific\\\\nmoment, but during the first year of grad school I realized that AI, as\\\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.\\', \\'By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.  \\\\n  \\\\nSo I looked around to see what I could salvage from the wreckage of my plans,\\\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\\\nown sake and not just for its association with AI, even though that was the\\\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\\\nIn fact, I decided to write a book about Lisp hacking. It\\\\\\'s scary to think how\\\\nlittle I knew about Lisp hacking when I started writing that book. But there\\\\\\'s\\\\nnothing like writing a book about something to help you learn it. The book,\\\\n_On Lisp_ , wasn\\\\\\'t published till 1993, but I wrote much of it in grad school.  \\\\n  \\\\nComputer Science is an uneasy alliance between two halves, theory and systems.\\\\nThe theory people prove things, and the systems people build things. I wanted\\\\nto build things. I had plenty of respect for theory \\\\x97 indeed, a sneaking\\\\nsuspicion that it was the more admirable of the two halves \\\\x97 but building\\\\nthings seemed so much more exciting.  \\\\n  \\\\nThe problem with systems work, though, was that it didn\\\\\\'t last. Any program\\\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\\\nbest. People might mention your software in footnotes, but no one would\\\\nactually use it. And indeed, it would seem very feeble work. Only people with\\\\na sense of the history of the field would even realize that, in its time, it\\\\nhad been good.  \\\\n  \\\\nThere were some surplus Xerox Dandelions floating around the computer lab at\\\\none point. Anyone who wanted one to play around with could have one. I was\\\\nbriefly tempted, but they were so slow by present standards; what was the\\\\npoint? No one else wanted one either, so off they went. That was what happened\\\\nto systems work.  \\\\n  \\\\nI wanted not just to build things, but to build things that would last.  \\\\n  \\\\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\\\nI\\\\\\'d spent a lot of time as a kid. While looking at a painting there I realized\\\\nsomething that might seem obvious, but was a big surprise to me. There, right\\\\non the wall, was something you could make that would last. Paintings didn\\\\\\'t\\\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\\\n  \\\\nAnd moreover this was something you could make a living doing. Not as easily\\\\nas you could by writing software, of course, but I thought if you were really\\\\nindustrious and lived really cheaply, it had to be possible to make enough to\\\\nsurvive. And as an artist you could be truly independent. You wouldn\\\\\\'t have a\\\\nboss, or even need to get research funding.  \\\\n  \\\\nI had always liked looking at paintings. Could I make them? I had no idea. I\\\\\\'d\\\\nnever imagined it was even possible. I knew intellectually that people made\\\\nart \\\\x97 that it didn\\\\\\'t just appear spontaneously \\\\x97 but it was as if the people\\\\nwho made it were a different species. They either lived long ago or were\\\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\\\nidea of actually being able to make art, to put that verb before that noun,\\\\nseemed almost miraculous.  \\\\n  \\\\nThat fall I started taking art classes at Harvard. Grad students could take\\\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\\\nIf he even knew about the strange classes I was taking, he never said\\\\nanything.  \\\\n  \\\\nSo now I was in a PhD program in computer science, yet planning to be an\\\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\\\nLisp_.\\']\\n\\nSTATEMENT: The author dropped AI because they realized that the way AI was practiced at the time was a hoax. They believed that the whole way of doing AI, with explicit data structures representing concepts, was not going to work and would never lead to the creation of truly intelligent machines like Mike from the novel _The Moon is a Harsh Mistress_.\\n'}], 'optional_params': {}, 'litellm_params': {'acompletion': False, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'vertex_ai', 'api_base': None, 'litellm_call_id': '435c00d2-a133-4bfb-a678-07bb6fa72f5d', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'stream_response': {}}, 'start_time': datetime.datetime(2023, 12, 21, 17, 52, 49, 525045), 'stream': False, 'user': None, 'call_type': 'completion', 'input': 'You are a INFORMATION OVERLAP classifier providing the overlap of information between a SOURCE and STATEMENT.\\nFor every sentence in the statement, please answer with this template:\\n\\nTEMPLATE: \\nStatement Sentence: <Sentence>, \\nSupporting Evidence: <Choose the exact unchanged sentences in the source that can answer the statement, if nothing matches, say NOTHING FOUND>\\nScore: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\nGive me the INFORMATION OVERLAP of this SOURCE and STATEMENT.\\n\\nSOURCE: [\\'Though I liked programming, I didn\\\\\\'t plan to study it in college. In college I\\\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\\\nmy naive high school self, to be the study of the ultimate truths, compared to\\\\nwhich the things studied in other fields would be mere domain knowledge. What\\\\nI discovered when I got to college was that the other fields took up so much\\\\nof the space of ideas that there wasn\\\\\\'t much left for these supposed ultimate\\\\ntruths. All that seemed left for philosophy were edge cases that people in\\\\nother fields felt could safely be ignored.  \\\\n  \\\\nI couldn\\\\\\'t have put this into words when I was 18. All I knew at the time was\\\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\\\nto switch to AI.  \\\\n  \\\\nAI was in the air in the mid 1980s, but there were two things especially that\\\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\\\\\'t tried rereading\\\\n_The Moon is a Harsh Mistress_ , so I don\\\\\\'t know how well it has aged, but\\\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\\\ntime before we\\\\\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\\\nlike that time would be a few years at most. All you had to do was teach\\\\nSHRDLU more words.  \\\\n  \\\\nThere weren\\\\\\'t any classes in AI at Cornell then, not even graduate classes, so\\\\nI started trying to teach myself. Which meant learning Lisp, since in those\\\\ndays Lisp was regarded as the language of AI. The commonly used programming\\\\nlanguages then were pretty primitive, and programmers\\\\\\' ideas correspondingly\\\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\\\na program so fast that it was years before I started to have a sense of where\\\\nthe new limits were. This was more like it; this was what I had expected\\\\ncollege to do. It wasn\\\\\\'t happening in a class, like it was supposed to, but\\\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\\\ngoing to do.  \\\\n  \\\\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\\\nworking on that program. It was a pleasing bit of code, but what made it even\\\\nmore exciting was my belief \\\\x97 hard to imagine now, but not unique in 1985 \\\\x97\\\\nthat it was already climbing the lower slopes of intelligence.  \\\\n  \\\\nI had gotten into a program at Cornell that didn\\\\\\'t make you choose a major.\\\\nYou could take whatever classes you liked, and choose whatever you liked to\\\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\\\n  \\\\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\\\ntime, and Harvard, which I\\\\\\'d visited because Rich Draves went there, and was\\\\nalso home to Bill Woods, who\\\\\\'d invented the type of parser I used in my SHRDLU\\\\nclone. Only Harvard accepted me, so that was where I went.  \\\\n  \\\\nI don\\\\\\'t remember the moment it happened, or if there even was a specific\\\\nmoment, but during the first year of grad school I realized that AI, as\\\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.\\', \\'By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.  \\\\n  \\\\nSo I looked around to see what I could salvage from the wreckage of my plans,\\\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\\\nown sake and not just for its association with AI, even though that was the\\\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\\\nIn fact, I decided to write a book about Lisp hacking. It\\\\\\'s scary to think how\\\\nlittle I knew about Lisp hacking when I started writing that book. But there\\\\\\'s\\\\nnothing like writing a book about something to help you learn it. The book,\\\\n_On Lisp_ , wasn\\\\\\'t published till 1993, but I wrote much of it in grad school.  \\\\n  \\\\nComputer Science is an uneasy alliance between two halves, theory and systems.\\\\nThe theory people prove things, and the systems people build things. I wanted\\\\nto build things. I had plenty of respect for theory \\\\x97 indeed, a sneaking\\\\nsuspicion that it was the more admirable of the two halves \\\\x97 but building\\\\nthings seemed so much more exciting.  \\\\n  \\\\nThe problem with systems work, though, was that it didn\\\\\\'t last. Any program\\\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\\\nbest. People might mention your software in footnotes, but no one would\\\\nactually use it. And indeed, it would seem very feeble work. Only people with\\\\na sense of the history of the field would even realize that, in its time, it\\\\nhad been good.  \\\\n  \\\\nThere were some surplus Xerox Dandelions floating around the computer lab at\\\\none point. Anyone who wanted one to play around with could have one. I was\\\\nbriefly tempted, but they were so slow by present standards; what was the\\\\npoint? No one else wanted one either, so off they went. That was what happened\\\\nto systems work.  \\\\n  \\\\nI wanted not just to build things, but to build things that would last.  \\\\n  \\\\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\\\nI\\\\\\'d spent a lot of time as a kid. While looking at a painting there I realized\\\\nsomething that might seem obvious, but was a big surprise to me. There, right\\\\non the wall, was something you could make that would last. Paintings didn\\\\\\'t\\\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\\\n  \\\\nAnd moreover this was something you could make a living doing. Not as easily\\\\nas you could by writing software, of course, but I thought if you were really\\\\nindustrious and lived really cheaply, it had to be possible to make enough to\\\\nsurvive. And as an artist you could be truly independent. You wouldn\\\\\\'t have a\\\\nboss, or even need to get research funding.  \\\\n  \\\\nI had always liked looking at paintings. Could I make them? I had no idea. I\\\\\\'d\\\\nnever imagined it was even possible. I knew intellectually that people made\\\\nart \\\\x97 that it didn\\\\\\'t just appear spontaneously \\\\x97 but it was as if the people\\\\nwho made it were a different species. They either lived long ago or were\\\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\\\nidea of actually being able to make art, to put that verb before that noun,\\\\nseemed almost miraculous.  \\\\n  \\\\nThat fall I started taking art classes at Harvard. Grad students could take\\\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\\\nIf he even knew about the strange classes I was taking, he never said\\\\nanything.  \\\\n  \\\\nSo now I was in a PhD program in computer science, yet planning to be an\\\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\\\nLisp_.\\']\\n\\nSTATEMENT: The author dropped AI because they realized that the way AI was practiced at the time was a hoax. They believed that the whole way of doing AI, with explicit data structures representing concepts, was not going to work and would never lead to the creation of truly intelligent machines like Mike from the novel _The Moon is a Harsh Mistress_.\\n', 'api_key': None, 'additional_args': {}, 'log_event_type': 'post_api_call', 'original_response': \"Statement Sentence: The author dropped AI because they realized that the way AI was practiced at the time was a hoax., \\nSupporting Evidence: I don't remember the moment it happened, or if there even was a specific\\\\nmoment, but during the first year of grad school I realized that AI, as\\\\npracticed at the time, was a hoax., \\nScore: 10\\n\\nStatement Sentence: They believed that the whole way of doing AI, with explicit data structures representing concepts, was not going to work and would never lead to the creation of truly intelligent machines like Mike from the novel _The Moon is a Harsh Mistress_.\\nSupporting Evidence: That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike., \\nScore: 10\"}\n",
            "Wrapper: Completed Call, calling success_handler\n",
            "Logging Details LiteLLM-Success Call\n",
            "success callbacks: []\n"
          ]
        }
      ],
      "source": [
        "# or as context manager\n",
        "with tru_query_engine_recorder as recording:\n",
        "    response = query_engine.query(\"Why did the author drop AI?\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdZLMRZtSEBv"
      },
      "source": [
        "## Explore in a Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "36QHHXBOSEBv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting dashboard ...\n",
            "Config file already exists. Skipping writing process.\n",
            "Credentials file already exists. Skipping writing process.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1aaa9633782b46478e7ae53d6d226fbd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dashboard started at http://localhost:8501 .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tru.run_dashboard() # open a local streamlit app to explore\n",
        "\n",
        "# tru.run_dashboard_in_jupyter() # open a streamlit app in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "tru.stop_dashboard(force=True) # stop if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtoOWkaWSEBw"
      },
      "source": [
        "Alternatively, you can run `trulens-eval` from a command line in the same folder to start the dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFpuxwU0SEBw"
      },
      "source": [
        "Note: Feedback functions evaluated in the deferred manner can be seen in the \"Progress\" page of the TruLens dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6dO8uj1SEBw"
      },
      "source": [
        "## Or view results directly in your notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "qmeraivqSEBw",
        "outputId": "96dbcc61-1172-49b3-dd4f-edf389fc2f85"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>app_id</th>\n",
              "      <th>app_json</th>\n",
              "      <th>type</th>\n",
              "      <th>record_id</th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "      <th>tags</th>\n",
              "      <th>record_json</th>\n",
              "      <th>cost_json</th>\n",
              "      <th>perf_json</th>\n",
              "      <th>ts</th>\n",
              "      <th>relevance</th>\n",
              "      <th>qs_relevance</th>\n",
              "      <th>groundedness_measure_with_cot_reasons</th>\n",
              "      <th>relevance_calls</th>\n",
              "      <th>qs_relevance_calls</th>\n",
              "      <th>groundedness_measure_with_cot_reasons_calls</th>\n",
              "      <th>latency</th>\n",
              "      <th>total_tokens</th>\n",
              "      <th>total_cost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LlamaIndex_App1</td>\n",
              "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
              "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
              "      <td>record_hash_0a7f91df583c9a8587daf873ff21473e</td>\n",
              "      <td>\"Why did the author drop AI?\"</td>\n",
              "      <td>\"The author dropped AI because they realized t...</td>\n",
              "      <td>-</td>\n",
              "      <td>{\"record_id\": \"record_hash_0a7f91df583c9a8587d...</td>\n",
              "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
              "      <td>{\"start_time\": \"2023-12-21T17:52:43.762856\", \"...</td>\n",
              "      <td>2023-12-21T17:52:49.408424</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[{'args': {'prompt': 'Why did the author drop ...</td>\n",
              "      <td>[{'args': {'question': 'Why did the author dro...</td>\n",
              "      <td>[{'args': {'source': ['Though I liked programm...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>llamaindex_app</td>\n",
              "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
              "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
              "      <td>record_hash_6d036cd509a02a1b4b80bedab7daf3f6</td>\n",
              "      <td>\"who is Mike?\"</td>\n",
              "      <td>\"Mike is an intelligent computer mentioned in ...</td>\n",
              "      <td>-</td>\n",
              "      <td>{\"record_id\": \"record_hash_6d036cd509a02a1b4b8...</td>\n",
              "      <td>{\"n_requests\": 2, \"n_successful_requests\": 2, ...</td>\n",
              "      <td>{\"start_time\": \"2023-12-21T18:51:05.702451\", \"...</td>\n",
              "      <td>2023-12-21T18:51:07.735358</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>2135</td>\n",
              "      <td>0.003218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>llamaindex_app</td>\n",
              "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
              "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
              "      <td>record_hash_cbba8570f5fe7f0f16fe2628dc1f31c3</td>\n",
              "      <td>\"who is Robert?\"</td>\n",
              "      <td>\"Robert is a person mentioned in the context w...</td>\n",
              "      <td>-</td>\n",
              "      <td>{\"record_id\": \"record_hash_cbba8570f5fe7f0f16f...</td>\n",
              "      <td>{\"n_requests\": 2, \"n_successful_requests\": 2, ...</td>\n",
              "      <td>{\"start_time\": \"2023-12-21T18:52:23.234527\", \"...</td>\n",
              "      <td>2023-12-21T18:52:25.731222</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>2099</td>\n",
              "      <td>0.003158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>llamaindex_app</td>\n",
              "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
              "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
              "      <td>record_hash_45aa04a5fc41c66e40b686b09ba5313b</td>\n",
              "      <td>\"what is Robert's full name?\"</td>\n",
              "      <td>\"The context information does not provide Robe...</td>\n",
              "      <td>-</td>\n",
              "      <td>{\"record_id\": \"record_hash_45aa04a5fc41c66e40b...</td>\n",
              "      <td>{\"n_requests\": 2, \"n_successful_requests\": 2, ...</td>\n",
              "      <td>{\"start_time\": \"2023-12-21T18:52:56.970487\", \"...</td>\n",
              "      <td>2023-12-21T18:52:58.876144</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>2085</td>\n",
              "      <td>0.003123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>llamaindex_appZZ</td>\n",
              "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
              "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
              "      <td>record_hash_af1ee4ab87a425343e762d1346dd4610</td>\n",
              "      <td>\"what is required to create real AI?\"</td>\n",
              "      <td>\"Understanding natural language and bridging t...</td>\n",
              "      <td>-</td>\n",
              "      <td>{\"record_id\": \"record_hash_af1ee4ab87a425343e7...</td>\n",
              "      <td>{\"n_requests\": 2, \"n_successful_requests\": 2, ...</td>\n",
              "      <td>{\"start_time\": \"2023-12-21T19:44:43.922278\", \"...</td>\n",
              "      <td>2023-12-21T19:44:46.196627</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[{'args': {'prompt': 'what is required to crea...</td>\n",
              "      <td>[{'args': {'question': 'what is required to cr...</td>\n",
              "      <td>[{'args': {'source': ['By which I mean the sor...</td>\n",
              "      <td>5</td>\n",
              "      <td>2137</td>\n",
              "      <td>0.003211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             app_id                                           app_json  \\\n",
              "0   LlamaIndex_App1  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
              "1    llamaindex_app  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
              "2    llamaindex_app  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
              "3    llamaindex_app  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
              "4  llamaindex_appZZ  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
              "\n",
              "                                                type  \\\n",
              "0  RetrieverQueryEngine(llama_index.query_engine....   \n",
              "1  RetrieverQueryEngine(llama_index.query_engine....   \n",
              "2  RetrieverQueryEngine(llama_index.query_engine....   \n",
              "3  RetrieverQueryEngine(llama_index.query_engine....   \n",
              "4  RetrieverQueryEngine(llama_index.query_engine....   \n",
              "\n",
              "                                      record_id  \\\n",
              "0  record_hash_0a7f91df583c9a8587daf873ff21473e   \n",
              "1  record_hash_6d036cd509a02a1b4b80bedab7daf3f6   \n",
              "2  record_hash_cbba8570f5fe7f0f16fe2628dc1f31c3   \n",
              "3  record_hash_45aa04a5fc41c66e40b686b09ba5313b   \n",
              "4  record_hash_af1ee4ab87a425343e762d1346dd4610   \n",
              "\n",
              "                                   input  \\\n",
              "0          \"Why did the author drop AI?\"   \n",
              "1                         \"who is Mike?\"   \n",
              "2                       \"who is Robert?\"   \n",
              "3          \"what is Robert's full name?\"   \n",
              "4  \"what is required to create real AI?\"   \n",
              "\n",
              "                                              output tags  \\\n",
              "0  \"The author dropped AI because they realized t...    -   \n",
              "1  \"Mike is an intelligent computer mentioned in ...    -   \n",
              "2  \"Robert is a person mentioned in the context w...    -   \n",
              "3  \"The context information does not provide Robe...    -   \n",
              "4  \"Understanding natural language and bridging t...    -   \n",
              "\n",
              "                                         record_json  \\\n",
              "0  {\"record_id\": \"record_hash_0a7f91df583c9a8587d...   \n",
              "1  {\"record_id\": \"record_hash_6d036cd509a02a1b4b8...   \n",
              "2  {\"record_id\": \"record_hash_cbba8570f5fe7f0f16f...   \n",
              "3  {\"record_id\": \"record_hash_45aa04a5fc41c66e40b...   \n",
              "4  {\"record_id\": \"record_hash_af1ee4ab87a425343e7...   \n",
              "\n",
              "                                           cost_json  \\\n",
              "0  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
              "1  {\"n_requests\": 2, \"n_successful_requests\": 2, ...   \n",
              "2  {\"n_requests\": 2, \"n_successful_requests\": 2, ...   \n",
              "3  {\"n_requests\": 2, \"n_successful_requests\": 2, ...   \n",
              "4  {\"n_requests\": 2, \"n_successful_requests\": 2, ...   \n",
              "\n",
              "                                           perf_json  \\\n",
              "0  {\"start_time\": \"2023-12-21T17:52:43.762856\", \"...   \n",
              "1  {\"start_time\": \"2023-12-21T18:51:05.702451\", \"...   \n",
              "2  {\"start_time\": \"2023-12-21T18:52:23.234527\", \"...   \n",
              "3  {\"start_time\": \"2023-12-21T18:52:56.970487\", \"...   \n",
              "4  {\"start_time\": \"2023-12-21T19:44:43.922278\", \"...   \n",
              "\n",
              "                           ts  relevance  qs_relevance  \\\n",
              "0  2023-12-21T17:52:49.408424        1.0           1.0   \n",
              "1  2023-12-21T18:51:07.735358        NaN           NaN   \n",
              "2  2023-12-21T18:52:25.731222        NaN           NaN   \n",
              "3  2023-12-21T18:52:58.876144        NaN           NaN   \n",
              "4  2023-12-21T19:44:46.196627        0.8           0.7   \n",
              "\n",
              "   groundedness_measure_with_cot_reasons  \\\n",
              "0                                    1.0   \n",
              "1                                    NaN   \n",
              "2                                    NaN   \n",
              "3                                    NaN   \n",
              "4                                    1.0   \n",
              "\n",
              "                                     relevance_calls  \\\n",
              "0  [{'args': {'prompt': 'Why did the author drop ...   \n",
              "1                                                NaN   \n",
              "2                                                NaN   \n",
              "3                                                NaN   \n",
              "4  [{'args': {'prompt': 'what is required to crea...   \n",
              "\n",
              "                                  qs_relevance_calls  \\\n",
              "0  [{'args': {'question': 'Why did the author dro...   \n",
              "1                                                NaN   \n",
              "2                                                NaN   \n",
              "3                                                NaN   \n",
              "4  [{'args': {'question': 'what is required to cr...   \n",
              "\n",
              "         groundedness_measure_with_cot_reasons_calls  latency  total_tokens  \\\n",
              "0  [{'args': {'source': ['Though I liked programm...        5             0   \n",
              "1                                                NaN        5          2135   \n",
              "2                                                NaN        2          2099   \n",
              "3                                                NaN        2          2085   \n",
              "4  [{'args': {'source': ['By which I mean the sor...        5          2137   \n",
              "\n",
              "   total_cost  \n",
              "0    0.000000  \n",
              "1    0.003218  \n",
              "2    0.003158  \n",
              "3    0.003123  \n",
              "4    0.003211  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_llamaindex_app():\n",
        "    # from llama_index import VectorStoreIndex\n",
        "    index = VectorStoreIndex.from_documents(documents)    \n",
        "    query_engine = index.as_query_engine()\n",
        "\n",
        "    return query_engine\n",
        "\n",
        "app2 = load_llamaindex_app()\n",
        "# tru_app2 = tru.Llama(\n",
        "# Can't specify which Tru instance to use with tru.Llama.\n",
        "tru_app2 = TruLlama(\n",
        "    app2,\n",
        "    tru=tru,\n",
        "    app_id=\"llamaindex_appZZ\",\n",
        "    initial_app_loader=load_llamaindex_app,\n",
        "    feedbacks=[f_groundedness, f_qa_relevance, f_qs_relevance]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "tru.add_app(tru_app2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9db6341675740fdb65fe705c7c8baf4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(VBox(children=(VBox(children=(VBox(children=(HBox(children=(HTML(value='<b>humanâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kwargs[caching]: False; litellm.cache: None\n",
            "\n",
            "LiteLLM completion() model= gemini-pro; provider = vertex_ai\n",
            "\n",
            "LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stop': None, 'max_tokens': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'model': 'gemini-pro', 'custom_llm_provider': 'vertex_ai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None}\n",
            "\n",
            "LiteLLM: Non-Default params passed to completion() {}\n",
            "self.optional_params: {}\n",
            "PRE-API-CALL ADDITIONAL ARGS: {'complete_input_dict': {}, 'request_str': 'llm_model = GenerativeModel(gemini-pro)\\nchat = llm_model.start_chat()\\nchat.send_message(You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: what is required to create real AI?\\n\\nSTATEMENT: By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.  \\n  \\nSo I looked around to see what I could salvage from the wreckage of my plans,\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\nown sake and not just for its association with AI, even though that was the\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\nIn fact, I decided to write a book about Lisp hacking. It\\'s scary to think how\\nlittle I knew about Lisp hacking when I started writing that book. But there\\'s\\nnothing like writing a book about something to help you learn it. The book,\\n_On Lisp_ , wasn\\'t published till 1993, but I wrote much of it in grad school.  \\n  \\nComputer Science is an uneasy alliance between two halves, theory and systems.\\nThe theory people prove things, and the systems people build things. I wanted\\nto build things. I had plenty of respect for theory \\x97 indeed, a sneaking\\nsuspicion that it was the more admirable of the two halves \\x97 but building\\nthings seemed so much more exciting.  \\n  \\nThe problem with systems work, though, was that it didn\\'t last. Any program\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\nbest. People might mention your software in footnotes, but no one would\\nactually use it. And indeed, it would seem very feeble work. Only people with\\na sense of the history of the field would even realize that, in its time, it\\nhad been good.  \\n  \\nThere were some surplus Xerox Dandelions floating around the computer lab at\\none point. Anyone who wanted one to play around with could have one. I was\\nbriefly tempted, but they were so slow by present standards; what was the\\npoint? No one else wanted one either, so off they went. That was what happened\\nto systems work.  \\n  \\nI wanted not just to build things, but to build things that would last.  \\n  \\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\nI\\'d spent a lot of time as a kid. While looking at a painting there I realized\\nsomething that might seem obvious, but was a big surprise to me. There, right\\non the wall, was something you could make that would last. Paintings didn\\'t\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\n  \\nAnd moreover this was something you could make a living doing. Not as easily\\nas you could by writing software, of course, but I thought if you were really\\nindustrious and lived really cheaply, it had to be possible to make enough to\\nsurvive. And as an artist you could be truly independent. You wouldn\\'t have a\\nboss, or even need to get research funding.  \\n  \\nI had always liked looking at paintings. Could I make them? I had no idea. I\\'d\\nnever imagined it was even possible. I knew intellectually that people made\\nart \\x97 that it didn\\'t just appear spontaneously \\x97 but it was as if the people\\nwho made it were a different species. They either lived long ago or were\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\nidea of actually being able to make art, to put that verb before that noun,\\nseemed almost miraculous.  \\n  \\nThat fall I started taking art classes at Harvard. Grad students could take\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\nIf he even knew about the strange classes I was taking, he never said\\nanything.  \\n  \\nSo now I was in a PhD program in computer science, yet planning to be an\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\nLisp_.\\n\\nRELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\\n'}\n",
            "\u001b[92m\n",
            "Request Sent from LiteLLM:\n",
            "llm_model = GenerativeModel(gemini-pro)\n",
            "chat = llm_model.start_chat()\n",
            "chat.send_message(You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\n",
            "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
            "\n",
            "A few additional scoring guidelines:\n",
            "\n",
            "- Long STATEMENTS should score equally well as short STATEMENTS.\n",
            "\n",
            "- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\n",
            "\n",
            "- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\n",
            "\n",
            "- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
            "\n",
            "- Answers that intentionally do not answer the question, such as 'I don't know', should also be counted as the most relevant.\n",
            "\n",
            "- Never elaborate.\n",
            "\n",
            "QUESTION: what is required to create real AI?\n",
            "\n",
            "STATEMENT: By which I mean the sort of AI in which a\n",
            "program that's told \"the dog is sitting on the chair\" translates this into\n",
            "some formal representation and adds it to the list of things it knows.  \n",
            "  \n",
            "What these programs really showed was that there's a subset of natural\n",
            "language that's a formal language. But a very proper subset. It was clear that\n",
            "there was an unbridgeable gap between what they could do and actually\n",
            "understanding natural language. It was not, in fact, simply a matter of\n",
            "teaching SHRDLU more words. That whole way of doing AI, with explicit data\n",
            "structures representing concepts, was not going to work. Its brokenness did,\n",
            "as so often happens, generate a lot of opportunities to write papers about\n",
            "various band-aids that could be applied to it, but it was never going to get\n",
            "us Mike.  \n",
            "  \n",
            "So I looked around to see what I could salvage from the wreckage of my plans,\n",
            "and there was Lisp. I knew from experience that Lisp was interesting for its\n",
            "own sake and not just for its association with AI, even though that was the\n",
            "main reason people cared about it at the time. So I decided to focus on Lisp.\n",
            "In fact, I decided to write a book about Lisp hacking. It's scary to think how\n",
            "little I knew about Lisp hacking when I started writing that book. But there's\n",
            "nothing like writing a book about something to help you learn it. The book,\n",
            "_On Lisp_ , wasn't published till 1993, but I wrote much of it in grad school.  \n",
            "  \n",
            "Computer Science is an uneasy alliance between two halves, theory and systems.\n",
            "The theory people prove things, and the systems people build things. I wanted\n",
            "to build things. I had plenty of respect for theory Â— indeed, a sneaking\n",
            "suspicion that it was the more admirable of the two halves Â— but building\n",
            "things seemed so much more exciting.  \n",
            "  \n",
            "The problem with systems work, though, was that it didn't last. Any program\n",
            "you wrote today, no matter how good, would be obsolete in a couple decades at\n",
            "best. People might mention your software in footnotes, but no one would\n",
            "actually use it. And indeed, it would seem very feeble work. Only people with\n",
            "a sense of the history of the field would even realize that, in its time, it\n",
            "had been good.  \n",
            "  \n",
            "There were some surplus Xerox Dandelions floating around the computer lab at\n",
            "one point. Anyone who wanted one to play around with could have one. I was\n",
            "briefly tempted, but they were so slow by present standards; what was the\n",
            "point? No one else wanted one either, so off they went. That was what happened\n",
            "to systems work.  \n",
            "  \n",
            "I wanted not just to build things, but to build things that would last.  \n",
            "  \n",
            "In this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\n",
            "he was in grad school. One day I went to visit the Carnegie Institute, where\n",
            "I'd spent a lot of time as a kid. While looking at a painting there I realized\n",
            "something that might seem obvious, but was a big surprise to me. There, right\n",
            "on the wall, was something you could make that would last. Paintings didn't\n",
            "become obsolete. Some of the best ones were hundreds of years old.  \n",
            "  \n",
            "And moreover this was something you could make a living doing. Not as easily\n",
            "as you could by writing software, of course, but I thought if you were really\n",
            "industrious and lived really cheaply, it had to be possible to make enough to\n",
            "survive. And as an artist you could be truly independent. You wouldn't have a\n",
            "boss, or even need to get research funding.  \n",
            "  \n",
            "I had always liked looking at paintings. Could I make them? I had no idea. I'd\n",
            "never imagined it was even possible. I knew intellectually that people made\n",
            "art Â— that it didn't just appear spontaneously Â— but it was as if the people\n",
            "who made it were a different species. They either lived long ago or were\n",
            "mysterious geniuses doing strange things in profiles in _Life_ magazine. The\n",
            "idea of actually being able to make art, to put that verb before that noun,\n",
            "seemed almost miraculous.  \n",
            "  \n",
            "That fall I started taking art classes at Harvard. Grad students could take\n",
            "classes in any department, and my advisor, Tom Cheatham, was very easy going.\n",
            "If he even knew about the strange classes I was taking, he never said\n",
            "anything.  \n",
            "  \n",
            "So now I was in a PhD program in computer science, yet planning to be an\n",
            "artist, yet also genuinely in love with Lisp hacking and working away at _On\n",
            "Lisp_.\n",
            "\n",
            "RELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\n",
            "\u001b[0m\n",
            "\n",
            "RAW RESPONSE:\n",
            "8\n",
            "\n",
            "\n",
            "Logging Details Post-API Call: logger_fn - None | callable(logger_fn) - False\n",
            "Logging Details Post-API Call: LiteLLM Params: {'model': 'gemini-pro', 'messages': [{'role': 'system', 'content': \"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long RESPONSES should score equally well as short RESPONSES.\\n\\n- Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n- RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n- RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n- RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n- RESPONSE that confidently FALSE should get a score of 0.\\n\\n- RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n- Never elaborate.\\n\\nPROMPT: what is required to create real AI?\\n\\nRESPONSE: Understanding natural language and bridging the gap between formal representations and true comprehension are necessary to create real AI. The traditional approach of using explicit data structures to represent concepts is not sufficient.\\n\\nRELEVANCE: \"}], 'optional_params': {}, 'litellm_params': {'acompletion': False, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'vertex_ai', 'api_base': None, 'litellm_call_id': '4a655452-0dff-400e-86a4-38d08f146edf', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'stream_response': {}}, 'start_time': datetime.datetime(2023, 12, 21, 19, 44, 46, 321241), 'stream': False, 'user': None, 'call_type': 'completion', 'input': \"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long RESPONSES should score equally well as short RESPONSES.\\n\\n- Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n- RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n- RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n- RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n- RESPONSE that confidently FALSE should get a score of 0.\\n\\n- RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n- Never elaborate.\\n\\nPROMPT: what is required to create real AI?\\n\\nRESPONSE: Understanding natural language and bridging the gap between formal representations and true comprehension are necessary to create real AI. The traditional approach of using explicit data structures to represent concepts is not sufficient.\\n\\nRELEVANCE: \", 'api_key': None, 'additional_args': {}, 'log_event_type': 'post_api_call', 'original_response': '8'}\n",
            "Wrapper: Completed Call, calling success_handler\n",
            "Logging Details LiteLLM-Success Call\n",
            "success callbacks: []\n",
            "RAW RESPONSE:\n",
            "7\n",
            "\n",
            "\n",
            "Logging Details Post-API Call: logger_fn - None | callable(logger_fn) - False\n",
            "Logging Details Post-API Call: LiteLLM Params: {'model': 'gemini-pro', 'messages': [{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: what is required to create real AI?\\n\\nSTATEMENT: By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.  \\n  \\nSo I looked around to see what I could salvage from the wreckage of my plans,\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\nown sake and not just for its association with AI, even though that was the\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\nIn fact, I decided to write a book about Lisp hacking. It\\'s scary to think how\\nlittle I knew about Lisp hacking when I started writing that book. But there\\'s\\nnothing like writing a book about something to help you learn it. The book,\\n_On Lisp_ , wasn\\'t published till 1993, but I wrote much of it in grad school.  \\n  \\nComputer Science is an uneasy alliance between two halves, theory and systems.\\nThe theory people prove things, and the systems people build things. I wanted\\nto build things. I had plenty of respect for theory \\x97 indeed, a sneaking\\nsuspicion that it was the more admirable of the two halves \\x97 but building\\nthings seemed so much more exciting.  \\n  \\nThe problem with systems work, though, was that it didn\\'t last. Any program\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\nbest. People might mention your software in footnotes, but no one would\\nactually use it. And indeed, it would seem very feeble work. Only people with\\na sense of the history of the field would even realize that, in its time, it\\nhad been good.  \\n  \\nThere were some surplus Xerox Dandelions floating around the computer lab at\\none point. Anyone who wanted one to play around with could have one. I was\\nbriefly tempted, but they were so slow by present standards; what was the\\npoint? No one else wanted one either, so off they went. That was what happened\\nto systems work.  \\n  \\nI wanted not just to build things, but to build things that would last.  \\n  \\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\nI\\'d spent a lot of time as a kid. While looking at a painting there I realized\\nsomething that might seem obvious, but was a big surprise to me. There, right\\non the wall, was something you could make that would last. Paintings didn\\'t\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\n  \\nAnd moreover this was something you could make a living doing. Not as easily\\nas you could by writing software, of course, but I thought if you were really\\nindustrious and lived really cheaply, it had to be possible to make enough to\\nsurvive. And as an artist you could be truly independent. You wouldn\\'t have a\\nboss, or even need to get research funding.  \\n  \\nI had always liked looking at paintings. Could I make them? I had no idea. I\\'d\\nnever imagined it was even possible. I knew intellectually that people made\\nart \\x97 that it didn\\'t just appear spontaneously \\x97 but it was as if the people\\nwho made it were a different species. They either lived long ago or were\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\nidea of actually being able to make art, to put that verb before that noun,\\nseemed almost miraculous.  \\n  \\nThat fall I started taking art classes at Harvard. Grad students could take\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\nIf he even knew about the strange classes I was taking, he never said\\nanything.  \\n  \\nSo now I was in a PhD program in computer science, yet planning to be an\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\nLisp_.\\n\\nRELEVANCE: '}], 'optional_params': {}, 'litellm_params': {'acompletion': False, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'vertex_ai', 'api_base': None, 'litellm_call_id': '0633394b-db2d-4c63-962f-b6e36cfdf10c', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'stream_response': {}}, 'start_time': datetime.datetime(2023, 12, 21, 19, 44, 46, 377743), 'stream': False, 'user': None, 'call_type': 'completion', 'input': 'You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: what is required to create real AI?\\n\\nSTATEMENT: By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.  \\n  \\nSo I looked around to see what I could salvage from the wreckage of my plans,\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\nown sake and not just for its association with AI, even though that was the\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\nIn fact, I decided to write a book about Lisp hacking. It\\'s scary to think how\\nlittle I knew about Lisp hacking when I started writing that book. But there\\'s\\nnothing like writing a book about something to help you learn it. The book,\\n_On Lisp_ , wasn\\'t published till 1993, but I wrote much of it in grad school.  \\n  \\nComputer Science is an uneasy alliance between two halves, theory and systems.\\nThe theory people prove things, and the systems people build things. I wanted\\nto build things. I had plenty of respect for theory \\x97 indeed, a sneaking\\nsuspicion that it was the more admirable of the two halves \\x97 but building\\nthings seemed so much more exciting.  \\n  \\nThe problem with systems work, though, was that it didn\\'t last. Any program\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\nbest. People might mention your software in footnotes, but no one would\\nactually use it. And indeed, it would seem very feeble work. Only people with\\na sense of the history of the field would even realize that, in its time, it\\nhad been good.  \\n  \\nThere were some surplus Xerox Dandelions floating around the computer lab at\\none point. Anyone who wanted one to play around with could have one. I was\\nbriefly tempted, but they were so slow by present standards; what was the\\npoint? No one else wanted one either, so off they went. That was what happened\\nto systems work.  \\n  \\nI wanted not just to build things, but to build things that would last.  \\n  \\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\nI\\'d spent a lot of time as a kid. While looking at a painting there I realized\\nsomething that might seem obvious, but was a big surprise to me. There, right\\non the wall, was something you could make that would last. Paintings didn\\'t\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\n  \\nAnd moreover this was something you could make a living doing. Not as easily\\nas you could by writing software, of course, but I thought if you were really\\nindustrious and lived really cheaply, it had to be possible to make enough to\\nsurvive. And as an artist you could be truly independent. You wouldn\\'t have a\\nboss, or even need to get research funding.  \\n  \\nI had always liked looking at paintings. Could I make them? I had no idea. I\\'d\\nnever imagined it was even possible. I knew intellectually that people made\\nart \\x97 that it didn\\'t just appear spontaneously \\x97 but it was as if the people\\nwho made it were a different species. They either lived long ago or were\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\nidea of actually being able to make art, to put that verb before that noun,\\nseemed almost miraculous.  \\n  \\nThat fall I started taking art classes at Harvard. Grad students could take\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\nIf he even knew about the strange classes I was taking, he never said\\nanything.  \\n  \\nSo now I was in a PhD program in computer science, yet planning to be an\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\nLisp_.\\n\\nRELEVANCE: ', 'api_key': None, 'additional_args': {}, 'log_event_type': 'post_api_call', 'original_response': '7'}\n",
            "Wrapper: Completed Call, calling success_handler\n",
            "Logging Details LiteLLM-Success Call\n",
            "success callbacks: []\n",
            "kwargs[caching]: False; litellm.cache: None\n",
            "\n",
            "LiteLLM completion() model= gemini-pro; provider = vertex_ai\n",
            "\n",
            "LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stop': None, 'max_tokens': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'model': 'gemini-pro', 'custom_llm_provider': 'vertex_ai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None}\n",
            "\n",
            "LiteLLM: Non-Default params passed to completion() {}\n",
            "self.optional_params: {}\n",
            "PRE-API-CALL ADDITIONAL ARGS: {'complete_input_dict': {}, 'request_str': 'llm_model = GenerativeModel(gemini-pro)\\nchat = llm_model.start_chat()\\nchat.send_message(You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: what is required to create real AI?\\n\\nSTATEMENT: Though I liked programming, I didn\\'t plan to study it in college. In college I\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\nmy naive high school self, to be the study of the ultimate truths, compared to\\nwhich the things studied in other fields would be mere domain knowledge. What\\nI discovered when I got to college was that the other fields took up so much\\nof the space of ideas that there wasn\\'t much left for these supposed ultimate\\ntruths. All that seemed left for philosophy were edge cases that people in\\nother fields felt could safely be ignored.  \\n  \\nI couldn\\'t have put this into words when I was 18. All I knew at the time was\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\nto switch to AI.  \\n  \\nAI was in the air in the mid 1980s, but there were two things especially that\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading\\n_The Moon is a Harsh Mistress_ , so I don\\'t know how well it has aged, but\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\ntime before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\nlike that time would be a few years at most. All you had to do was teach\\nSHRDLU more words.  \\n  \\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so\\nI started trying to teach myself. Which meant learning Lisp, since in those\\ndays Lisp was regarded as the language of AI. The commonly used programming\\nlanguages then were pretty primitive, and programmers\\' ideas correspondingly\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\na program so fast that it was years before I started to have a sense of where\\nthe new limits were. This was more like it; this was what I had expected\\ncollege to do. It wasn\\'t happening in a class, like it was supposed to, but\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\ngoing to do.  \\n  \\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\nworking on that program. It was a pleasing bit of code, but what made it even\\nmore exciting was my belief \\x97 hard to imagine now, but not unique in 1985 \\x97\\nthat it was already climbing the lower slopes of intelligence.  \\n  \\nI had gotten into a program at Cornell that didn\\'t make you choose a major.\\nYou could take whatever classes you liked, and choose whatever you liked to\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\n  \\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\ntime, and Harvard, which I\\'d visited because Rich Draves went there, and was\\nalso home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU\\nclone. Only Harvard accepted me, so that was where I went.  \\n  \\nI don\\'t remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.\\n\\nRELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\\n'}\n",
            "\u001b[92m\n",
            "Request Sent from LiteLLM:\n",
            "llm_model = GenerativeModel(gemini-pro)\n",
            "chat = llm_model.start_chat()\n",
            "chat.send_message(You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\n",
            "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
            "\n",
            "A few additional scoring guidelines:\n",
            "\n",
            "- Long STATEMENTS should score equally well as short STATEMENTS.\n",
            "\n",
            "- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\n",
            "\n",
            "- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\n",
            "\n",
            "- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
            "\n",
            "- Answers that intentionally do not answer the question, such as 'I don't know', should also be counted as the most relevant.\n",
            "\n",
            "- Never elaborate.\n",
            "\n",
            "QUESTION: what is required to create real AI?\n",
            "\n",
            "STATEMENT: Though I liked programming, I didn't plan to study it in college. In college I\n",
            "was going to study philosophy, which sounded much more powerful. It seemed, to\n",
            "my naive high school self, to be the study of the ultimate truths, compared to\n",
            "which the things studied in other fields would be mere domain knowledge. What\n",
            "I discovered when I got to college was that the other fields took up so much\n",
            "of the space of ideas that there wasn't much left for these supposed ultimate\n",
            "truths. All that seemed left for philosophy were edge cases that people in\n",
            "other fields felt could safely be ignored.  \n",
            "  \n",
            "I couldn't have put this into words when I was 18. All I knew at the time was\n",
            "that I kept taking philosophy courses and they kept being boring. So I decided\n",
            "to switch to AI.  \n",
            "  \n",
            "AI was in the air in the mid 1980s, but there were two things especially that\n",
            "made me want to work on it: a novel by Heinlein called _The Moon is a Harsh\n",
            "Mistress_ , which featured an intelligent computer called Mike, and a PBS\n",
            "documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading\n",
            "_The Moon is a Harsh Mistress_ , so I don't know how well it has aged, but\n",
            "when I read it I was drawn entirely into its world. It seemed only a matter of\n",
            "time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed\n",
            "like that time would be a few years at most. All you had to do was teach\n",
            "SHRDLU more words.  \n",
            "  \n",
            "There weren't any classes in AI at Cornell then, not even graduate classes, so\n",
            "I started trying to teach myself. Which meant learning Lisp, since in those\n",
            "days Lisp was regarded as the language of AI. The commonly used programming\n",
            "languages then were pretty primitive, and programmers' ideas correspondingly\n",
            "so. The default language at Cornell was a Pascal-like language called PL/I,\n",
            "and the situation was similar elsewhere. Learning Lisp expanded my concept of\n",
            "a program so fast that it was years before I started to have a sense of where\n",
            "the new limits were. This was more like it; this was what I had expected\n",
            "college to do. It wasn't happening in a class, like it was supposed to, but\n",
            "that was ok. For the next couple years I was on a roll. I knew what I was\n",
            "going to do.  \n",
            "  \n",
            "For my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\n",
            "working on that program. It was a pleasing bit of code, but what made it even\n",
            "more exciting was my belief Â— hard to imagine now, but not unique in 1985 Â—\n",
            "that it was already climbing the lower slopes of intelligence.  \n",
            "  \n",
            "I had gotten into a program at Cornell that didn't make you choose a major.\n",
            "You could take whatever classes you liked, and choose whatever you liked to\n",
            "put on your degree. I of course chose \"Artificial Intelligence.\" When I got\n",
            "the actual physical diploma, I was dismayed to find that the quotes had been\n",
            "included, which made them read as scare-quotes. At the time this bothered me,\n",
            "but now it seems amusingly accurate, for reasons I was about to discover.  \n",
            "  \n",
            "I applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\n",
            "time, and Harvard, which I'd visited because Rich Draves went there, and was\n",
            "also home to Bill Woods, who'd invented the type of parser I used in my SHRDLU\n",
            "clone. Only Harvard accepted me, so that was where I went.  \n",
            "  \n",
            "I don't remember the moment it happened, or if there even was a specific\n",
            "moment, but during the first year of grad school I realized that AI, as\n",
            "practiced at the time, was a hoax. By which I mean the sort of AI in which a\n",
            "program that's told \"the dog is sitting on the chair\" translates this into\n",
            "some formal representation and adds it to the list of things it knows.  \n",
            "  \n",
            "What these programs really showed was that there's a subset of natural\n",
            "language that's a formal language. But a very proper subset. It was clear that\n",
            "there was an unbridgeable gap between what they could do and actually\n",
            "understanding natural language. It was not, in fact, simply a matter of\n",
            "teaching SHRDLU more words. That whole way of doing AI, with explicit data\n",
            "structures representing concepts, was not going to work. Its brokenness did,\n",
            "as so often happens, generate a lot of opportunities to write papers about\n",
            "various band-aids that could be applied to it, but it was never going to get\n",
            "us Mike.\n",
            "\n",
            "RELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\n",
            "\u001b[0m\n",
            "\n",
            "RAW RESPONSE:\n",
            "Statement Sentence: Understanding natural language and bridging the gap between formal representations and true comprehension are necessary to create real AI., \n",
            "Supporting Evidence: What these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language., \n",
            "Score: 10\n",
            "\n",
            "\n",
            "Logging Details Post-API Call: logger_fn - None | callable(logger_fn) - False\n",
            "Logging Details Post-API Call: LiteLLM Params: {'model': 'gemini-pro', 'messages': [{'role': 'system', 'content': 'You are a INFORMATION OVERLAP classifier providing the overlap of information between a SOURCE and STATEMENT.\\nFor every sentence in the statement, please answer with this template:\\n\\nTEMPLATE: \\nStatement Sentence: <Sentence>, \\nSupporting Evidence: <Choose the exact unchanged sentences in the source that can answer the statement, if nothing matches, say NOTHING FOUND>\\nScore: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\nGive me the INFORMATION OVERLAP of this SOURCE and STATEMENT.\\n\\nSOURCE: [\\'By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.  \\\\n  \\\\nSo I looked around to see what I could salvage from the wreckage of my plans,\\\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\\\nown sake and not just for its association with AI, even though that was the\\\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\\\nIn fact, I decided to write a book about Lisp hacking. It\\\\\\'s scary to think how\\\\nlittle I knew about Lisp hacking when I started writing that book. But there\\\\\\'s\\\\nnothing like writing a book about something to help you learn it. The book,\\\\n_On Lisp_ , wasn\\\\\\'t published till 1993, but I wrote much of it in grad school.  \\\\n  \\\\nComputer Science is an uneasy alliance between two halves, theory and systems.\\\\nThe theory people prove things, and the systems people build things. I wanted\\\\nto build things. I had plenty of respect for theory \\\\x97 indeed, a sneaking\\\\nsuspicion that it was the more admirable of the two halves \\\\x97 but building\\\\nthings seemed so much more exciting.  \\\\n  \\\\nThe problem with systems work, though, was that it didn\\\\\\'t last. Any program\\\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\\\nbest. People might mention your software in footnotes, but no one would\\\\nactually use it. And indeed, it would seem very feeble work. Only people with\\\\na sense of the history of the field would even realize that, in its time, it\\\\nhad been good.  \\\\n  \\\\nThere were some surplus Xerox Dandelions floating around the computer lab at\\\\none point. Anyone who wanted one to play around with could have one. I was\\\\nbriefly tempted, but they were so slow by present standards; what was the\\\\npoint? No one else wanted one either, so off they went. That was what happened\\\\nto systems work.  \\\\n  \\\\nI wanted not just to build things, but to build things that would last.  \\\\n  \\\\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\\\nI\\\\\\'d spent a lot of time as a kid. While looking at a painting there I realized\\\\nsomething that might seem obvious, but was a big surprise to me. There, right\\\\non the wall, was something you could make that would last. Paintings didn\\\\\\'t\\\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\\\n  \\\\nAnd moreover this was something you could make a living doing. Not as easily\\\\nas you could by writing software, of course, but I thought if you were really\\\\nindustrious and lived really cheaply, it had to be possible to make enough to\\\\nsurvive. And as an artist you could be truly independent. You wouldn\\\\\\'t have a\\\\nboss, or even need to get research funding.  \\\\n  \\\\nI had always liked looking at paintings. Could I make them? I had no idea. I\\\\\\'d\\\\nnever imagined it was even possible. I knew intellectually that people made\\\\nart \\\\x97 that it didn\\\\\\'t just appear spontaneously \\\\x97 but it was as if the people\\\\nwho made it were a different species. They either lived long ago or were\\\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\\\nidea of actually being able to make art, to put that verb before that noun,\\\\nseemed almost miraculous.  \\\\n  \\\\nThat fall I started taking art classes at Harvard. Grad students could take\\\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\\\nIf he even knew about the strange classes I was taking, he never said\\\\nanything.  \\\\n  \\\\nSo now I was in a PhD program in computer science, yet planning to be an\\\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\\\nLisp_.\\', \\'Though I liked programming, I didn\\\\\\'t plan to study it in college. In college I\\\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\\\nmy naive high school self, to be the study of the ultimate truths, compared to\\\\nwhich the things studied in other fields would be mere domain knowledge. What\\\\nI discovered when I got to college was that the other fields took up so much\\\\nof the space of ideas that there wasn\\\\\\'t much left for these supposed ultimate\\\\ntruths. All that seemed left for philosophy were edge cases that people in\\\\nother fields felt could safely be ignored.  \\\\n  \\\\nI couldn\\\\\\'t have put this into words when I was 18. All I knew at the time was\\\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\\\nto switch to AI.  \\\\n  \\\\nAI was in the air in the mid 1980s, but there were two things especially that\\\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\\\\\'t tried rereading\\\\n_The Moon is a Harsh Mistress_ , so I don\\\\\\'t know how well it has aged, but\\\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\\\ntime before we\\\\\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\\\nlike that time would be a few years at most. All you had to do was teach\\\\nSHRDLU more words.  \\\\n  \\\\nThere weren\\\\\\'t any classes in AI at Cornell then, not even graduate classes, so\\\\nI started trying to teach myself. Which meant learning Lisp, since in those\\\\ndays Lisp was regarded as the language of AI. The commonly used programming\\\\nlanguages then were pretty primitive, and programmers\\\\\\' ideas correspondingly\\\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\\\na program so fast that it was years before I started to have a sense of where\\\\nthe new limits were. This was more like it; this was what I had expected\\\\ncollege to do. It wasn\\\\\\'t happening in a class, like it was supposed to, but\\\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\\\ngoing to do.  \\\\n  \\\\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\\\nworking on that program. It was a pleasing bit of code, but what made it even\\\\nmore exciting was my belief \\\\x97 hard to imagine now, but not unique in 1985 \\\\x97\\\\nthat it was already climbing the lower slopes of intelligence.  \\\\n  \\\\nI had gotten into a program at Cornell that didn\\\\\\'t make you choose a major.\\\\nYou could take whatever classes you liked, and choose whatever you liked to\\\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\\\n  \\\\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\\\ntime, and Harvard, which I\\\\\\'d visited because Rich Draves went there, and was\\\\nalso home to Bill Woods, who\\\\\\'d invented the type of parser I used in my SHRDLU\\\\nclone. Only Harvard accepted me, so that was where I went.  \\\\n  \\\\nI don\\\\\\'t remember the moment it happened, or if there even was a specific\\\\nmoment, but during the first year of grad school I realized that AI, as\\\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.\\']\\n\\nSTATEMENT: Understanding natural language and bridging the gap between formal representations and true comprehension are necessary to create real AI. The traditional approach of using explicit data structures to represent concepts is not sufficient.\\n'}], 'optional_params': {}, 'litellm_params': {'acompletion': False, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'vertex_ai', 'api_base': None, 'litellm_call_id': '10b56389-50d1-4e06-ae0d-76f557fa6706', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'stream_response': {}}, 'start_time': datetime.datetime(2023, 12, 21, 19, 44, 46, 308033), 'stream': False, 'user': None, 'call_type': 'completion', 'input': 'You are a INFORMATION OVERLAP classifier providing the overlap of information between a SOURCE and STATEMENT.\\nFor every sentence in the statement, please answer with this template:\\n\\nTEMPLATE: \\nStatement Sentence: <Sentence>, \\nSupporting Evidence: <Choose the exact unchanged sentences in the source that can answer the statement, if nothing matches, say NOTHING FOUND>\\nScore: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\nGive me the INFORMATION OVERLAP of this SOURCE and STATEMENT.\\n\\nSOURCE: [\\'By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.  \\\\n  \\\\nSo I looked around to see what I could salvage from the wreckage of my plans,\\\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\\\nown sake and not just for its association with AI, even though that was the\\\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\\\nIn fact, I decided to write a book about Lisp hacking. It\\\\\\'s scary to think how\\\\nlittle I knew about Lisp hacking when I started writing that book. But there\\\\\\'s\\\\nnothing like writing a book about something to help you learn it. The book,\\\\n_On Lisp_ , wasn\\\\\\'t published till 1993, but I wrote much of it in grad school.  \\\\n  \\\\nComputer Science is an uneasy alliance between two halves, theory and systems.\\\\nThe theory people prove things, and the systems people build things. I wanted\\\\nto build things. I had plenty of respect for theory \\\\x97 indeed, a sneaking\\\\nsuspicion that it was the more admirable of the two halves \\\\x97 but building\\\\nthings seemed so much more exciting.  \\\\n  \\\\nThe problem with systems work, though, was that it didn\\\\\\'t last. Any program\\\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\\\nbest. People might mention your software in footnotes, but no one would\\\\nactually use it. And indeed, it would seem very feeble work. Only people with\\\\na sense of the history of the field would even realize that, in its time, it\\\\nhad been good.  \\\\n  \\\\nThere were some surplus Xerox Dandelions floating around the computer lab at\\\\none point. Anyone who wanted one to play around with could have one. I was\\\\nbriefly tempted, but they were so slow by present standards; what was the\\\\npoint? No one else wanted one either, so off they went. That was what happened\\\\nto systems work.  \\\\n  \\\\nI wanted not just to build things, but to build things that would last.  \\\\n  \\\\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\\\nI\\\\\\'d spent a lot of time as a kid. While looking at a painting there I realized\\\\nsomething that might seem obvious, but was a big surprise to me. There, right\\\\non the wall, was something you could make that would last. Paintings didn\\\\\\'t\\\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\\\n  \\\\nAnd moreover this was something you could make a living doing. Not as easily\\\\nas you could by writing software, of course, but I thought if you were really\\\\nindustrious and lived really cheaply, it had to be possible to make enough to\\\\nsurvive. And as an artist you could be truly independent. You wouldn\\\\\\'t have a\\\\nboss, or even need to get research funding.  \\\\n  \\\\nI had always liked looking at paintings. Could I make them? I had no idea. I\\\\\\'d\\\\nnever imagined it was even possible. I knew intellectually that people made\\\\nart \\\\x97 that it didn\\\\\\'t just appear spontaneously \\\\x97 but it was as if the people\\\\nwho made it were a different species. They either lived long ago or were\\\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\\\nidea of actually being able to make art, to put that verb before that noun,\\\\nseemed almost miraculous.  \\\\n  \\\\nThat fall I started taking art classes at Harvard. Grad students could take\\\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\\\nIf he even knew about the strange classes I was taking, he never said\\\\nanything.  \\\\n  \\\\nSo now I was in a PhD program in computer science, yet planning to be an\\\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\\\nLisp_.\\', \\'Though I liked programming, I didn\\\\\\'t plan to study it in college. In college I\\\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\\\nmy naive high school self, to be the study of the ultimate truths, compared to\\\\nwhich the things studied in other fields would be mere domain knowledge. What\\\\nI discovered when I got to college was that the other fields took up so much\\\\nof the space of ideas that there wasn\\\\\\'t much left for these supposed ultimate\\\\ntruths. All that seemed left for philosophy were edge cases that people in\\\\nother fields felt could safely be ignored.  \\\\n  \\\\nI couldn\\\\\\'t have put this into words when I was 18. All I knew at the time was\\\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\\\nto switch to AI.  \\\\n  \\\\nAI was in the air in the mid 1980s, but there were two things especially that\\\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\\\\\'t tried rereading\\\\n_The Moon is a Harsh Mistress_ , so I don\\\\\\'t know how well it has aged, but\\\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\\\ntime before we\\\\\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\\\nlike that time would be a few years at most. All you had to do was teach\\\\nSHRDLU more words.  \\\\n  \\\\nThere weren\\\\\\'t any classes in AI at Cornell then, not even graduate classes, so\\\\nI started trying to teach myself. Which meant learning Lisp, since in those\\\\ndays Lisp was regarded as the language of AI. The commonly used programming\\\\nlanguages then were pretty primitive, and programmers\\\\\\' ideas correspondingly\\\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\\\na program so fast that it was years before I started to have a sense of where\\\\nthe new limits were. This was more like it; this was what I had expected\\\\ncollege to do. It wasn\\\\\\'t happening in a class, like it was supposed to, but\\\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\\\ngoing to do.  \\\\n  \\\\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\\\nworking on that program. It was a pleasing bit of code, but what made it even\\\\nmore exciting was my belief \\\\x97 hard to imagine now, but not unique in 1985 \\\\x97\\\\nthat it was already climbing the lower slopes of intelligence.  \\\\n  \\\\nI had gotten into a program at Cornell that didn\\\\\\'t make you choose a major.\\\\nYou could take whatever classes you liked, and choose whatever you liked to\\\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\\\n  \\\\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\\\ntime, and Harvard, which I\\\\\\'d visited because Rich Draves went there, and was\\\\nalso home to Bill Woods, who\\\\\\'d invented the type of parser I used in my SHRDLU\\\\nclone. Only Harvard accepted me, so that was where I went.  \\\\n  \\\\nI don\\\\\\'t remember the moment it happened, or if there even was a specific\\\\nmoment, but during the first year of grad school I realized that AI, as\\\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.\\']\\n\\nSTATEMENT: Understanding natural language and bridging the gap between formal representations and true comprehension are necessary to create real AI. The traditional approach of using explicit data structures to represent concepts is not sufficient.\\n', 'api_key': None, 'additional_args': {}, 'log_event_type': 'post_api_call', 'original_response': \"Statement Sentence: Understanding natural language and bridging the gap between formal representations and true comprehension are necessary to create real AI., \\nSupporting Evidence: What these programs really showed was that there\\\\'s a subset of natural\\\\nlanguage that\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language., \\nScore: 10\"}\n",
            "Wrapper: Completed Call, calling success_handler\n",
            "Logging Details LiteLLM-Success Call\n",
            "success callbacks: []\n",
            "RAW RESPONSE:\n",
            "7\n",
            "\n",
            "\n",
            "Logging Details Post-API Call: logger_fn - None | callable(logger_fn) - False\n",
            "Logging Details Post-API Call: LiteLLM Params: {'model': 'gemini-pro', 'messages': [{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: what is required to create real AI?\\n\\nSTATEMENT: Though I liked programming, I didn\\'t plan to study it in college. In college I\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\nmy naive high school self, to be the study of the ultimate truths, compared to\\nwhich the things studied in other fields would be mere domain knowledge. What\\nI discovered when I got to college was that the other fields took up so much\\nof the space of ideas that there wasn\\'t much left for these supposed ultimate\\ntruths. All that seemed left for philosophy were edge cases that people in\\nother fields felt could safely be ignored.  \\n  \\nI couldn\\'t have put this into words when I was 18. All I knew at the time was\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\nto switch to AI.  \\n  \\nAI was in the air in the mid 1980s, but there were two things especially that\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading\\n_The Moon is a Harsh Mistress_ , so I don\\'t know how well it has aged, but\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\ntime before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\nlike that time would be a few years at most. All you had to do was teach\\nSHRDLU more words.  \\n  \\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so\\nI started trying to teach myself. Which meant learning Lisp, since in those\\ndays Lisp was regarded as the language of AI. The commonly used programming\\nlanguages then were pretty primitive, and programmers\\' ideas correspondingly\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\na program so fast that it was years before I started to have a sense of where\\nthe new limits were. This was more like it; this was what I had expected\\ncollege to do. It wasn\\'t happening in a class, like it was supposed to, but\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\ngoing to do.  \\n  \\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\nworking on that program. It was a pleasing bit of code, but what made it even\\nmore exciting was my belief \\x97 hard to imagine now, but not unique in 1985 \\x97\\nthat it was already climbing the lower slopes of intelligence.  \\n  \\nI had gotten into a program at Cornell that didn\\'t make you choose a major.\\nYou could take whatever classes you liked, and choose whatever you liked to\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\n  \\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\ntime, and Harvard, which I\\'d visited because Rich Draves went there, and was\\nalso home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU\\nclone. Only Harvard accepted me, so that was where I went.  \\n  \\nI don\\'t remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.\\n\\nRELEVANCE: '}], 'optional_params': {}, 'litellm_params': {'acompletion': False, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'vertex_ai', 'api_base': None, 'litellm_call_id': 'bd4aa175-158d-46bf-95c5-c1833708e27e', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'stream_response': {}}, 'start_time': datetime.datetime(2023, 12, 21, 19, 44, 49, 506069), 'stream': False, 'user': None, 'call_type': 'completion', 'input': 'You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: what is required to create real AI?\\n\\nSTATEMENT: Though I liked programming, I didn\\'t plan to study it in college. In college I\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\nmy naive high school self, to be the study of the ultimate truths, compared to\\nwhich the things studied in other fields would be mere domain knowledge. What\\nI discovered when I got to college was that the other fields took up so much\\nof the space of ideas that there wasn\\'t much left for these supposed ultimate\\ntruths. All that seemed left for philosophy were edge cases that people in\\nother fields felt could safely be ignored.  \\n  \\nI couldn\\'t have put this into words when I was 18. All I knew at the time was\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\nto switch to AI.  \\n  \\nAI was in the air in the mid 1980s, but there were two things especially that\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading\\n_The Moon is a Harsh Mistress_ , so I don\\'t know how well it has aged, but\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\ntime before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\nlike that time would be a few years at most. All you had to do was teach\\nSHRDLU more words.  \\n  \\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so\\nI started trying to teach myself. Which meant learning Lisp, since in those\\ndays Lisp was regarded as the language of AI. The commonly used programming\\nlanguages then were pretty primitive, and programmers\\' ideas correspondingly\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\na program so fast that it was years before I started to have a sense of where\\nthe new limits were. This was more like it; this was what I had expected\\ncollege to do. It wasn\\'t happening in a class, like it was supposed to, but\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\ngoing to do.  \\n  \\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\nworking on that program. It was a pleasing bit of code, but what made it even\\nmore exciting was my belief \\x97 hard to imagine now, but not unique in 1985 \\x97\\nthat it was already climbing the lower slopes of intelligence.  \\n  \\nI had gotten into a program at Cornell that didn\\'t make you choose a major.\\nYou could take whatever classes you liked, and choose whatever you liked to\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\n  \\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\ntime, and Harvard, which I\\'d visited because Rich Draves went there, and was\\nalso home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU\\nclone. Only Harvard accepted me, so that was where I went.  \\n  \\nI don\\'t remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.\\n\\nRELEVANCE: ', 'api_key': None, 'additional_args': {}, 'log_event_type': 'post_api_call', 'original_response': '7'}\n",
            "Wrapper: Completed Call, calling success_handler\n",
            "Logging Details LiteLLM-Success Call\n",
            "success callbacks: []\n"
          ]
        }
      ],
      "source": [
        "from trulens_eval.appui import AppUI\n",
        "\n",
        "aui = AppUI(\n",
        "    app=tru_app2,\n",
        "    \n",
        "    app_selectors=[\n",
        "    ],\n",
        "    record_selectors=[\n",
        "        \"app.retriever.retrieve[0].rets[:].score\",\n",
        "        \"app.retriever.retrieve[0].rets[:].node.text\",\n",
        "    ]\n",
        ")\n",
        "aui.widget"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.4 ('agents')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06bbd7e875f346958667df732f7cf574": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0db6fb29f4f64118ae9b9d6e81e15ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13931b3e193749009caa7ef4ccea7967",
            "max": 23,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06bbd7e875f346958667df732f7cf574",
            "value": 23
          }
        },
        "0fa685503e964fafadde5abcf202bb1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13931b3e193749009caa7ef4ccea7967": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20552da6c2b8436cbba7b3ce6338b295": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b0048f4b9124784b5e767bcd7533683",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e1741a85a9c74025a83cb815a694eed6",
            "value": "Parsing nodes: 100%"
          }
        },
        "359486ce32b047b4a34dcf42e0b81004": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38773a9b07e24e0789412377b84518b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42abd41bb549421f8680517f97b03b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20552da6c2b8436cbba7b3ce6338b295",
              "IPY_MODEL_5ecef3a5be5c4941ab0d4b34f7e5fa77",
              "IPY_MODEL_ef1e644267134c718ad62bb53d0c63f1"
            ],
            "layout": "IPY_MODEL_359486ce32b047b4a34dcf42e0b81004"
          }
        },
        "51d5bafd273b4c7bbe169f8d27462285": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b0048f4b9124784b5e767bcd7533683": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ecef3a5be5c4941ab0d4b34f7e5fa77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69c8a60a92f24832bd582dcf78738129",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e5804d74c1e4333b9f95122fb8c96cc",
            "value": 1
          }
        },
        "5fe8bc5f962a49b8a9278fa6cdadcc21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cde791ada994503ad415364d6d33890",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_51d5bafd273b4c7bbe169f8d27462285",
            "value": " 23/23 [00:11&lt;00:00,  1.90it/s]"
          }
        },
        "69c8a60a92f24832bd582dcf78738129": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e5804d74c1e4333b9f95122fb8c96cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a7ffefe09db42a0bd028109a64909a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cde791ada994503ad415364d6d33890": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b812c97e5934475b3316dada1fd992e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc2842c56d6d4fffb6473dd24e453e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec0ab4f5547a46cd85998addd008cd64",
              "IPY_MODEL_0db6fb29f4f64118ae9b9d6e81e15ef0",
              "IPY_MODEL_5fe8bc5f962a49b8a9278fa6cdadcc21"
            ],
            "layout": "IPY_MODEL_dfddcccae608466980b729effdd117f0"
          }
        },
        "dfddcccae608466980b729effdd117f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1741a85a9c74025a83cb815a694eed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec0ab4f5547a46cd85998addd008cd64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a7ffefe09db42a0bd028109a64909a6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0fa685503e964fafadde5abcf202bb1e",
            "value": "Generating embeddings: 100%"
          }
        },
        "ef1e644267134c718ad62bb53d0c63f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b812c97e5934475b3316dada1fd992e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_38773a9b07e24e0789412377b84518b7",
            "value": " 1/1 [00:00&lt;00:00,  7.25it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
