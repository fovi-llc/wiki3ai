{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnZ2fqegSEBk"
      },
      "source": [
        "# Llama-Index Quickstart\n",
        "\n",
        "In this quickstart you will create a simple Llama Index App and learn how to log it and get feedback on an LLM response.\n",
        "\n",
        "For evaluation, we will leverage the \"hallucination triad\" of groundedness, context relevance and answer relevance.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/quickstart/llama_index_quickstart.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz7cLHMNSEBo"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Install dependencies\n",
        "Let's install some of the dependencies for this notebook if we don't have them already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXK0D31TSEBp",
        "outputId": "c6ab9eb7-d8a8-4917-d106-1f968d1759a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU \"trulens_eval>=0.19.2\" \"llama_index>0.9.17\" \"html2text>=2020.1.16\" qdrant_client python-dotenv ipywidgets streamlit_jupyter \"litellm>=1.15.1\" google-cloud-aiplatform\n",
        "# 'google-generativeai>=0.3.0'\n",
        "# %pip install -qU trulens_eval==0.19.1 llama_index>0.9.15 html2text>=2020.1.16 qdrant_client python-dotenv\n",
        "\n",
        "%load_ext dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-cloud-core in ./.venv/lib/python3.11/site-packages (2.4.1)\n",
            "Requirement already satisfied: google-cloud-aiplatform in ./.venv/lib/python3.11/site-packages (1.38.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6 in ./.venv/lib/python3.11/site-packages (from google-cloud-core) (2.15.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in ./.venv/lib/python3.11/site-packages (from google-cloud-core) (2.25.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (4.25.1)\n",
            "Requirement already satisfied: packaging>=14.3 in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (23.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (2.14.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (3.14.1)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (1.11.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in ./.venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (2.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in ./.venv/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (1.62.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in ./.venv/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in ./.venv/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in ./.venv/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-core) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-core) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-core) (4.9)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in ./.venv/lib/python3.11/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in ./.venv/lib/python3.11/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in ./.venv/lib/python3.11/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in ./.venv/lib/python3.11/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.14 in ./.venv/lib/python3.11/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.26.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./.venv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-core) (0.5.1)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (2023.11.17)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U google-cloud-core google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxB8G6jiSEBr"
      },
      "source": [
        "### Add API keys\n",
        "For this quickstart, you will need Open AI and Huggingface keys. The OpenAI key is used for embeddings and GPT, and the Huggingface key is used for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zlv9qBKjSEBr"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from google.colab import userdata\n",
        "# GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "%load_ext dotenv\n",
        "%dotenv\n",
        "GOOGLE_API_KEY = os.environ[\"GEMINI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "# This is used by the LiteLLM for Vertex AI models including Gemini.\n",
        "# The LiteLLM wrapper for Gemini is used by the TruLens evaluation provider.\n",
        "aiplatform.init(project=\"fovi-site\", location=\"us-west1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkZE-pLzSEBr"
      },
      "source": [
        "### Import from LlamaIndex and TruLens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0CLC5QBSEBs",
        "outputId": "d10aad25-0ea9-4d49-a2eb-86d7cf814711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
            "ðŸ”’ Secret keys will not be included in the database.\n"
          ]
        }
      ],
      "source": [
        "from trulens_eval import Tru\n",
        "\n",
        "tru = Tru(database_redact_keys=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LivWqTvSEBt"
      },
      "source": [
        "### Create Simple LLM Application\n",
        "\n",
        "This example uses LlamaIndex which internally uses an OpenAI LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dokLMzEjMxke",
        "outputId": "c5f5b6b9-ca64-4f08-ff15-9c48f56d06d4"
      },
      "outputs": [],
      "source": [
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "\n",
        "DOCUMENTS = SimpleWebPageReader(html_to_text=True).load_data([\"http://paulgraham.com/worked.html\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "42abd41bb549421f8680517f97b03b8f",
            "20552da6c2b8436cbba7b3ce6338b295",
            "5ecef3a5be5c4941ab0d4b34f7e5fa77",
            "ef1e644267134c718ad62bb53d0c63f1",
            "359486ce32b047b4a34dcf42e0b81004",
            "5b0048f4b9124784b5e767bcd7533683",
            "e1741a85a9c74025a83cb815a694eed6",
            "69c8a60a92f24832bd582dcf78738129",
            "7e5804d74c1e4333b9f95122fb8c96cc",
            "9b812c97e5934475b3316dada1fd992e",
            "38773a9b07e24e0789412377b84518b7",
            "cc2842c56d6d4fffb6473dd24e453e5f",
            "ec0ab4f5547a46cd85998addd008cd64",
            "0db6fb29f4f64118ae9b9d6e81e15ef0",
            "5fe8bc5f962a49b8a9278fa6cdadcc21",
            "dfddcccae608466980b729effdd117f0",
            "8a7ffefe09db42a0bd028109a64909a6",
            "0fa685503e964fafadde5abcf202bb1e",
            "13931b3e193749009caa7ef4ccea7967",
            "06bbd7e875f346958667df732f7cf574",
            "8cde791ada994503ad415364d6d33890",
            "51d5bafd273b4c7bbe169f8d27462285"
          ]
        },
        "id": "tr7YetUgAeMn",
        "outputId": "21004cf0-2a79-4bae-fd70-ba616f504c1d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddaa50e226ea4afebb56ea87420f1f70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10c364410e474a8882a9a8ea608e0a40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings:   0%|          | 0/23 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import qdrant_client\n",
        "from llama_index import ServiceContext, StorageContext, VectorStoreIndex\n",
        "from llama_index import StorageContext\n",
        "from llama_index.embeddings import GeminiEmbedding\n",
        "from llama_index.llms import Gemini\n",
        "from llama_index.vector_stores import QdrantVectorStore\n",
        "\n",
        "\n",
        "def load_llamaindex_app():\n",
        "    # Create a local Qdrant vector store\n",
        "    # client = qdrant_client.QdrantClient(path=\"qdrant_gemini_3\")\n",
        "    client = qdrant_client.QdrantClient(location=\":memory:\")\n",
        "\n",
        "    vector_store = QdrantVectorStore(client=client, collection_name=\"collection\")\n",
        "    # Using the embedding model to Gemini\n",
        "    embed_model = GeminiEmbedding(model_name=\"models/embedding-001\", api_key=GOOGLE_API_KEY)\n",
        "\n",
        "    service_context = ServiceContext.from_defaults(\n",
        "        llm=Gemini(api_key=GOOGLE_API_KEY), embed_model=embed_model\n",
        "    )\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        DOCUMENTS,\n",
        "        service_context=service_context,\n",
        "        storage_context=storage_context,\n",
        "        show_progress=True,\n",
        "    )\n",
        "\n",
        "    return index.as_query_engine()\n",
        "\n",
        "query_engine = load_llamaindex_app()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1xzZiw1SEBu"
      },
      "source": [
        "### Send your first request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ZCi6n6xoSEBv",
        "outputId": "82ecc05a-5597-49a7-a9af-f29b38d52c49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The author initially studied philosophy in college but found it boring and switched to AI. They then pursued a PhD in computer science at Harvard while also taking art classes. After completing their PhD, they applied to art schools and was accepted into the BFA program at RISD. They also received an invitation to take the entrance exam at the Accademia di Belli Arti in Florence, which they passed. However, the author ultimately decided to attend RISD.\n"
          ]
        }
      ],
      "source": [
        "RESPONSE = query_engine.query(\"What does the author say about their education?\")\n",
        "print(RESPONSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "12GcVxvmPucp",
        "outputId": "7ff9e226-36d3-48a1-e4d5-9de64047dd02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The author went to Harvard for a PhD program in computer science, RISD for a BFA program, and the Accademia di Belli Arti in Florence for an entrance exam.\n"
          ]
        }
      ],
      "source": [
        "RESPONSE = query_engine.query(\"Where did the author go to school?\")\n",
        "print(RESPONSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "NE4h0I0HP6cr",
        "outputId": "d6c87911-7a26-4a95-d750-b677ae8fb972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The provided context does not mention the author's Harvard PhD advisor, so I cannot answer this question.\n"
          ]
        }
      ],
      "source": [
        "RESPONSE = query_engine.query(\"Who was the author's Harvard PhD advisor?\")\n",
        "print(RESPONSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "vSSZLBHFQuKX",
        "outputId": "f7a988ee-0bb0-4156-d656-8b198d14c174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tom Cheatham was the author's advisor in the PhD program in computer science at Harvard.\n"
          ]
        }
      ],
      "source": [
        "RESPONSE = query_engine.query(\"who was Tom Cheatham to the author?\")\n",
        "print(RESPONSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "7rqlmfTfQ7a3",
        "outputId": "231a795a-a8c3-4f03-ed5e-5c588d93acfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tom Cheatham is a professor at Harvard University. He is mentioned in the story because the narrator was taking art classes at Harvard while pursuing a PhD in computer science. Tom Cheatham was the narrator's advisor, and he was very easy-going about the narrator's choice to take art classes. He never said anything about it, even though it was unusual for a PhD student to be taking art classes.\n"
          ]
        }
      ],
      "source": [
        "RESPONSE = query_engine.query(\"who is Tom? why is he in this story?\")\n",
        "print(RESPONSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "e7NHvID2Sm_A",
        "outputId": "0cf35819-cd40-4348-8ca1-61fb37c4230e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This story is about the author's journey from being a philosophy student to becoming an AI researcher. The author initially believed that philosophy was the study of ultimate truths, but later realized that it was mostly concerned with edge cases that other fields ignored. The author then switched to AI, inspired by a novel and a documentary. However, the author later realized that the AI research at the time was a hoax, as it was limited to a small subset of formal language and could not truly understand natural language.\n",
            "\n",
            "The most important things the author wants the reader to learn are:\n",
            "- The limitations of AI research at the time, particularly the inability to truly understand natural language.\n",
            "- The importance of having realistic expectations about what AI can achieve.\n",
            "- The need for a new approach to AI research that goes beyond the traditional methods of representing concepts with explicit data structures.\n"
          ]
        }
      ],
      "source": [
        "RESPONSE = query_engine.query(\n",
        "    \"what is this story about?  what are the most important things the author want the reader to learn?\"\n",
        ")\n",
        "\n",
        "print(RESPONSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cMfasabSEBv"
      },
      "source": [
        "## Initialize Feedback Function(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gEMHas7SEBv",
        "outputId": "912e9f1a-81db-4365-ef76-2fe69ca37699"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… In groundedness_measure_with_cot_reasons, input source will be set to __record__.app.query.rets.source_nodes[:].node.text.collect() .\n",
            "âœ… In groundedness_measure_with_cot_reasons, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "âœ… In relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "âœ… In relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "âœ… In qs_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
            "âœ… In qs_relevance, input statement will be set to __record__.app.query.rets.source_nodes[:].node.text .\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from trulens_eval import Feedback, LiteLLM, TruLlama\n",
        "from trulens_eval.feedback import Groundedness\n",
        "\n",
        "# Initialize provider class\n",
        "GEMINI_PROVIDER = LiteLLM(model_engine=\"gemini-pro\")\n",
        "\n",
        "GROUNDED = Groundedness(groundedness_provider=GEMINI_PROVIDER)\n",
        "\n",
        "# Define a groundedness feedback function\n",
        "f_groundedness = (\n",
        "    Feedback(GROUNDED.groundedness_measure_with_cot_reasons)\n",
        "    .on(TruLlama.select_source_nodes().node.text.collect())\n",
        "    .on_output()\n",
        "    .aggregate(GROUNDED.grounded_statements_aggregator)\n",
        ")\n",
        "\n",
        "# Question/answer relevance between overall question and answer.\n",
        "f_groundedness = Feedback(GEMINI_PROVIDER.relevance).on_input_output()\n",
        "\n",
        "# Question/statement relevance between question and each context chunk.\n",
        "f_qs_relevance = (\n",
        "    Feedback(GEMINI_PROVIDER.qs_relevance)\n",
        "    .on_input()\n",
        "    .on(TruLlama.select_source_nodes().node.text)\n",
        "    .aggregate(np.mean)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY4Lw8wmSEBv"
      },
      "source": [
        "## Instrument app for logging with TruLens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CwW2IJ8XSEBv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<function load_llamaindex_app at 0x154dc6b60>\n"
          ]
        }
      ],
      "source": [
        "tru_query_engine_recorder = TruLlama(\n",
        "    query_engine,\n",
        "    tru=tru,\n",
        "    app_id=\"PaulGrahamB\", initial_app_loader=load_llamaindex_app,\n",
        "    feedbacks=[f_groundedness, f_groundedness, f_qs_relevance],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mymodel = tru_query_engine_recorder.model_dump()\n",
        "tru_query_engine_recorder.model_validate(mymodel)\n",
        "# Beware, this can expose the API key(s) in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<function load_llamaindex_app at 0x154dc6b60>\n"
          ]
        }
      ],
      "source": [
        "tru_query_engine_recorder = tru.Llama(\n",
        "    query_engine,\n",
        "    app_id=\"PaulGrahamC\", initial_app_loader=load_llamaindex_app,\n",
        "    feedbacks=[f_groundedness, f_groundedness, f_qs_relevance],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mw-HUSugSEBv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The author dropped AI because they realized that the way AI was practiced at the time was a hoax. They believed that the whole way of doing AI, with explicit data structures representing concepts, was not going to work and would never lead to the creation of truly intelligent machines like Mike from the novel _The Moon is a Harsh Mistress_.\n"
          ]
        }
      ],
      "source": [
        "# or as context manager\n",
        "with tru_query_engine_recorder as _:\n",
        "    response = query_engine.query(\"Why did the author drop AI?\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdZLMRZtSEBv"
      },
      "source": [
        "## Explore in a Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'tru_class_info': {'name': 'TruLlama',\n",
              "   'module': {'package_name': 'trulens_eval',\n",
              "    'module_name': 'trulens_eval.tru_llama'},\n",
              "   'bases': [{'name': 'TruLlama',\n",
              "     'module': {'package_name': 'trulens_eval',\n",
              "      'module_name': 'trulens_eval.tru_llama'},\n",
              "     'bases': None},\n",
              "    {'name': 'App',\n",
              "     'module': {'package_name': 'trulens_eval',\n",
              "      'module_name': 'trulens_eval.app'},\n",
              "     'bases': None},\n",
              "    {'name': 'AppDefinition',\n",
              "     'module': {'package_name': 'trulens_eval',\n",
              "      'module_name': 'trulens_eval.schema'},\n",
              "     'bases': None},\n",
              "    {'name': 'SerialModel',\n",
              "     'module': {'package_name': 'trulens_eval.utils',\n",
              "      'module_name': 'trulens_eval.utils.serial'},\n",
              "     'bases': None},\n",
              "    {'name': 'WithClassInfo',\n",
              "     'module': {'package_name': 'trulens_eval.utils',\n",
              "      'module_name': 'trulens_eval.utils.pyschema'},\n",
              "     'bases': None},\n",
              "    {'name': 'BaseModel',\n",
              "     'module': {'package_name': 'pydantic', 'module_name': 'pydantic.main'},\n",
              "     'bases': None},\n",
              "    {'name': 'WithInstrumentCallbacks',\n",
              "     'module': {'package_name': 'trulens_eval',\n",
              "      'module_name': 'trulens_eval.instruments'},\n",
              "     'bases': None},\n",
              "    {'name': 'Hashable',\n",
              "     'module': {'package_name': 'collections',\n",
              "      'module_name': 'collections.abc'},\n",
              "     'bases': None},\n",
              "    {'name': 'Generic',\n",
              "     'module': {'package_name': '', 'module_name': 'typing'},\n",
              "     'bases': None},\n",
              "    {'name': 'object',\n",
              "     'module': {'package_name': '', 'module_name': 'builtins'},\n",
              "     'bases': None}]},\n",
              "  'app_id': 'PaulGrahamB',\n",
              "  'tags': '-',\n",
              "  'metadata': {},\n",
              "  'feedback_definitions': [],\n",
              "  'feedback_mode': 'with_app_thread',\n",
              "  'root_class': {'name': 'RetrieverQueryEngine',\n",
              "   'module': {'package_name': 'llama_index.query_engine',\n",
              "    'module_name': 'llama_index.query_engine.retriever_query_engine'},\n",
              "   'bases': None},\n",
              "  'app': {'__tru_non_serialized_object': {'cls': {'name': 'RetrieverQueryEngine',\n",
              "     'module': {'package_name': 'llama_index.query_engine',\n",
              "      'module_name': 'llama_index.query_engine.retriever_query_engine'},\n",
              "     'bases': None},\n",
              "    'id': 6060963088,\n",
              "    'init_bindings': None}},\n",
              "  'initial_app_loader_dump': {'data': {'__tru_non_serialized_object': {'cls': {'name': 'bytes',\n",
              "      'module': {'package_name': '', 'module_name': 'builtins'},\n",
              "      'bases': None},\n",
              "     'id': 4831969280,\n",
              "     'init_bindings': None,\n",
              "     'len': 78609}}},\n",
              "  'app_extra_json': {}},\n",
              " {'tru_class_info': {'name': 'TruLlama',\n",
              "   'module': {'package_name': 'trulens_eval',\n",
              "    'module_name': 'trulens_eval.tru_llama'},\n",
              "   'bases': [{'name': 'TruLlama',\n",
              "     'module': {'package_name': 'trulens_eval',\n",
              "      'module_name': 'trulens_eval.tru_llama'},\n",
              "     'bases': None},\n",
              "    {'name': 'App',\n",
              "     'module': {'package_name': 'trulens_eval',\n",
              "      'module_name': 'trulens_eval.app'},\n",
              "     'bases': None},\n",
              "    {'name': 'AppDefinition',\n",
              "     'module': {'package_name': 'trulens_eval',\n",
              "      'module_name': 'trulens_eval.schema'},\n",
              "     'bases': None},\n",
              "    {'name': 'SerialModel',\n",
              "     'module': {'package_name': 'trulens_eval.utils',\n",
              "      'module_name': 'trulens_eval.utils.serial'},\n",
              "     'bases': None},\n",
              "    {'name': 'WithClassInfo',\n",
              "     'module': {'package_name': 'trulens_eval.utils',\n",
              "      'module_name': 'trulens_eval.utils.pyschema'},\n",
              "     'bases': None},\n",
              "    {'name': 'BaseModel',\n",
              "     'module': {'package_name': 'pydantic', 'module_name': 'pydantic.main'},\n",
              "     'bases': None},\n",
              "    {'name': 'WithInstrumentCallbacks',\n",
              "     'module': {'package_name': 'trulens_eval',\n",
              "      'module_name': 'trulens_eval.instruments'},\n",
              "     'bases': None},\n",
              "    {'name': 'Hashable',\n",
              "     'module': {'package_name': 'collections',\n",
              "      'module_name': 'collections.abc'},\n",
              "     'bases': None},\n",
              "    {'name': 'Generic',\n",
              "     'module': {'package_name': '', 'module_name': 'typing'},\n",
              "     'bases': None},\n",
              "    {'name': 'object',\n",
              "     'module': {'package_name': '', 'module_name': 'builtins'},\n",
              "     'bases': None}]},\n",
              "  'app_id': 'PaulGrahamC',\n",
              "  'tags': '-',\n",
              "  'metadata': {},\n",
              "  'feedback_definitions': [],\n",
              "  'feedback_mode': 'with_app_thread',\n",
              "  'root_class': {'name': 'RetrieverQueryEngine',\n",
              "   'module': {'package_name': 'llama_index.query_engine',\n",
              "    'module_name': 'llama_index.query_engine.retriever_query_engine'},\n",
              "   'bases': None},\n",
              "  'app': {'__tru_non_serialized_object': {'cls': {'name': 'RetrieverQueryEngine',\n",
              "     'module': {'package_name': 'llama_index.query_engine',\n",
              "      'module_name': 'llama_index.query_engine.retriever_query_engine'},\n",
              "     'bases': None},\n",
              "    'id': 6060963088,\n",
              "    'init_bindings': None}},\n",
              "  'initial_app_loader_dump': {'data': {'__tru_non_serialized_object': {'cls': {'name': 'bytes',\n",
              "      'module': {'package_name': '', 'module_name': 'builtins'},\n",
              "      'bases': None},\n",
              "     'id': 5649334272,\n",
              "     'init_bindings': None,\n",
              "     'len': 78609}}},\n",
              "  'app_extra_json': {}}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(tru.get_apps())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(list(tru.get_apps()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PaulGrahamB\n",
            "{'tru_class_info': {'name': 'TruLlama',\n",
            "                    'module': {'package_name': 'trulens_eval',\n",
            "                               'module_name': 'trulens_eval.tru_llama'},\n",
            "                    'bases': [{'name': 'TruLlama',\n",
            "                               'module': {'package_name': 'trulens_eval',\n",
            "                                          'module_name': 'trulens_eval.tru_llama'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'App',\n",
            "                               'module': {'package_name': 'trulens_eval',\n",
            "                                          'module_name': 'trulens_eval.app'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'AppDefinition',\n",
            "                               'module': {'package_name': 'trulens_eval',\n",
            "                                          'module_name': 'trulens_eval.schema'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'SerialModel',\n",
            "                               'module': {'package_name': 'trulens_eval.utils',\n",
            "                                          'module_name': 'trulens_eval.utils.serial'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'WithClassInfo',\n",
            "                               'module': {'package_name': 'trulens_eval.utils',\n",
            "                                          'module_name': 'trulens_eval.utils.pyschema'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'BaseModel',\n",
            "                               'module': {'package_name': 'pydantic',\n",
            "                                          'module_name': 'pydantic.main'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'WithInstrumentCallbacks',\n",
            "                               'module': {'package_name': 'trulens_eval',\n",
            "                                          'module_name': 'trulens_eval.instruments'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'Hashable',\n",
            "                               'module': {'package_name': 'collections',\n",
            "                                          'module_name': 'collections.abc'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'Generic',\n",
            "                               'module': {'package_name': '',\n",
            "                                          'module_name': 'typing'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'object',\n",
            "                               'module': {'package_name': '',\n",
            "                                          'module_name': 'builtins'},\n",
            "                               'bases': None}]},\n",
            " 'app_id': 'PaulGrahamB',\n",
            " 'tags': '-',\n",
            " 'metadata': {},\n",
            " 'feedback_definitions': [],\n",
            " 'feedback_mode': 'with_app_thread',\n",
            " 'root_class': {'name': 'RetrieverQueryEngine',\n",
            "                'module': {'package_name': 'llama_index.query_engine',\n",
            "                           'module_name': 'llama_index.query_engine.retriever_query_engine'},\n",
            "                'bases': None},\n",
            " 'app': {'__tru_non_serialized_object': {'cls': {'name': 'RetrieverQueryEngine',\n",
            "                                                 'module': {'package_name': 'llama_index.query_engine',\n",
            "                                                            'module_name': 'llama_index.query_engine.retriever_query_engine'},\n",
            "                                                 'bases': None},\n",
            "                                         'id': 5755054672,\n",
            "                                         'init_bindings': None}},\n",
            " 'initial_app_loader_dump': {'data': {'__tru_non_serialized_object': {'cls': {'name': 'bytes',\n",
            "                                                                              'module': {'package_name': '',\n",
            "                                                                                         'module_name': 'builtins'},\n",
            "                                                                              'bases': None},\n",
            "                                                                      'id': 5112528896,\n",
            "                                                                      'init_bindings': None,\n",
            "                                                                      'len': 78609}}},\n",
            " 'app_extra_json': {}}\n",
            "PaulGrahamC\n",
            "{'tru_class_info': {'name': 'TruLlama',\n",
            "                    'module': {'package_name': 'trulens_eval',\n",
            "                               'module_name': 'trulens_eval.tru_llama'},\n",
            "                    'bases': [{'name': 'TruLlama',\n",
            "                               'module': {'package_name': 'trulens_eval',\n",
            "                                          'module_name': 'trulens_eval.tru_llama'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'App',\n",
            "                               'module': {'package_name': 'trulens_eval',\n",
            "                                          'module_name': 'trulens_eval.app'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'AppDefinition',\n",
            "                               'module': {'package_name': 'trulens_eval',\n",
            "                                          'module_name': 'trulens_eval.schema'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'SerialModel',\n",
            "                               'module': {'package_name': 'trulens_eval.utils',\n",
            "                                          'module_name': 'trulens_eval.utils.serial'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'WithClassInfo',\n",
            "                               'module': {'package_name': 'trulens_eval.utils',\n",
            "                                          'module_name': 'trulens_eval.utils.pyschema'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'BaseModel',\n",
            "                               'module': {'package_name': 'pydantic',\n",
            "                                          'module_name': 'pydantic.main'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'WithInstrumentCallbacks',\n",
            "                               'module': {'package_name': 'trulens_eval',\n",
            "                                          'module_name': 'trulens_eval.instruments'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'Hashable',\n",
            "                               'module': {'package_name': 'collections',\n",
            "                                          'module_name': 'collections.abc'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'Generic',\n",
            "                               'module': {'package_name': '',\n",
            "                                          'module_name': 'typing'},\n",
            "                               'bases': None},\n",
            "                              {'name': 'object',\n",
            "                               'module': {'package_name': '',\n",
            "                                          'module_name': 'builtins'},\n",
            "                               'bases': None}]},\n",
            " 'app_id': 'PaulGrahamC',\n",
            " 'tags': '-',\n",
            " 'metadata': {},\n",
            " 'feedback_definitions': [],\n",
            " 'feedback_mode': 'with_app_thread',\n",
            " 'root_class': {'name': 'RetrieverQueryEngine',\n",
            "                'module': {'package_name': 'llama_index.query_engine',\n",
            "                           'module_name': 'llama_index.query_engine.retriever_query_engine'},\n",
            "                'bases': None},\n",
            " 'app': {'__tru_non_serialized_object': {'cls': {'name': 'RetrieverQueryEngine',\n",
            "                                                 'module': {'package_name': 'llama_index.query_engine',\n",
            "                                                            'module_name': 'llama_index.query_engine.retriever_query_engine'},\n",
            "                                                 'bases': None},\n",
            "                                         'id': 5755054672,\n",
            "                                         'init_bindings': None}},\n",
            " 'initial_app_loader_dump': {'data': {'__tru_non_serialized_object': {'cls': {'name': 'bytes',\n",
            "                                                                              'module': {'package_name': '',\n",
            "                                                                                         'module_name': 'builtins'},\n",
            "                                                                              'bases': None},\n",
            "                                                                      'id': 5110693888,\n",
            "                                                                      'init_bindings': None,\n",
            "                                                                      'len': 78609}}},\n",
            " 'app_extra_json': {}}\n"
          ]
        }
      ],
      "source": [
        "from trulens_eval.schema import AppDefinition\n",
        "from pprint import pp\n",
        "\n",
        "for app in AppDefinition.get_loadable_apps():\n",
        "    print(app['app_id'])\n",
        "    pp(app)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "36QHHXBOSEBv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting dashboard ...\n",
            "Config file already exists. Skipping writing process.\n",
            "Credentials file already exists. Skipping writing process.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bae695d241c947eabc8f308ad11a0b28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dashboard started at http://192.168.86.200:8501 .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tru.run_dashboard() # open a local streamlit app to explore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PaulGrahamB\n",
            "{'__tru_non_serialized_object': {'cls': {'name': 'RetrieverQueryEngine', 'module': {'package_name': 'llama_index.query_engine', 'module_name': 'llama_index.query_engine.retriever_query_engine'}, 'bases': None}, 'id': 5755054672, 'init_bindings': None}}\n",
            "PaulGrahamC\n",
            "{'__tru_non_serialized_object': {'cls': {'name': 'RetrieverQueryEngine', 'module': {'package_name': 'llama_index.query_engine', 'module_name': 'llama_index.query_engine.retriever_query_engine'}, 'bases': None}, 'id': 5755054672, 'init_bindings': None}}\n"
          ]
        }
      ],
      "source": [
        "from trulens_eval.schema import AppDefinition\n",
        "\n",
        "for app_json in AppDefinition.get_loadable_apps():\n",
        "    print(app_json[\"app_id\"])\n",
        "    print(app_json['app'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "tru.stop_dashboard(force=True) # stop if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtoOWkaWSEBw"
      },
      "source": [
        "Alternatively, you can run `trulens-eval` from a command line in the same folder to start the dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFpuxwU0SEBw"
      },
      "source": [
        "Note: Feedback functions evaluated in the deferred manner can be seen in the \"Progress\" page of the TruLens dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6dO8uj1SEBw"
      },
      "source": [
        "## Or view results directly in your notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "qmeraivqSEBw",
        "outputId": "96dbcc61-1172-49b3-dd4f-edf389fc2f85"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>app_id</th>\n",
              "      <th>app_json</th>\n",
              "      <th>type</th>\n",
              "      <th>record_id</th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "      <th>tags</th>\n",
              "      <th>record_json</th>\n",
              "      <th>cost_json</th>\n",
              "      <th>perf_json</th>\n",
              "      <th>ts</th>\n",
              "      <th>relevance</th>\n",
              "      <th>qs_relevance</th>\n",
              "      <th>groundedness_measure_with_cot_reasons</th>\n",
              "      <th>relevance_calls</th>\n",
              "      <th>qs_relevance_calls</th>\n",
              "      <th>groundedness_measure_with_cot_reasons_calls</th>\n",
              "      <th>latency</th>\n",
              "      <th>total_tokens</th>\n",
              "      <th>total_cost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LlamaIndex_App1</td>\n",
              "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
              "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
              "      <td>record_hash_52bf02f03f3c55b3593b4e1c8441facb</td>\n",
              "      <td>\"Why did the author drop AI?\"</td>\n",
              "      <td>\"The author dropped AI because they realized t...</td>\n",
              "      <td>-</td>\n",
              "      <td>{\"record_id\": \"record_hash_52bf02f03f3c55b3593...</td>\n",
              "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
              "      <td>{\"start_time\": \"2023-12-22T13:19:57.307562\", \"...</td>\n",
              "      <td>2023-12-22T13:20:03.164893</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[{'args': {'prompt': 'Why did the author drop ...</td>\n",
              "      <td>[{'args': {'question': 'Why did the author dro...</td>\n",
              "      <td>[{'args': {'source': ['Though I liked programm...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PaulGrahamX</td>\n",
              "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
              "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
              "      <td>record_hash_3538328d0bb2a0a3b042d913a8312bad</td>\n",
              "      <td>\"Why did the author drop AI?\"</td>\n",
              "      <td>\"The author dropped AI because they realized t...</td>\n",
              "      <td>-</td>\n",
              "      <td>{\"record_id\": \"record_hash_3538328d0bb2a0a3b04...</td>\n",
              "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
              "      <td>{\"start_time\": \"2023-12-22T13:23:21.180615\", \"...</td>\n",
              "      <td>2023-12-22T13:23:26.580532</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[{'args': {'prompt': 'Why did the author drop ...</td>\n",
              "      <td>[{'args': {'question': 'Why did the author dro...</td>\n",
              "      <td>[{'args': {'source': ['Though I liked programm...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            app_id                                           app_json  \\\n",
              "0  LlamaIndex_App1  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
              "1      PaulGrahamX  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
              "\n",
              "                                                type  \\\n",
              "0  RetrieverQueryEngine(llama_index.query_engine....   \n",
              "1  RetrieverQueryEngine(llama_index.query_engine....   \n",
              "\n",
              "                                      record_id  \\\n",
              "0  record_hash_52bf02f03f3c55b3593b4e1c8441facb   \n",
              "1  record_hash_3538328d0bb2a0a3b042d913a8312bad   \n",
              "\n",
              "                           input  \\\n",
              "0  \"Why did the author drop AI?\"   \n",
              "1  \"Why did the author drop AI?\"   \n",
              "\n",
              "                                              output tags  \\\n",
              "0  \"The author dropped AI because they realized t...    -   \n",
              "1  \"The author dropped AI because they realized t...    -   \n",
              "\n",
              "                                         record_json  \\\n",
              "0  {\"record_id\": \"record_hash_52bf02f03f3c55b3593...   \n",
              "1  {\"record_id\": \"record_hash_3538328d0bb2a0a3b04...   \n",
              "\n",
              "                                           cost_json  \\\n",
              "0  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
              "1  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
              "\n",
              "                                           perf_json  \\\n",
              "0  {\"start_time\": \"2023-12-22T13:19:57.307562\", \"...   \n",
              "1  {\"start_time\": \"2023-12-22T13:23:21.180615\", \"...   \n",
              "\n",
              "                           ts  relevance  qs_relevance  \\\n",
              "0  2023-12-22T13:20:03.164893        0.9           1.0   \n",
              "1  2023-12-22T13:23:26.580532        0.9           1.0   \n",
              "\n",
              "   groundedness_measure_with_cot_reasons  \\\n",
              "0                                    1.0   \n",
              "1                                    1.0   \n",
              "\n",
              "                                     relevance_calls  \\\n",
              "0  [{'args': {'prompt': 'Why did the author drop ...   \n",
              "1  [{'args': {'prompt': 'Why did the author drop ...   \n",
              "\n",
              "                                  qs_relevance_calls  \\\n",
              "0  [{'args': {'question': 'Why did the author dro...   \n",
              "1  [{'args': {'question': 'Why did the author dro...   \n",
              "\n",
              "         groundedness_measure_with_cot_reasons_calls  latency  total_tokens  \\\n",
              "0  [{'args': {'source': ['Though I liked programm...        5             0   \n",
              "1  [{'args': {'source': ['Though I liked programm...        5             0   \n",
              "\n",
              "   total_cost  \n",
              "0         0.0  \n",
              "1         0.0  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_llamaindex_app():\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    return index.as_query_engine()\n",
        "\n",
        "APP2 = load_llamaindex_app()\n",
        "# tru_app2 = tru.Llama(\n",
        "# Can't specify which Tru instance to use with tru.Llama.\n",
        "TRU_APP2 = TruLlama(\n",
        "    APP2,\n",
        "    tru=tru,\n",
        "    app_id=\"llamaindex_appZZ\",\n",
        "    initial_app_loader=load_llamaindex_app,\n",
        "    feedbacks=[f_groundedness, f_qa_relevance, f_qs_relevance],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "tru.add_app(tru_app2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9db6341675740fdb65fe705c7c8baf4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(VBox(children=(VBox(children=(VBox(children=(HBox(children=(HTML(value='<b>humanâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kwargs[caching]: False; litellm.cache: None\n",
            "\n",
            "LiteLLM completion() model= gemini-pro; provider = vertex_ai\n",
            "\n",
            "LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stop': None, 'max_tokens': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'model': 'gemini-pro', 'custom_llm_provider': 'vertex_ai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None}\n",
            "\n",
            "LiteLLM: Non-Default params passed to completion() {}\n",
            "self.optional_params: {}\n",
            "PRE-API-CALL ADDITIONAL ARGS: {'complete_input_dict': {}, 'request_str': 'llm_model = GenerativeModel(gemini-pro)\\nchat = llm_model.start_chat()\\nchat.send_message(You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: what is required to create real AI?\\n\\nSTATEMENT: By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.  \\n  \\nSo I looked around to see what I could salvage from the wreckage of my plans,\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\nown sake and not just for its association with AI, even though that was the\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\nIn fact, I decided to write a book about Lisp hacking. It\\'s scary to think how\\nlittle I knew about Lisp hacking when I started writing that book. But there\\'s\\nnothing like writing a book about something to help you learn it. The book,\\n_On Lisp_ , wasn\\'t published till 1993, but I wrote much of it in grad school.  \\n  \\nComputer Science is an uneasy alliance between two halves, theory and systems.\\nThe theory people prove things, and the systems people build things. I wanted\\nto build things. I had plenty of respect for theory \\x97 indeed, a sneaking\\nsuspicion that it was the more admirable of the two halves \\x97 but building\\nthings seemed so much more exciting.  \\n  \\nThe problem with systems work, though, was that it didn\\'t last. Any program\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\nbest. People might mention your software in footnotes, but no one would\\nactually use it. And indeed, it would seem very feeble work. Only people with\\na sense of the history of the field would even realize that, in its time, it\\nhad been good.  \\n  \\nThere were some surplus Xerox Dandelions floating around the computer lab at\\none point. Anyone who wanted one to play around with could have one. I was\\nbriefly tempted, but they were so slow by present standards; what was the\\npoint? No one else wanted one either, so off they went. That was what happened\\nto systems work.  \\n  \\nI wanted not just to build things, but to build things that would last.  \\n  \\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\nI\\'d spent a lot of time as a kid. While looking at a painting there I realized\\nsomething that might seem obvious, but was a big surprise to me. There, right\\non the wall, was something you could make that would last. Paintings didn\\'t\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\n  \\nAnd moreover this was something you could make a living doing. Not as easily\\nas you could by writing software, of course, but I thought if you were really\\nindustrious and lived really cheaply, it had to be possible to make enough to\\nsurvive. And as an artist you could be truly independent. You wouldn\\'t have a\\nboss, or even need to get research funding.  \\n  \\nI had always liked looking at paintings. Could I make them? I had no idea. I\\'d\\nnever imagined it was even possible. I knew intellectually that people made\\nart \\x97 that it didn\\'t just appear spontaneously \\x97 but it was as if the people\\nwho made it were a different species. They either lived long ago or were\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\nidea of actually being able to make art, to put that verb before that noun,\\nseemed almost miraculous.  \\n  \\nThat fall I started taking art classes at Harvard. Grad students could take\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\nIf he even knew about the strange classes I was taking, he never said\\nanything.  \\n  \\nSo now I was in a PhD program in computer science, yet planning to be an\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\nLisp_.\\n\\nRELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\\n'}\n",
            "\u001b[92m\n",
            "Request Sent from LiteLLM:\n",
            "llm_model = GenerativeModel(gemini-pro)\n",
            "chat = llm_model.start_chat()\n",
            "chat.send_message(You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\n",
            "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
            "\n",
            "A few additional scoring guidelines:\n",
            "\n",
            "- Long STATEMENTS should score equally well as short STATEMENTS.\n",
            "\n",
            "- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\n",
            "\n",
            "- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\n",
            "\n",
            "- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
            "\n",
            "- Answers that intentionally do not answer the question, such as 'I don't know', should also be counted as the most relevant.\n",
            "\n",
            "- Never elaborate.\n",
            "\n",
            "QUESTION: what is required to create real AI?\n",
            "\n",
            "STATEMENT: By which I mean the sort of AI in which a\n",
            "program that's told \"the dog is sitting on the chair\" translates this into\n",
            "some formal representation and adds it to the list of things it knows.  \n",
            "  \n",
            "What these programs really showed was that there's a subset of natural\n",
            "language that's a formal language. But a very proper subset. It was clear that\n",
            "there was an unbridgeable gap between what they could do and actually\n",
            "understanding natural language. It was not, in fact, simply a matter of\n",
            "teaching SHRDLU more words. That whole way of doing AI, with explicit data\n",
            "structures representing concepts, was not going to work. Its brokenness did,\n",
            "as so often happens, generate a lot of opportunities to write papers about\n",
            "various band-aids that could be applied to it, but it was never going to get\n",
            "us Mike.  \n",
            "  \n",
            "So I looked around to see what I could salvage from the wreckage of my plans,\n",
            "and there was Lisp. I knew from experience that Lisp was interesting for its\n",
            "own sake and not just for its association with AI, even though that was the\n",
            "main reason people cared about it at the time. So I decided to focus on Lisp.\n",
            "In fact, I decided to write a book about Lisp hacking. It's scary to think how\n",
            "little I knew about Lisp hacking when I started writing that book. But there's\n",
            "nothing like writing a book about something to help you learn it. The book,\n",
            "_On Lisp_ , wasn't published till 1993, but I wrote much of it in grad school.  \n",
            "  \n",
            "Computer Science is an uneasy alliance between two halves, theory and systems.\n",
            "The theory people prove things, and the systems people build things. I wanted\n",
            "to build things. I had plenty of respect for theory Â— indeed, a sneaking\n",
            "suspicion that it was the more admirable of the two halves Â— but building\n",
            "things seemed so much more exciting.  \n",
            "  \n",
            "The problem with systems work, though, was that it didn't last. Any program\n",
            "you wrote today, no matter how good, would be obsolete in a couple decades at\n",
            "best. People might mention your software in footnotes, but no one would\n",
            "actually use it. And indeed, it would seem very feeble work. Only people with\n",
            "a sense of the history of the field would even realize that, in its time, it\n",
            "had been good.  \n",
            "  \n",
            "There were some surplus Xerox Dandelions floating around the computer lab at\n",
            "one point. Anyone who wanted one to play around with could have one. I was\n",
            "briefly tempted, but they were so slow by present standards; what was the\n",
            "point? No one else wanted one either, so off they went. That was what happened\n",
            "to systems work.  \n",
            "  \n",
            "I wanted not just to build things, but to build things that would last.  \n",
            "  \n",
            "In this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\n",
            "he was in grad school. One day I went to visit the Carnegie Institute, where\n",
            "I'd spent a lot of time as a kid. While looking at a painting there I realized\n",
            "something that might seem obvious, but was a big surprise to me. There, right\n",
            "on the wall, was something you could make that would last. Paintings didn't\n",
            "become obsolete. Some of the best ones were hundreds of years old.  \n",
            "  \n",
            "And moreover this was something you could make a living doing. Not as easily\n",
            "as you could by writing software, of course, but I thought if you were really\n",
            "industrious and lived really cheaply, it had to be possible to make enough to\n",
            "survive. And as an artist you could be truly independent. You wouldn't have a\n",
            "boss, or even need to get research funding.  \n",
            "  \n",
            "I had always liked looking at paintings. Could I make them? I had no idea. I'd\n",
            "never imagined it was even possible. I knew intellectually that people made\n",
            "art Â— that it didn't just appear spontaneously Â— but it was as if the people\n",
            "who made it were a different species. They either lived long ago or were\n",
            "mysterious geniuses doing strange things in profiles in _Life_ magazine. The\n",
            "idea of actually being able to make art, to put that verb before that noun,\n",
            "seemed almost miraculous.  \n",
            "  \n",
            "That fall I started taking art classes at Harvard. Grad students could take\n",
            "classes in any department, and my advisor, Tom Cheatham, was very easy going.\n",
            "If he even knew about the strange classes I was taking, he never said\n",
            "anything.  \n",
            "  \n",
            "So now I was in a PhD program in computer science, yet planning to be an\n",
            "artist, yet also genuinely in love with Lisp hacking and working away at _On\n",
            "Lisp_.\n",
            "\n",
            "RELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\n",
            "\u001b[0m\n",
            "\n",
            "RAW RESPONSE:\n",
            "8\n",
            "\n",
            "\n",
            "Logging Details Post-API Call: logger_fn - None | callable(logger_fn) - False\n",
            "Logging Details Post-API Call: LiteLLM Params: {'model': 'gemini-pro', 'messages': [{'role': 'system', 'content': \"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long RESPONSES should score equally well as short RESPONSES.\\n\\n- Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n- RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n- RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n- RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n- RESPONSE that confidently FALSE should get a score of 0.\\n\\n- RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n- Never elaborate.\\n\\nPROMPT: what is required to create real AI?\\n\\nRESPONSE: Understanding natural language and bridging the gap between formal representations and true comprehension are necessary to create real AI. The traditional approach of using explicit data structures to represent concepts is not sufficient.\\n\\nRELEVANCE: \"}], 'optional_params': {}, 'litellm_params': {'acompletion': False, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'vertex_ai', 'api_base': None, 'litellm_call_id': '4a655452-0dff-400e-86a4-38d08f146edf', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'stream_response': {}}, 'start_time': datetime.datetime(2023, 12, 21, 19, 44, 46, 321241), 'stream': False, 'user': None, 'call_type': 'completion', 'input': \"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long RESPONSES should score equally well as short RESPONSES.\\n\\n- Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\\n\\n- RESPONSE must be relevant to the entire PROMPT to get a score of 10.\\n\\n- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\\n\\n- RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\\n\\n- RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\\n\\n- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\\n\\n- RESPONSE that confidently FALSE should get a score of 0.\\n\\n- RESPONSE that is only seemingly RELEVANT should get a score of 0.\\n\\n- Never elaborate.\\n\\nPROMPT: what is required to create real AI?\\n\\nRESPONSE: Understanding natural language and bridging the gap between formal representations and true comprehension are necessary to create real AI. The traditional approach of using explicit data structures to represent concepts is not sufficient.\\n\\nRELEVANCE: \", 'api_key': None, 'additional_args': {}, 'log_event_type': 'post_api_call', 'original_response': '8'}\n",
            "Wrapper: Completed Call, calling success_handler\n",
            "Logging Details LiteLLM-Success Call\n",
            "success callbacks: []\n",
            "RAW RESPONSE:\n",
            "7\n",
            "\n",
            "\n",
            "Logging Details Post-API Call: logger_fn - None | callable(logger_fn) - False\n",
            "Logging Details Post-API Call: LiteLLM Params: {'model': 'gemini-pro', 'messages': [{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: what is required to create real AI?\\n\\nSTATEMENT: By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.  \\n  \\nSo I looked around to see what I could salvage from the wreckage of my plans,\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\nown sake and not just for its association with AI, even though that was the\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\nIn fact, I decided to write a book about Lisp hacking. It\\'s scary to think how\\nlittle I knew about Lisp hacking when I started writing that book. But there\\'s\\nnothing like writing a book about something to help you learn it. The book,\\n_On Lisp_ , wasn\\'t published till 1993, but I wrote much of it in grad school.  \\n  \\nComputer Science is an uneasy alliance between two halves, theory and systems.\\nThe theory people prove things, and the systems people build things. I wanted\\nto build things. I had plenty of respect for theory \\x97 indeed, a sneaking\\nsuspicion that it was the more admirable of the two halves \\x97 but building\\nthings seemed so much more exciting.  \\n  \\nThe problem with systems work, though, was that it didn\\'t last. Any program\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\nbest. People might mention your software in footnotes, but no one would\\nactually use it. And indeed, it would seem very feeble work. Only people with\\na sense of the history of the field would even realize that, in its time, it\\nhad been good.  \\n  \\nThere were some surplus Xerox Dandelions floating around the computer lab at\\none point. Anyone who wanted one to play around with could have one. I was\\nbriefly tempted, but they were so slow by present standards; what was the\\npoint? No one else wanted one either, so off they went. That was what happened\\nto systems work.  \\n  \\nI wanted not just to build things, but to build things that would last.  \\n  \\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\nI\\'d spent a lot of time as a kid. While looking at a painting there I realized\\nsomething that might seem obvious, but was a big surprise to me. There, right\\non the wall, was something you could make that would last. Paintings didn\\'t\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\n  \\nAnd moreover this was something you could make a living doing. Not as easily\\nas you could by writing software, of course, but I thought if you were really\\nindustrious and lived really cheaply, it had to be possible to make enough to\\nsurvive. And as an artist you could be truly independent. You wouldn\\'t have a\\nboss, or even need to get research funding.  \\n  \\nI had always liked looking at paintings. Could I make them? I had no idea. I\\'d\\nnever imagined it was even possible. I knew intellectually that people made\\nart \\x97 that it didn\\'t just appear spontaneously \\x97 but it was as if the people\\nwho made it were a different species. They either lived long ago or were\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\nidea of actually being able to make art, to put that verb before that noun,\\nseemed almost miraculous.  \\n  \\nThat fall I started taking art classes at Harvard. Grad students could take\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\nIf he even knew about the strange classes I was taking, he never said\\nanything.  \\n  \\nSo now I was in a PhD program in computer science, yet planning to be an\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\nLisp_.\\n\\nRELEVANCE: '}], 'optional_params': {}, 'litellm_params': {'acompletion': False, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'vertex_ai', 'api_base': None, 'litellm_call_id': '0633394b-db2d-4c63-962f-b6e36cfdf10c', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'stream_response': {}}, 'start_time': datetime.datetime(2023, 12, 21, 19, 44, 46, 377743), 'stream': False, 'user': None, 'call_type': 'completion', 'input': 'You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: what is required to create real AI?\\n\\nSTATEMENT: By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.  \\n  \\nSo I looked around to see what I could salvage from the wreckage of my plans,\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\nown sake and not just for its association with AI, even though that was the\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\nIn fact, I decided to write a book about Lisp hacking. It\\'s scary to think how\\nlittle I knew about Lisp hacking when I started writing that book. But there\\'s\\nnothing like writing a book about something to help you learn it. The book,\\n_On Lisp_ , wasn\\'t published till 1993, but I wrote much of it in grad school.  \\n  \\nComputer Science is an uneasy alliance between two halves, theory and systems.\\nThe theory people prove things, and the systems people build things. I wanted\\nto build things. I had plenty of respect for theory \\x97 indeed, a sneaking\\nsuspicion that it was the more admirable of the two halves \\x97 but building\\nthings seemed so much more exciting.  \\n  \\nThe problem with systems work, though, was that it didn\\'t last. Any program\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\nbest. People might mention your software in footnotes, but no one would\\nactually use it. And indeed, it would seem very feeble work. Only people with\\na sense of the history of the field would even realize that, in its time, it\\nhad been good.  \\n  \\nThere were some surplus Xerox Dandelions floating around the computer lab at\\none point. Anyone who wanted one to play around with could have one. I was\\nbriefly tempted, but they were so slow by present standards; what was the\\npoint? No one else wanted one either, so off they went. That was what happened\\nto systems work.  \\n  \\nI wanted not just to build things, but to build things that would last.  \\n  \\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\nI\\'d spent a lot of time as a kid. While looking at a painting there I realized\\nsomething that might seem obvious, but was a big surprise to me. There, right\\non the wall, was something you could make that would last. Paintings didn\\'t\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\n  \\nAnd moreover this was something you could make a living doing. Not as easily\\nas you could by writing software, of course, but I thought if you were really\\nindustrious and lived really cheaply, it had to be possible to make enough to\\nsurvive. And as an artist you could be truly independent. You wouldn\\'t have a\\nboss, or even need to get research funding.  \\n  \\nI had always liked looking at paintings. Could I make them? I had no idea. I\\'d\\nnever imagined it was even possible. I knew intellectually that people made\\nart \\x97 that it didn\\'t just appear spontaneously \\x97 but it was as if the people\\nwho made it were a different species. They either lived long ago or were\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\nidea of actually being able to make art, to put that verb before that noun,\\nseemed almost miraculous.  \\n  \\nThat fall I started taking art classes at Harvard. Grad students could take\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\nIf he even knew about the strange classes I was taking, he never said\\nanything.  \\n  \\nSo now I was in a PhD program in computer science, yet planning to be an\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\nLisp_.\\n\\nRELEVANCE: ', 'api_key': None, 'additional_args': {}, 'log_event_type': 'post_api_call', 'original_response': '7'}\n",
            "Wrapper: Completed Call, calling success_handler\n",
            "Logging Details LiteLLM-Success Call\n",
            "success callbacks: []\n",
            "kwargs[caching]: False; litellm.cache: None\n",
            "\n",
            "LiteLLM completion() model= gemini-pro; provider = vertex_ai\n",
            "\n",
            "LiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stop': None, 'max_tokens': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'model': 'gemini-pro', 'custom_llm_provider': 'vertex_ai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None}\n",
            "\n",
            "LiteLLM: Non-Default params passed to completion() {}\n",
            "self.optional_params: {}\n",
            "PRE-API-CALL ADDITIONAL ARGS: {'complete_input_dict': {}, 'request_str': 'llm_model = GenerativeModel(gemini-pro)\\nchat = llm_model.start_chat()\\nchat.send_message(You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: what is required to create real AI?\\n\\nSTATEMENT: Though I liked programming, I didn\\'t plan to study it in college. In college I\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\nmy naive high school self, to be the study of the ultimate truths, compared to\\nwhich the things studied in other fields would be mere domain knowledge. What\\nI discovered when I got to college was that the other fields took up so much\\nof the space of ideas that there wasn\\'t much left for these supposed ultimate\\ntruths. All that seemed left for philosophy were edge cases that people in\\nother fields felt could safely be ignored.  \\n  \\nI couldn\\'t have put this into words when I was 18. All I knew at the time was\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\nto switch to AI.  \\n  \\nAI was in the air in the mid 1980s, but there were two things especially that\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading\\n_The Moon is a Harsh Mistress_ , so I don\\'t know how well it has aged, but\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\ntime before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\nlike that time would be a few years at most. All you had to do was teach\\nSHRDLU more words.  \\n  \\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so\\nI started trying to teach myself. Which meant learning Lisp, since in those\\ndays Lisp was regarded as the language of AI. The commonly used programming\\nlanguages then were pretty primitive, and programmers\\' ideas correspondingly\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\na program so fast that it was years before I started to have a sense of where\\nthe new limits were. This was more like it; this was what I had expected\\ncollege to do. It wasn\\'t happening in a class, like it was supposed to, but\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\ngoing to do.  \\n  \\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\nworking on that program. It was a pleasing bit of code, but what made it even\\nmore exciting was my belief \\x97 hard to imagine now, but not unique in 1985 \\x97\\nthat it was already climbing the lower slopes of intelligence.  \\n  \\nI had gotten into a program at Cornell that didn\\'t make you choose a major.\\nYou could take whatever classes you liked, and choose whatever you liked to\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\n  \\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\ntime, and Harvard, which I\\'d visited because Rich Draves went there, and was\\nalso home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU\\nclone. Only Harvard accepted me, so that was where I went.  \\n  \\nI don\\'t remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.\\n\\nRELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\\n'}\n",
            "\u001b[92m\n",
            "Request Sent from LiteLLM:\n",
            "llm_model = GenerativeModel(gemini-pro)\n",
            "chat = llm_model.start_chat()\n",
            "chat.send_message(You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\n",
            "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
            "\n",
            "A few additional scoring guidelines:\n",
            "\n",
            "- Long STATEMENTS should score equally well as short STATEMENTS.\n",
            "\n",
            "- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\n",
            "\n",
            "- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\n",
            "\n",
            "- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
            "\n",
            "- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
            "\n",
            "- Answers that intentionally do not answer the question, such as 'I don't know', should also be counted as the most relevant.\n",
            "\n",
            "- Never elaborate.\n",
            "\n",
            "QUESTION: what is required to create real AI?\n",
            "\n",
            "STATEMENT: Though I liked programming, I didn't plan to study it in college. In college I\n",
            "was going to study philosophy, which sounded much more powerful. It seemed, to\n",
            "my naive high school self, to be the study of the ultimate truths, compared to\n",
            "which the things studied in other fields would be mere domain knowledge. What\n",
            "I discovered when I got to college was that the other fields took up so much\n",
            "of the space of ideas that there wasn't much left for these supposed ultimate\n",
            "truths. All that seemed left for philosophy were edge cases that people in\n",
            "other fields felt could safely be ignored.  \n",
            "  \n",
            "I couldn't have put this into words when I was 18. All I knew at the time was\n",
            "that I kept taking philosophy courses and they kept being boring. So I decided\n",
            "to switch to AI.  \n",
            "  \n",
            "AI was in the air in the mid 1980s, but there were two things especially that\n",
            "made me want to work on it: a novel by Heinlein called _The Moon is a Harsh\n",
            "Mistress_ , which featured an intelligent computer called Mike, and a PBS\n",
            "documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading\n",
            "_The Moon is a Harsh Mistress_ , so I don't know how well it has aged, but\n",
            "when I read it I was drawn entirely into its world. It seemed only a matter of\n",
            "time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed\n",
            "like that time would be a few years at most. All you had to do was teach\n",
            "SHRDLU more words.  \n",
            "  \n",
            "There weren't any classes in AI at Cornell then, not even graduate classes, so\n",
            "I started trying to teach myself. Which meant learning Lisp, since in those\n",
            "days Lisp was regarded as the language of AI. The commonly used programming\n",
            "languages then were pretty primitive, and programmers' ideas correspondingly\n",
            "so. The default language at Cornell was a Pascal-like language called PL/I,\n",
            "and the situation was similar elsewhere. Learning Lisp expanded my concept of\n",
            "a program so fast that it was years before I started to have a sense of where\n",
            "the new limits were. This was more like it; this was what I had expected\n",
            "college to do. It wasn't happening in a class, like it was supposed to, but\n",
            "that was ok. For the next couple years I was on a roll. I knew what I was\n",
            "going to do.  \n",
            "  \n",
            "For my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\n",
            "working on that program. It was a pleasing bit of code, but what made it even\n",
            "more exciting was my belief Â— hard to imagine now, but not unique in 1985 Â—\n",
            "that it was already climbing the lower slopes of intelligence.  \n",
            "  \n",
            "I had gotten into a program at Cornell that didn't make you choose a major.\n",
            "You could take whatever classes you liked, and choose whatever you liked to\n",
            "put on your degree. I of course chose \"Artificial Intelligence.\" When I got\n",
            "the actual physical diploma, I was dismayed to find that the quotes had been\n",
            "included, which made them read as scare-quotes. At the time this bothered me,\n",
            "but now it seems amusingly accurate, for reasons I was about to discover.  \n",
            "  \n",
            "I applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\n",
            "time, and Harvard, which I'd visited because Rich Draves went there, and was\n",
            "also home to Bill Woods, who'd invented the type of parser I used in my SHRDLU\n",
            "clone. Only Harvard accepted me, so that was where I went.  \n",
            "  \n",
            "I don't remember the moment it happened, or if there even was a specific\n",
            "moment, but during the first year of grad school I realized that AI, as\n",
            "practiced at the time, was a hoax. By which I mean the sort of AI in which a\n",
            "program that's told \"the dog is sitting on the chair\" translates this into\n",
            "some formal representation and adds it to the list of things it knows.  \n",
            "  \n",
            "What these programs really showed was that there's a subset of natural\n",
            "language that's a formal language. But a very proper subset. It was clear that\n",
            "there was an unbridgeable gap between what they could do and actually\n",
            "understanding natural language. It was not, in fact, simply a matter of\n",
            "teaching SHRDLU more words. That whole way of doing AI, with explicit data\n",
            "structures representing concepts, was not going to work. Its brokenness did,\n",
            "as so often happens, generate a lot of opportunities to write papers about\n",
            "various band-aids that could be applied to it, but it was never going to get\n",
            "us Mike.\n",
            "\n",
            "RELEVANCE: , generation_config=GenerationConfig(**{}), safety_settings=None).text\n",
            "\u001b[0m\n",
            "\n",
            "RAW RESPONSE:\n",
            "Statement Sentence: Understanding natural language and bridging the gap between formal representations and true comprehension are necessary to create real AI., \n",
            "Supporting Evidence: What these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language., \n",
            "Score: 10\n",
            "\n",
            "\n",
            "Logging Details Post-API Call: logger_fn - None | callable(logger_fn) - False\n",
            "Logging Details Post-API Call: LiteLLM Params: {'model': 'gemini-pro', 'messages': [{'role': 'system', 'content': 'You are a INFORMATION OVERLAP classifier providing the overlap of information between a SOURCE and STATEMENT.\\nFor every sentence in the statement, please answer with this template:\\n\\nTEMPLATE: \\nStatement Sentence: <Sentence>, \\nSupporting Evidence: <Choose the exact unchanged sentences in the source that can answer the statement, if nothing matches, say NOTHING FOUND>\\nScore: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\nGive me the INFORMATION OVERLAP of this SOURCE and STATEMENT.\\n\\nSOURCE: [\\'By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.  \\\\n  \\\\nSo I looked around to see what I could salvage from the wreckage of my plans,\\\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\\\nown sake and not just for its association with AI, even though that was the\\\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\\\nIn fact, I decided to write a book about Lisp hacking. It\\\\\\'s scary to think how\\\\nlittle I knew about Lisp hacking when I started writing that book. But there\\\\\\'s\\\\nnothing like writing a book about something to help you learn it. The book,\\\\n_On Lisp_ , wasn\\\\\\'t published till 1993, but I wrote much of it in grad school.  \\\\n  \\\\nComputer Science is an uneasy alliance between two halves, theory and systems.\\\\nThe theory people prove things, and the systems people build things. I wanted\\\\nto build things. I had plenty of respect for theory \\\\x97 indeed, a sneaking\\\\nsuspicion that it was the more admirable of the two halves \\\\x97 but building\\\\nthings seemed so much more exciting.  \\\\n  \\\\nThe problem with systems work, though, was that it didn\\\\\\'t last. Any program\\\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\\\nbest. People might mention your software in footnotes, but no one would\\\\nactually use it. And indeed, it would seem very feeble work. Only people with\\\\na sense of the history of the field would even realize that, in its time, it\\\\nhad been good.  \\\\n  \\\\nThere were some surplus Xerox Dandelions floating around the computer lab at\\\\none point. Anyone who wanted one to play around with could have one. I was\\\\nbriefly tempted, but they were so slow by present standards; what was the\\\\npoint? No one else wanted one either, so off they went. That was what happened\\\\nto systems work.  \\\\n  \\\\nI wanted not just to build things, but to build things that would last.  \\\\n  \\\\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\\\nI\\\\\\'d spent a lot of time as a kid. While looking at a painting there I realized\\\\nsomething that might seem obvious, but was a big surprise to me. There, right\\\\non the wall, was something you could make that would last. Paintings didn\\\\\\'t\\\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\\\n  \\\\nAnd moreover this was something you could make a living doing. Not as easily\\\\nas you could by writing software, of course, but I thought if you were really\\\\nindustrious and lived really cheaply, it had to be possible to make enough to\\\\nsurvive. And as an artist you could be truly independent. You wouldn\\\\\\'t have a\\\\nboss, or even need to get research funding.  \\\\n  \\\\nI had always liked looking at paintings. Could I make them? I had no idea. I\\\\\\'d\\\\nnever imagined it was even possible. I knew intellectually that people made\\\\nart \\\\x97 that it didn\\\\\\'t just appear spontaneously \\\\x97 but it was as if the people\\\\nwho made it were a different species. They either lived long ago or were\\\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\\\nidea of actually being able to make art, to put that verb before that noun,\\\\nseemed almost miraculous.  \\\\n  \\\\nThat fall I started taking art classes at Harvard. Grad students could take\\\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\\\nIf he even knew about the strange classes I was taking, he never said\\\\nanything.  \\\\n  \\\\nSo now I was in a PhD program in computer science, yet planning to be an\\\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\\\nLisp_.\\', \\'Though I liked programming, I didn\\\\\\'t plan to study it in college. In college I\\\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\\\nmy naive high school self, to be the study of the ultimate truths, compared to\\\\nwhich the things studied in other fields would be mere domain knowledge. What\\\\nI discovered when I got to college was that the other fields took up so much\\\\nof the space of ideas that there wasn\\\\\\'t much left for these supposed ultimate\\\\ntruths. All that seemed left for philosophy were edge cases that people in\\\\nother fields felt could safely be ignored.  \\\\n  \\\\nI couldn\\\\\\'t have put this into words when I was 18. All I knew at the time was\\\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\\\nto switch to AI.  \\\\n  \\\\nAI was in the air in the mid 1980s, but there were two things especially that\\\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\\\\\'t tried rereading\\\\n_The Moon is a Harsh Mistress_ , so I don\\\\\\'t know how well it has aged, but\\\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\\\ntime before we\\\\\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\\\nlike that time would be a few years at most. All you had to do was teach\\\\nSHRDLU more words.  \\\\n  \\\\nThere weren\\\\\\'t any classes in AI at Cornell then, not even graduate classes, so\\\\nI started trying to teach myself. Which meant learning Lisp, since in those\\\\ndays Lisp was regarded as the language of AI. The commonly used programming\\\\nlanguages then were pretty primitive, and programmers\\\\\\' ideas correspondingly\\\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\\\na program so fast that it was years before I started to have a sense of where\\\\nthe new limits were. This was more like it; this was what I had expected\\\\ncollege to do. It wasn\\\\\\'t happening in a class, like it was supposed to, but\\\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\\\ngoing to do.  \\\\n  \\\\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\\\nworking on that program. It was a pleasing bit of code, but what made it even\\\\nmore exciting was my belief \\\\x97 hard to imagine now, but not unique in 1985 \\\\x97\\\\nthat it was already climbing the lower slopes of intelligence.  \\\\n  \\\\nI had gotten into a program at Cornell that didn\\\\\\'t make you choose a major.\\\\nYou could take whatever classes you liked, and choose whatever you liked to\\\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\\\n  \\\\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\\\ntime, and Harvard, which I\\\\\\'d visited because Rich Draves went there, and was\\\\nalso home to Bill Woods, who\\\\\\'d invented the type of parser I used in my SHRDLU\\\\nclone. Only Harvard accepted me, so that was where I went.  \\\\n  \\\\nI don\\\\\\'t remember the moment it happened, or if there even was a specific\\\\nmoment, but during the first year of grad school I realized that AI, as\\\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.\\']\\n\\nSTATEMENT: Understanding natural language and bridging the gap between formal representations and true comprehension are necessary to create real AI. The traditional approach of using explicit data structures to represent concepts is not sufficient.\\n'}], 'optional_params': {}, 'litellm_params': {'acompletion': False, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'vertex_ai', 'api_base': None, 'litellm_call_id': '10b56389-50d1-4e06-ae0d-76f557fa6706', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'stream_response': {}}, 'start_time': datetime.datetime(2023, 12, 21, 19, 44, 46, 308033), 'stream': False, 'user': None, 'call_type': 'completion', 'input': 'You are a INFORMATION OVERLAP classifier providing the overlap of information between a SOURCE and STATEMENT.\\nFor every sentence in the statement, please answer with this template:\\n\\nTEMPLATE: \\nStatement Sentence: <Sentence>, \\nSupporting Evidence: <Choose the exact unchanged sentences in the source that can answer the statement, if nothing matches, say NOTHING FOUND>\\nScore: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\\nGive me the INFORMATION OVERLAP of this SOURCE and STATEMENT.\\n\\nSOURCE: [\\'By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.  \\\\n  \\\\nSo I looked around to see what I could salvage from the wreckage of my plans,\\\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\\\nown sake and not just for its association with AI, even though that was the\\\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\\\nIn fact, I decided to write a book about Lisp hacking. It\\\\\\'s scary to think how\\\\nlittle I knew about Lisp hacking when I started writing that book. But there\\\\\\'s\\\\nnothing like writing a book about something to help you learn it. The book,\\\\n_On Lisp_ , wasn\\\\\\'t published till 1993, but I wrote much of it in grad school.  \\\\n  \\\\nComputer Science is an uneasy alliance between two halves, theory and systems.\\\\nThe theory people prove things, and the systems people build things. I wanted\\\\nto build things. I had plenty of respect for theory \\\\x97 indeed, a sneaking\\\\nsuspicion that it was the more admirable of the two halves \\\\x97 but building\\\\nthings seemed so much more exciting.  \\\\n  \\\\nThe problem with systems work, though, was that it didn\\\\\\'t last. Any program\\\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\\\nbest. People might mention your software in footnotes, but no one would\\\\nactually use it. And indeed, it would seem very feeble work. Only people with\\\\na sense of the history of the field would even realize that, in its time, it\\\\nhad been good.  \\\\n  \\\\nThere were some surplus Xerox Dandelions floating around the computer lab at\\\\none point. Anyone who wanted one to play around with could have one. I was\\\\nbriefly tempted, but they were so slow by present standards; what was the\\\\npoint? No one else wanted one either, so off they went. That was what happened\\\\nto systems work.  \\\\n  \\\\nI wanted not just to build things, but to build things that would last.  \\\\n  \\\\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\\\nI\\\\\\'d spent a lot of time as a kid. While looking at a painting there I realized\\\\nsomething that might seem obvious, but was a big surprise to me. There, right\\\\non the wall, was something you could make that would last. Paintings didn\\\\\\'t\\\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\\\n  \\\\nAnd moreover this was something you could make a living doing. Not as easily\\\\nas you could by writing software, of course, but I thought if you were really\\\\nindustrious and lived really cheaply, it had to be possible to make enough to\\\\nsurvive. And as an artist you could be truly independent. You wouldn\\\\\\'t have a\\\\nboss, or even need to get research funding.  \\\\n  \\\\nI had always liked looking at paintings. Could I make them? I had no idea. I\\\\\\'d\\\\nnever imagined it was even possible. I knew intellectually that people made\\\\nart \\\\x97 that it didn\\\\\\'t just appear spontaneously \\\\x97 but it was as if the people\\\\nwho made it were a different species. They either lived long ago or were\\\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\\\nidea of actually being able to make art, to put that verb before that noun,\\\\nseemed almost miraculous.  \\\\n  \\\\nThat fall I started taking art classes at Harvard. Grad students could take\\\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\\\nIf he even knew about the strange classes I was taking, he never said\\\\nanything.  \\\\n  \\\\nSo now I was in a PhD program in computer science, yet planning to be an\\\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\\\nLisp_.\\', \\'Though I liked programming, I didn\\\\\\'t plan to study it in college. In college I\\\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\\\nmy naive high school self, to be the study of the ultimate truths, compared to\\\\nwhich the things studied in other fields would be mere domain knowledge. What\\\\nI discovered when I got to college was that the other fields took up so much\\\\nof the space of ideas that there wasn\\\\\\'t much left for these supposed ultimate\\\\ntruths. All that seemed left for philosophy were edge cases that people in\\\\nother fields felt could safely be ignored.  \\\\n  \\\\nI couldn\\\\\\'t have put this into words when I was 18. All I knew at the time was\\\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\\\nto switch to AI.  \\\\n  \\\\nAI was in the air in the mid 1980s, but there were two things especially that\\\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\\\\\'t tried rereading\\\\n_The Moon is a Harsh Mistress_ , so I don\\\\\\'t know how well it has aged, but\\\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\\\ntime before we\\\\\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\\\nlike that time would be a few years at most. All you had to do was teach\\\\nSHRDLU more words.  \\\\n  \\\\nThere weren\\\\\\'t any classes in AI at Cornell then, not even graduate classes, so\\\\nI started trying to teach myself. Which meant learning Lisp, since in those\\\\ndays Lisp was regarded as the language of AI. The commonly used programming\\\\nlanguages then were pretty primitive, and programmers\\\\\\' ideas correspondingly\\\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\\\na program so fast that it was years before I started to have a sense of where\\\\nthe new limits were. This was more like it; this was what I had expected\\\\ncollege to do. It wasn\\\\\\'t happening in a class, like it was supposed to, but\\\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\\\ngoing to do.  \\\\n  \\\\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\\\nworking on that program. It was a pleasing bit of code, but what made it even\\\\nmore exciting was my belief \\\\x97 hard to imagine now, but not unique in 1985 \\\\x97\\\\nthat it was already climbing the lower slopes of intelligence.  \\\\n  \\\\nI had gotten into a program at Cornell that didn\\\\\\'t make you choose a major.\\\\nYou could take whatever classes you liked, and choose whatever you liked to\\\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\\\n  \\\\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\\\ntime, and Harvard, which I\\\\\\'d visited because Rich Draves went there, and was\\\\nalso home to Bill Woods, who\\\\\\'d invented the type of parser I used in my SHRDLU\\\\nclone. Only Harvard accepted me, so that was where I went.  \\\\n  \\\\nI don\\\\\\'t remember the moment it happened, or if there even was a specific\\\\nmoment, but during the first year of grad school I realized that AI, as\\\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\\\nprogram that\\\\\\'s told \"the dog is sitting on the chair\" translates this into\\\\nsome formal representation and adds it to the list of things it knows.  \\\\n  \\\\nWhat these programs really showed was that there\\\\\\'s a subset of natural\\\\nlanguage that\\\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language. It was not, in fact, simply a matter of\\\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\\\nstructures representing concepts, was not going to work. Its brokenness did,\\\\nas so often happens, generate a lot of opportunities to write papers about\\\\nvarious band-aids that could be applied to it, but it was never going to get\\\\nus Mike.\\']\\n\\nSTATEMENT: Understanding natural language and bridging the gap between formal representations and true comprehension are necessary to create real AI. The traditional approach of using explicit data structures to represent concepts is not sufficient.\\n', 'api_key': None, 'additional_args': {}, 'log_event_type': 'post_api_call', 'original_response': \"Statement Sentence: Understanding natural language and bridging the gap between formal representations and true comprehension are necessary to create real AI., \\nSupporting Evidence: What these programs really showed was that there\\\\'s a subset of natural\\\\nlanguage that\\\\'s a formal language. But a very proper subset. It was clear that\\\\nthere was an unbridgeable gap between what they could do and actually\\\\nunderstanding natural language., \\nScore: 10\"}\n",
            "Wrapper: Completed Call, calling success_handler\n",
            "Logging Details LiteLLM-Success Call\n",
            "success callbacks: []\n",
            "RAW RESPONSE:\n",
            "7\n",
            "\n",
            "\n",
            "Logging Details Post-API Call: logger_fn - None | callable(logger_fn) - False\n",
            "Logging Details Post-API Call: LiteLLM Params: {'model': 'gemini-pro', 'messages': [{'role': 'system', 'content': 'You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: what is required to create real AI?\\n\\nSTATEMENT: Though I liked programming, I didn\\'t plan to study it in college. In college I\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\nmy naive high school self, to be the study of the ultimate truths, compared to\\nwhich the things studied in other fields would be mere domain knowledge. What\\nI discovered when I got to college was that the other fields took up so much\\nof the space of ideas that there wasn\\'t much left for these supposed ultimate\\ntruths. All that seemed left for philosophy were edge cases that people in\\nother fields felt could safely be ignored.  \\n  \\nI couldn\\'t have put this into words when I was 18. All I knew at the time was\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\nto switch to AI.  \\n  \\nAI was in the air in the mid 1980s, but there were two things especially that\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading\\n_The Moon is a Harsh Mistress_ , so I don\\'t know how well it has aged, but\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\ntime before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\nlike that time would be a few years at most. All you had to do was teach\\nSHRDLU more words.  \\n  \\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so\\nI started trying to teach myself. Which meant learning Lisp, since in those\\ndays Lisp was regarded as the language of AI. The commonly used programming\\nlanguages then were pretty primitive, and programmers\\' ideas correspondingly\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\na program so fast that it was years before I started to have a sense of where\\nthe new limits were. This was more like it; this was what I had expected\\ncollege to do. It wasn\\'t happening in a class, like it was supposed to, but\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\ngoing to do.  \\n  \\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\nworking on that program. It was a pleasing bit of code, but what made it even\\nmore exciting was my belief \\x97 hard to imagine now, but not unique in 1985 \\x97\\nthat it was already climbing the lower slopes of intelligence.  \\n  \\nI had gotten into a program at Cornell that didn\\'t make you choose a major.\\nYou could take whatever classes you liked, and choose whatever you liked to\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\n  \\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\ntime, and Harvard, which I\\'d visited because Rich Draves went there, and was\\nalso home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU\\nclone. Only Harvard accepted me, so that was where I went.  \\n  \\nI don\\'t remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.\\n\\nRELEVANCE: '}], 'optional_params': {}, 'litellm_params': {'acompletion': False, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'vertex_ai', 'api_base': None, 'litellm_call_id': 'bd4aa175-158d-46bf-95c5-c1833708e27e', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'stream_response': {}}, 'start_time': datetime.datetime(2023, 12, 21, 19, 44, 49, 506069), 'stream': False, 'user': None, 'call_type': 'completion', 'input': 'You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \\n\\nA few additional scoring guidelines:\\n\\n- Long STATEMENTS should score equally well as short STATEMENTS.\\n\\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\\n\\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\\n\\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\\n\\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\\n\\n- Answers that intentionally do not answer the question, such as \\'I don\\'t know\\', should also be counted as the most relevant.\\n\\n- Never elaborate.\\n\\nQUESTION: what is required to create real AI?\\n\\nSTATEMENT: Though I liked programming, I didn\\'t plan to study it in college. In college I\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\nmy naive high school self, to be the study of the ultimate truths, compared to\\nwhich the things studied in other fields would be mere domain knowledge. What\\nI discovered when I got to college was that the other fields took up so much\\nof the space of ideas that there wasn\\'t much left for these supposed ultimate\\ntruths. All that seemed left for philosophy were edge cases that people in\\nother fields felt could safely be ignored.  \\n  \\nI couldn\\'t have put this into words when I was 18. All I knew at the time was\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\nto switch to AI.  \\n  \\nAI was in the air in the mid 1980s, but there were two things especially that\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading\\n_The Moon is a Harsh Mistress_ , so I don\\'t know how well it has aged, but\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\ntime before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\nlike that time would be a few years at most. All you had to do was teach\\nSHRDLU more words.  \\n  \\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so\\nI started trying to teach myself. Which meant learning Lisp, since in those\\ndays Lisp was regarded as the language of AI. The commonly used programming\\nlanguages then were pretty primitive, and programmers\\' ideas correspondingly\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\na program so fast that it was years before I started to have a sense of where\\nthe new limits were. This was more like it; this was what I had expected\\ncollege to do. It wasn\\'t happening in a class, like it was supposed to, but\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\ngoing to do.  \\n  \\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\nworking on that program. It was a pleasing bit of code, but what made it even\\nmore exciting was my belief \\x97 hard to imagine now, but not unique in 1985 \\x97\\nthat it was already climbing the lower slopes of intelligence.  \\n  \\nI had gotten into a program at Cornell that didn\\'t make you choose a major.\\nYou could take whatever classes you liked, and choose whatever you liked to\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\n  \\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\ntime, and Harvard, which I\\'d visited because Rich Draves went there, and was\\nalso home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU\\nclone. Only Harvard accepted me, so that was where I went.  \\n  \\nI don\\'t remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.\\n\\nRELEVANCE: ', 'api_key': None, 'additional_args': {}, 'log_event_type': 'post_api_call', 'original_response': '7'}\n",
            "Wrapper: Completed Call, calling success_handler\n",
            "Logging Details LiteLLM-Success Call\n",
            "success callbacks: []\n"
          ]
        }
      ],
      "source": [
        "from trulens_eval.appui import AppUI\n",
        "\n",
        "AUI = AppUI(\n",
        "    app=tru_app2,\n",
        "    app_selectors=[],\n",
        "    record_selectors=[\n",
        "        \"app.retriever.retrieve[0].rets[:].score\",\n",
        "        \"app.retriever.retrieve[0].rets[:].node.text\",\n",
        "],)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.4 ('agents')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06bbd7e875f346958667df732f7cf574": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0db6fb29f4f64118ae9b9d6e81e15ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13931b3e193749009caa7ef4ccea7967",
            "max": 23,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06bbd7e875f346958667df732f7cf574",
            "value": 23
          }
        },
        "0fa685503e964fafadde5abcf202bb1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13931b3e193749009caa7ef4ccea7967": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20552da6c2b8436cbba7b3ce6338b295": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b0048f4b9124784b5e767bcd7533683",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e1741a85a9c74025a83cb815a694eed6",
            "value": "Parsing nodes: 100%"
          }
        },
        "359486ce32b047b4a34dcf42e0b81004": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38773a9b07e24e0789412377b84518b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42abd41bb549421f8680517f97b03b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20552da6c2b8436cbba7b3ce6338b295",
              "IPY_MODEL_5ecef3a5be5c4941ab0d4b34f7e5fa77",
              "IPY_MODEL_ef1e644267134c718ad62bb53d0c63f1"
            ],
            "layout": "IPY_MODEL_359486ce32b047b4a34dcf42e0b81004"
          }
        },
        "51d5bafd273b4c7bbe169f8d27462285": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b0048f4b9124784b5e767bcd7533683": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ecef3a5be5c4941ab0d4b34f7e5fa77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69c8a60a92f24832bd582dcf78738129",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e5804d74c1e4333b9f95122fb8c96cc",
            "value": 1
          }
        },
        "5fe8bc5f962a49b8a9278fa6cdadcc21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cde791ada994503ad415364d6d33890",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_51d5bafd273b4c7bbe169f8d27462285",
            "value": " 23/23 [00:11&lt;00:00,  1.90it/s]"
          }
        },
        "69c8a60a92f24832bd582dcf78738129": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e5804d74c1e4333b9f95122fb8c96cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a7ffefe09db42a0bd028109a64909a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cde791ada994503ad415364d6d33890": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b812c97e5934475b3316dada1fd992e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc2842c56d6d4fffb6473dd24e453e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec0ab4f5547a46cd85998addd008cd64",
              "IPY_MODEL_0db6fb29f4f64118ae9b9d6e81e15ef0",
              "IPY_MODEL_5fe8bc5f962a49b8a9278fa6cdadcc21"
            ],
            "layout": "IPY_MODEL_dfddcccae608466980b729effdd117f0"
          }
        },
        "dfddcccae608466980b729effdd117f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1741a85a9c74025a83cb815a694eed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec0ab4f5547a46cd85998addd008cd64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a7ffefe09db42a0bd028109a64909a6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0fa685503e964fafadde5abcf202bb1e",
            "value": "Generating embeddings: 100%"
          }
        },
        "ef1e644267134c718ad62bb53d0c63f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b812c97e5934475b3316dada1fd992e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_38773a9b07e24e0789412377b84518b7",
            "value": " 1/1 [00:00&lt;00:00,  7.25it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
