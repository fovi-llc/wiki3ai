{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebLLM + Pyodide Integration Example\n",
    "\n",
    "This notebook demonstrates how to use WebLLM (Web Large Language Models) within Pyodide in JupyterLite.\n",
    "\n",
    "## Features:\n",
    "- üöÄ Client-side LLM inference (no server required)\n",
    "- üîí Privacy-first (all processing happens in your browser)\n",
    "- üåê Works offline after initial model download\n",
    "- üêç Python integration via Pyodide\n",
    "\n",
    "## Available Models:\n",
    "- `Llama-3.2-1B-Instruct-q4f16_1-MLC` (default, ~800MB)\n",
    "- `Llama-3.2-3B-Instruct-q4f16_1-MLC` (larger, better quality)\n",
    "- `TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC` (smallest, ~700MB)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyodide import js\n",
    "\n",
    "js.eval(\"\"\"\n",
    "const script = document.createElement('script');\n",
    "script.src = 'https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm/dist/index.min.js';\n",
    "document.head.appendChild(script);\n",
    "\"\"\")\n",
    "\n",
    "# WebLLM is automatically loaded via the extension\n",
    "# Initialize WebLLM with default model\n",
    "await webllm.initialize()\n",
    "print(\"WebLLM ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Wait for the script to load\n",
    "import asyncio\n",
    "await asyncio.sleep(1)\n",
    "\n",
    "# Initialize WebLLM\n",
    "js.eval(\"\"\"\n",
    "window.webllm = await window.mlc.llm.create();\n",
    "await window.webllm.loadModel('TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC-1k');\n",
    "\"\"\")\n",
    "\n",
    "# Chat with WebLLM\n",
    "response = await webllm.chat(\"Hello! Can you tell me about Python?\")\n",
    "print(\"WebLLM Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate text completion\n",
    "response = await webllm.generate(\"The benefits of using Python for data science are:\")\n",
    "print(\"Generated text:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced usage - Initialize with specific model\n",
    "# await webllm.initialize(\"Llama-3.2-3B-Instruct-q4f16_1-MLC\")  # Larger model\n",
    "\n",
    "# Multiple chat turns\n",
    "messages = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does it relate to artificial intelligence?\",\n",
    "    \"Give me a simple example\"\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    print(f\"\\n--- Question {i} ---\")\n",
    "    print(f\"User: {msg}\")\n",
    "    response = await webllm.chat(msg, max_tokens=256)\n",
    "    print(f\"WebLLM: {response}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
