{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee264a6",
   "metadata": {},
   "source": [
    "# WebLLM with JavaScript Kernel\n",
    "\n",
    "This notebook demonstrates how to load and use WebLLM directly in JavaScript, running in a Web Worker environment.\n",
    "\n",
    "The JavaScript kernel allows us to work directly with WebLLM's native JavaScript API without Python wrappers.\n",
    "\n",
    "**Note:** Since the JavaScript kernel doesn't support top-level `await`, we'll wrap async operations in immediately invoked async function expressions (IIFE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39704323",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// Import WebLLM directly using dynamic import\n",
    "console.log(\"Loading WebLLM...\");\n",
    "\n",
    "// Load WebLLM from CDN\n",
    "async function loadWebLLM() {\n",
    "    try {\n",
    "        // Import WebLLM module\n",
    "        const module = await import('https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.46/lib/index.min.js');\n",
    "        \n",
    "        // Make WebLLM available globally\n",
    "        globalThis.WebLLM = module.webllm || module.default || module;\n",
    "        \n",
    "        console.log(\"‚úÖ WebLLM loaded successfully!\");\n",
    "        console.log(\"Available WebLLM APIs:\", Object.keys(globalThis.WebLLM));\n",
    "        \n",
    "        return true;\n",
    "    } catch (error) {\n",
    "        console.error(\"‚ùå Failed to load WebLLM:\", error);\n",
    "        return false;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Load WebLLM using IIFE (Immediately Invoked Function Expression)\n",
    "(async () => {\n",
    "    const success = await loadWebLLM();\n",
    "    if (success) {\n",
    "        console.log(\"üéâ Ready to proceed to next cell!\");\n",
    "    }\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46403b4b",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// Check if WebLLM is available\n",
    "if (typeof globalThis.WebLLM === 'undefined') {\n",
    "    console.log(\"‚ö†Ô∏è WebLLM not loaded. Please run the previous cell first.\");\n",
    "} else {\n",
    "    console.log(\"üöÄ WebLLM is ready!\");\n",
    "    console.log(\"Available methods:\", Object.keys(globalThis.WebLLM));\n",
    "    \n",
    "    // Store WebLLM for easier access\n",
    "    window.WebLLM = globalThis.WebLLM;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88648f2e",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// Create WebLLM engine and load model\n",
    "let engine = null;\n",
    "\n",
    "async function initializeWebLLM(modelId = \"Llama-3.2-1B-Instruct-q4f16_1-MLC\") {\n",
    "    try {\n",
    "        console.log(\"üîÑ Initializing WebLLM engine...\");\n",
    "        console.log(\"‚è≥ This may take several minutes for the first download...\");\n",
    "        \n",
    "        // Create the MLC engine\n",
    "        engine = await globalThis.WebLLM.CreateMLCEngine(modelId);\n",
    "        \n",
    "        console.log(\"‚úÖ WebLLM engine initialized successfully!\");\n",
    "        console.log(\"üéâ Ready to chat!\");\n",
    "        \n",
    "        return true;\n",
    "    } catch (error) {\n",
    "        console.error(\"‚ùå Failed to initialize WebLLM:\", error);\n",
    "        return false;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Initialize the engine using IIFE\n",
    "(async () => {\n",
    "    const success = await initializeWebLLM();\n",
    "    if (success) {\n",
    "        console.log(\"‚ú® Model ready for use!\");\n",
    "    }\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ddbd1",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// Chat function\n",
    "async function chat(message) {\n",
    "    if (!engine) {\n",
    "        return \"‚ùå Engine not initialized. Please run the initialization cell first.\";\n",
    "    }\n",
    "    \n",
    "    try {\n",
    "        console.log(`üë§ User: ${message}`);\n",
    "        \n",
    "        const messages = [\n",
    "            { role: \"user\", content: message }\n",
    "        ];\n",
    "        \n",
    "        const response = await engine.chat.completions.create({\n",
    "            messages: messages,\n",
    "            max_tokens: 512,\n",
    "            temperature: 0.7\n",
    "        });\n",
    "        \n",
    "        const reply = response.choices[0].message.content;\n",
    "        console.log(`ü§ñ Assistant: ${reply}`);\n",
    "        \n",
    "        return reply;\n",
    "    } catch (error) {\n",
    "        console.error(\"‚ùå Chat error:\", error);\n",
    "        return `Error: ${error.message}`;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Test chat using IIFE\n",
    "(async () => {\n",
    "    const response = await chat(\"Hello! What can you tell me about WebLLM?\");\n",
    "    console.log(\"Response received:\", response);\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce64b8",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// Text completion function\n",
    "async function complete(prompt) {\n",
    "    if (!engine) {\n",
    "        return \"‚ùå Engine not initialized. Please run the initialization cell first.\";\n",
    "    }\n",
    "    \n",
    "    try {\n",
    "        console.log(`üìù Prompt: ${prompt}`);\n",
    "        \n",
    "        const response = await engine.completions.create({\n",
    "            prompt: prompt,\n",
    "            max_tokens: 256,\n",
    "            temperature: 0.7\n",
    "        });\n",
    "        \n",
    "        const completion = response.choices[0].text;\n",
    "        console.log(`‚ú® Completion: ${completion}`);\n",
    "        \n",
    "        return completion;\n",
    "    } catch (error) {\n",
    "        console.error(\"‚ùå Completion error:\", error);\n",
    "        return `Error: ${error.message}`;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Test completion using IIFE\n",
    "(async () => {\n",
    "    const completion = await complete(\"The main advantages of running AI models in the browser are:\");\n",
    "    console.log(\"Completion received:\", completion);\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b2a04",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// Interactive demo with multiple examples\n",
    "async function runDemo() {\n",
    "    console.log(\"üéØ Running WebLLM Demo Session\");\n",
    "    console.log(\"=\".repeat(50));\n",
    "\n",
    "    const testMessages = [\n",
    "        \"Explain Pyodide in one sentence\",\n",
    "        \"What makes WebLLM special for web development?\", \n",
    "        \"How does client-side AI benefit users?\",\n",
    "        \"Tell me about the future of browser-based ML\"\n",
    "    ];\n",
    "\n",
    "    // Run all test messages\n",
    "    for (let i = 0; i < testMessages.length; i++) {\n",
    "        const message = testMessages[i];\n",
    "        console.log(`\\nüí¨ Demo ${i + 1}:`);\n",
    "        console.log(\"-\".repeat(30));\n",
    "        \n",
    "        const response = await chat(message);\n",
    "        \n",
    "        // Add a small delay between requests\n",
    "        await new Promise(resolve => setTimeout(resolve, 1000));\n",
    "    }\n",
    "\n",
    "    console.log(\"\\nüéâ Demo complete!\");\n",
    "    console.log(\"üí° You can modify the testMessages array to try your own questions!\");\n",
    "}\n",
    "\n",
    "// Run demo using IIFE\n",
    "(async () => {\n",
    "    await runDemo();\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62ad27",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// Advanced WebLLM utilities\n",
    "const WebLLMUtils = {\n",
    "    // Get model information\n",
    "    async getModelInfo() {\n",
    "        if (!engine) return \"Engine not initialized\";\n",
    "        \n",
    "        try {\n",
    "            console.log(\"üìä Model Information:\");\n",
    "            console.log(\"Engine:\", engine);\n",
    "            console.log(\"Engine type:\", typeof engine);\n",
    "            \n",
    "            // Try to get more info if available\n",
    "            if (engine.getModelInfo) {\n",
    "                const info = await engine.getModelInfo();\n",
    "                console.log(\"Model details:\", info);\n",
    "            }\n",
    "            \n",
    "            return \"Model info logged to console\";\n",
    "        } catch (error) {\n",
    "            return `Error getting model info: ${error.message}`;\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    // Batch chat - send multiple messages\n",
    "    async batchChat(messages) {\n",
    "        if (!engine) return \"Engine not initialized\";\n",
    "        \n",
    "        const results = [];\n",
    "        for (const message of messages) {\n",
    "            try {\n",
    "                const response = await chat(message);\n",
    "                results.push({ message, response });\n",
    "                \n",
    "                // Small delay between requests\n",
    "                await new Promise(resolve => setTimeout(resolve, 500));\n",
    "            } catch (error) {\n",
    "                results.push({ message, error: error.message });\n",
    "            }\n",
    "        }\n",
    "        return results;\n",
    "    },\n",
    "    \n",
    "    // Reset engine\n",
    "    reset() {\n",
    "        engine = null;\n",
    "        console.log(\"üîÑ Engine reset. Run initialization cell to restart.\");\n",
    "    }\n",
    "};\n",
    "\n",
    "// Test utility functions using IIFE\n",
    "(async () => {\n",
    "    console.log(\"üõ†Ô∏è WebLLM Utilities loaded:\");\n",
    "    console.log(\"- WebLLMUtils.getModelInfo()\");\n",
    "    console.log(\"- WebLLMUtils.batchChat([messages])\");\n",
    "    console.log(\"- WebLLMUtils.reset()\");\n",
    "    \n",
    "    // Get model info\n",
    "    const info = await WebLLMUtils.getModelInfo();\n",
    "    console.log(\"Info result:\", info);\n",
    "})();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882117b0",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// Example: Custom chat with different parameters\n",
    "async function customChat(message, options = {}) {\n",
    "    if (!engine) {\n",
    "        console.log(\"‚ùå Engine not initialized\");\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    const defaultOptions = {\n",
    "        max_tokens: 512,\n",
    "        temperature: 0.7,\n",
    "        top_p: 0.9\n",
    "    };\n",
    "    \n",
    "    const chatOptions = { ...defaultOptions, ...options };\n",
    "    \n",
    "    try {\n",
    "        const messages = [{ role: \"user\", content: message }];\n",
    "        \n",
    "        console.log(`üéõÔ∏è Chat with options:`, chatOptions);\n",
    "        console.log(`üë§ User: ${message}`);\n",
    "        \n",
    "        const response = await engine.chat.completions.create({\n",
    "            messages: messages,\n",
    "            ...chatOptions\n",
    "        });\n",
    "        \n",
    "        const reply = response.choices[0].message.content;\n",
    "        console.log(`ü§ñ Assistant: ${reply}`);\n",
    "        \n",
    "        return reply;\n",
    "    } catch (error) {\n",
    "        console.error(\"‚ùå Custom chat error:\", error);\n",
    "        return `Error: ${error.message}`;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Test custom chat with different temperatures\n",
    "(async () => {\n",
    "    console.log(\"üé® Testing different chat parameters:\");\n",
    "    \n",
    "    const testMessage = \"Write a creative story about AI in the browser\";\n",
    "    \n",
    "    // Creative response (high temperature)\n",
    "    console.log(\"\\nüî• High creativity (temperature: 1.0):\");\n",
    "    await customChat(testMessage, { temperature: 1.0, max_tokens: 200 });\n",
    "    \n",
    "    // Balanced response (medium temperature)\n",
    "    console.log(\"\\n‚öñÔ∏è Balanced (temperature: 0.7):\");\n",
    "    await customChat(testMessage, { temperature: 0.7, max_tokens: 200 });\n",
    "    \n",
    "    // Focused response (low temperature)\n",
    "    console.log(\"\\nüéØ Focused (temperature: 0.1):\");\n",
    "    await customChat(testMessage, { temperature: 0.1, max_tokens: 200 });\n",
    "})();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
