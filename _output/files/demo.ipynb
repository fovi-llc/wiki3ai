{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chrome Built-in AI Demo in JupyterLab and JupyterLite Python Notebook\n",
    "\n",
    "This notebook demonstrates the Python wrapper for Chrome's built-in AI Prompt API.\n",
    "\n",
    "https://developer.chrome.com/docs/ai/prompt-api\n",
    "\n",
    "**Requirements:**\n",
    "- Chrome browser with Prompt API enabled\n",
    "- Running in JupyterLab, Jupyter Notebook, or JupyterLite (statically served, all execution locally in browser)\n",
    "\n",
    "## For developers: Using the Prompt API\n",
    "* Open Chrome and go to `chrome://flags`.\n",
    "* Enable the `#prompt-api-for-gemini-nano` flag.\n",
    "* Enable the `#optimization-guide-on-device-model` flag. If you see a \"BypassPerfRequirement\" option, select it.\n",
    "* Restart Chrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q anywidget ipywidgets wiki3_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from wiki3_ai import LanguageModel, Availability, LanguageModelWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModel()\n",
    "lm.widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = await lm.create()\n",
    "session_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Availability\n",
    "\n",
    "First, let's check if the Prompt API is available in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request ID:  bf5ba2a5-4470-46dd-9bdf-0f79caf38b8a\n",
      "Sent request:  {'id': 'bf5ba2a5-4470-46dd-9bdf-0f79caf38b8a', 'method': 'availability', 'params': {'options': {}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Availability.AVAILABLE: 'available'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the API is available\n",
    "await lm.availability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Prompt\n",
    "\n",
    "Create a session and send a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request ID:  699491c0-62a4-4098-9411-b4b90b0784b5\n",
      "Sent request:  {'id': '699491c0-62a4-4098-9411-b4b90b0784b5', 'method': 'prompt', 'params': {'sessionId': 'bd47ff02-a718-45dc-96d0-db868e947ed6', 'input': 'Write a haiku about Python programming.', 'options': {}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Code flows, clean and bright,\\nEasy to learn, so versatile,\\nScripts bloom, tasks are done. \\n\\n\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send a prompt\n",
    "result = await lm.prompt(\"Write a haiku about Python programming.\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt\n",
    "\n",
    "Use a system prompt to set the context for the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'widget'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create a session with a system prompt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m assistant_session = \u001b[38;5;28;01mawait\u001b[39;00m LanguageModel.create(\n\u001b[32m      3\u001b[39m     {\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minitialPrompts\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      5\u001b[39m             {\n\u001b[32m      6\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful Python programming assistant who gives concise answers.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m             }\n\u001b[32m      9\u001b[39m         ]\n\u001b[32m     10\u001b[39m     }\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Ask a question\u001b[39;00m\n\u001b[32m     14\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m assistant_session.prompt(\u001b[33m\"\u001b[39m\u001b[33mHow do I read a CSV file in Python?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/site-packages/wiki3_ai/language_model.py:338\u001b[39m, in \u001b[36mLanguageModel.create\u001b[39m\u001b[34m(self, options)\u001b[39m\n\u001b[32m    335\u001b[39m     options_dict = options\n\u001b[32m    337\u001b[39m params = {\u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m: session_id, \u001b[33m\"\u001b[39m\u001b[33moptions\u001b[39m\u001b[33m\"\u001b[39m: options_dict}\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwidget\u001b[49m.send_request(\u001b[33m\"\u001b[39m\u001b[33mcreate\u001b[39m\u001b[33m\"\u001b[39m, params)\n\u001b[32m    339\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreated session result: \u001b[39m\u001b[33m\"\u001b[39m, result)\n\u001b[32m    341\u001b[39m \u001b[38;5;28mself\u001b[39m._session_id = session_id\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'widget'"
     ]
    }
   ],
   "source": [
    "# Create a session with a system prompt\n",
    "assistant_session = await LanguageModel.create(\n",
    "    {\n",
    "        \"initialPrompts\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful Python programming assistant who gives concise answers.\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "result = await assistant_session.prompt(\"How do I read a CSV file in Python?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Response\n",
    "\n",
    "Stream the response as it's generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Sending request ID:  431eca0d-29bc-44c9-8014-d6c99abf4647\n",
      "Sent request:  {'id': '431eca0d-29bc-44c9-8014-d6c99abf4647', 'method': 'promptStreaming', 'params': {'sessionId': 'bd47ff02-a718-45dc-96d0-db868e947ed6', 'requestId': '310051d8-0304-466e-9243-b4b1e9041680', 'input': 'Tell me a short joke about computers.', 'options': {}}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Response: \", end=\"\")\n",
    "async for chunk in lm.prompt_streaming(\"Tell me a short joke about computers.\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-turn Conversation\n",
    "\n",
    "Have a conversation with context maintained across prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First message\n",
    "result1 = await assistant_session.prompt(\"What is a list comprehension?\")\n",
    "print(\"Assistant:\", result1)\n",
    "print()\n",
    "\n",
    "# Follow-up message (the assistant remembers the context)\n",
    "result2 = await assistant_session.prompt(\"Can you show me an example?\")\n",
    "print(\"Assistant:\", result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Token Usage\n",
    "\n",
    "Monitor how many tokens you've used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current usage: {assistant_session.input_usage}/{assistant_session.input_quota} tokens\")\n",
    "\n",
    "# Measure how many tokens a prompt would use\n",
    "usage = await assistant_session.measure_input_usage(\"What is machine learning?\")\n",
    "print(f\"\\nThis prompt would use approximately {usage} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Parameters\n",
    "\n",
    "Let's see what parameters the model supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = await lm.params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params:\n",
    "    print(f\"Default temperature: {params.default_temperature}\")\n",
    "    print(f\"Max temperature: {params.max_temperature}\")\n",
    "    print(f\"Default top-K: {params.default_top_k}\")\n",
    "    print(f\"Max top-K: {params.max_top_k}\")\n",
    "else:\n",
    "    print(\"Model parameters not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "Use JSON schema to get structured responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define a JSON schema\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"sentiment\", \"score\"],\n",
    "    \"properties\": {\n",
    "        \"sentiment\": {\"type\": \"string\", \"enum\": [\"positive\", \"negative\", \"neutral\"]},\n",
    "        \"score\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Get structured response\n",
    "result = await session.prompt(\n",
    "    \"Analyze this review: The product exceeded all my expectations! Absolutely amazing!\",\n",
    "    {\"responseConstraint\": schema},\n",
    ")\n",
    "\n",
    "# Parse the JSON response\n",
    "data = json.loads(result)\n",
    "print(f\"Sentiment: {data['sentiment']}\")\n",
    "print(f\"Score: {data['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Cloning\n",
    "\n",
    "Clone a session to create different conversation branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a story\n",
    "story_session = await LanguageModel.create(\n",
    "    {\"initialPrompts\": [{\"role\": \"system\", \"content\": \"You are a creative storyteller.\"}]}\n",
    ")\n",
    "\n",
    "await story_session.prompt(\"Once upon a time, there was a dragon.\")\n",
    "\n",
    "# Create two different story branches\n",
    "branch1 = await story_session.clone()\n",
    "branch2 = await story_session.clone()\n",
    "\n",
    "result1 = await branch1.prompt(\"The dragon was friendly and helpful.\")\n",
    "print(\"Branch 1:\", result1)\n",
    "print()\n",
    "\n",
    "result2 = await branch2.prompt(\"The dragon was fierce and terrifying.\")\n",
    "print(\"Branch 2:\", result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Destroy sessions when you're done to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await session.destroy()\n",
    "await assistant_session.destroy()\n",
    "await story_session.destroy()\n",
    "await branch1.destroy()\n",
    "await branch2.destroy()\n",
    "\n",
    "print(\"âœ… All sessions destroyed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
