{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e990a9c5",
   "metadata": {},
   "source": [
    "# Installing WebLLM in Pyodide with Micropip\n",
    "\n",
    "This notebook shows how to install and use WebLLM packages in Pyodide using micropip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4332807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JavaScript interop is built into Pyodide - no need to install packages\n",
    "# Import the built-in JavaScript bridge\n",
    "from js import console, document, window\n",
    "from pyodide.ffi import create_proxy\n",
    "import asyncio\n",
    "\n",
    "print(\"JavaScript interop ready!\")\n",
    "print(f\"Running in: {window.navigator.userAgent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WebLLM from CDN using Pyodide's built-in JS access\n",
    "from js import document, window, Promise\n",
    "import asyncio\n",
    "\n",
    "# Create script element to load WebLLM\n",
    "script = document.createElement('script')\n",
    "script.src = 'https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.46/lib/index.min.js'\n",
    "script.type = 'module'\n",
    "\n",
    "# Create a promise to wait for WebLLM to load\n",
    "def on_load(event):\n",
    "    console.log('WebLLM script loaded')\n",
    "    # Set up WebLLM reference\n",
    "    window.WebLLM = window.tvmjs.webllm if hasattr(window, 'tvmjs') else None\n",
    "\n",
    "def on_error(event):\n",
    "    console.error('Failed to load WebLLM script')\n",
    "\n",
    "script.addEventListener('load', create_proxy(on_load))\n",
    "script.addEventListener('error', create_proxy(on_error))\n",
    "\n",
    "# Add script to document head\n",
    "document.head.appendChild(script)\n",
    "\n",
    "print(\"Loading WebLLM...\")\n",
    "# Give some time for the script to load\n",
    "await asyncio.sleep(2)\n",
    "\n",
    "if hasattr(window, 'tvmjs') and hasattr(window.tvmjs, 'webllm'):\n",
    "    print(\"‚úÖ WebLLM loaded successfully!\")\n",
    "    window.WebLLM = window.tvmjs.webllm\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WebLLM might still be loading... try running the next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python wrapper for WebLLM in Pyodide\n",
    "from js import window, console, Object\n",
    "from pyodide.ffi import to_js, create_proxy\n",
    "import json\n",
    "\n",
    "class PyodideWebLLM:\n",
    "    def __init__(self):\n",
    "        self.engine = None\n",
    "        self.ready = False\n",
    "        self.webllm = None\n",
    "    \n",
    "    def check_webllm(self):\n",
    "        \"\"\"Check if WebLLM is available\"\"\"\n",
    "        if hasattr(window, 'tvmjs') and hasattr(window.tvmjs, 'webllm'):\n",
    "            self.webllm = window.tvmjs.webllm\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    async def load_model(self, model_id=\"Llama-3.2-1B-Instruct-q4f16_1-MLC\"):\n",
    "        \"\"\"Load a WebLLM model\"\"\"\n",
    "        if not self.check_webllm():\n",
    "            print(\"‚ùå WebLLM not loaded. Run the previous cell first.\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            print(f\"üîÑ Loading model: {model_id}\")\n",
    "            print(\"‚è≥ This may take several minutes for the first download...\")\n",
    "            \n",
    "            # Create the engine using WebLLM's CreateMLCEngine\n",
    "            self.engine = await self.webllm.CreateMLCEngine(model_id)\n",
    "            self.ready = True\n",
    "            print(\"‚úÖ Model loaded successfully!\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            console.error(e)\n",
    "            return False\n",
    "    \n",
    "    async def chat(self, message, max_tokens=512, temperature=0.7):\n",
    "        \"\"\"Chat with the model\"\"\"\n",
    "        if not self.ready:\n",
    "            return \"‚ùå Model not loaded. Call load_model() first.\"\n",
    "        \n",
    "        try:\n",
    "            # Prepare chat messages in the format WebLLM expects\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": message}\n",
    "            ]\n",
    "            \n",
    "            # Convert to JavaScript object\n",
    "            js_messages = to_js(messages)\n",
    "            \n",
    "            # Create request object\n",
    "            request = {\n",
    "                \"messages\": js_messages,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "            js_request = to_js(request)\n",
    "            \n",
    "            # Generate response\n",
    "            response = await self.engine.chat.completions.create(js_request)\n",
    "            \n",
    "            # Extract the response content\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            error_msg = f\"‚ùå Chat error: {e}\"\n",
    "            console.error(e)\n",
    "            return error_msg\n",
    "    \n",
    "    async def complete(self, prompt, max_tokens=512, temperature=0.7):\n",
    "        \"\"\"Complete text\"\"\"\n",
    "        if not self.ready:\n",
    "            return \"‚ùå Model not loaded. Call load_model() first.\"\n",
    "        \n",
    "        try:\n",
    "            request = {\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "            js_request = to_js(request)\n",
    "            \n",
    "            response = await self.engine.completions.create(js_request)\n",
    "            return response.choices[0].text\n",
    "        except Exception as e:\n",
    "            error_msg = f\"‚ùå Completion error: {e}\"\n",
    "            console.error(e)\n",
    "            return error_msg\n",
    "\n",
    "# Create WebLLM instance\n",
    "llm = PyodideWebLLM()\n",
    "print(\"ü§ñ WebLLM wrapper created!\")\n",
    "print(\"üìù Next: Run the model loading cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3614fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model (this may take a few minutes for first download)\n",
    "print(\"üöÄ Starting model load...\")\n",
    "print(\"üí° Tip: The first time will download ~1-2GB, please be patient!\")\n",
    "\n",
    "success = await llm.load_model()\n",
    "\n",
    "if success:\n",
    "    print(\"üéâ Ready to chat!\")\n",
    "    print(\"üí¨ You can now run the chat examples below\")\n",
    "else:\n",
    "    print(\"‚ùå Model loading failed. Try reloading WebLLM in the previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7536812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat example\n",
    "print(\"üí¨ Chat Example\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "question = \"Explain what Pyodide is in simple terms\"\n",
    "print(f\"üë§ User: {question}\")\n",
    "print(\"ü§ñ Assistant: \", end=\"\")\n",
    "\n",
    "response = await llm.chat(question)\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text completion example\n",
    "print(\"‚úçÔ∏è Text Completion Example\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "prompt = \"The advantages of running Python in the browser are:\"\n",
    "print(f\"üìù Prompt: {prompt}\")\n",
    "print(\"ü§ñ Completion: \", end=\"\")\n",
    "\n",
    "response = await llm.complete(prompt)\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo chat session with multiple examples\n",
    "print(\"ü§ñ WebLLM Demo Chat Session\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Predefined test messages for demonstration\n",
    "test_messages = [\n",
    "    \"Hello! What can you do?\",\n",
    "    \"Explain machine learning in one sentence\",\n",
    "    \"What makes WebLLM special?\",\n",
    "    \"How does running AI in the browser help developers?\"\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(test_messages, 1):\n",
    "    print(f\"\\nüí¨ Example {i}:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"üë§ User: {msg}\")\n",
    "    print(\"ü§ñ Assistant: \", end=\"\")\n",
    "    \n",
    "    response = await llm.chat(msg, max_tokens=256)\n",
    "    print(response)\n",
    "\n",
    "print(\"\\nüéâ Demo complete!\")\n",
    "print(\"üí° Try modifying the test_messages list with your own questions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3315231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Using the WebLLM Extension\n",
    "\n",
    "If the direct JavaScript approach above doesn't work, you can also use our custom WebLLM extension that should be automatically loaded in this JupyterLite environment.\n",
    "\n",
    "```python\n",
    "# Check if WebLLM extension is available\n",
    "if hasattr(window, 'WebLLMHelper'):\n",
    "    print(\"‚úÖ WebLLM Extension found!\")\n",
    "    \n",
    "    # Use the extension's helper class\n",
    "    extension_llm = window.WebLLMHelper.new()\n",
    "    \n",
    "    # Initialize and use\n",
    "    await extension_llm.initialize()\n",
    "    response = await extension_llm.chat(\"Hello from the extension!\")\n",
    "    print(f\"Extension response: {response}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WebLLM Extension not found - using direct approach above\")\n",
    "```\n",
    "\n",
    "Both approaches should work, but the extension provides a more integrated experience."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
